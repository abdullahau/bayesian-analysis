{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"StanCon 2018 Helsinki Intro Workshop: Pest Control Example\"\n",
        "author: \"Abdullah Mahmood\"\n",
        "date: today\n",
        "format:\n",
        "    html:\n",
        "        toc: true\n",
        "        toc-depth: 3\n",
        "        code-fold: true\n",
        "        html-math-method:\n",
        "            method: mathjax\n",
        "            url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n",
        "editor: source\n",
        "jupyter: main\n",
        "---\n",
        "\n",
        "\n",
        "### Setup\n"
      ],
      "id": "a3863d96"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cmdstanpy\n",
        "from cmdstanpy import CmdStanModel\n",
        "\n",
        "from matplotlib import style \n",
        "style.use('../../../PlottingStyle.mplstyle')"
      ],
      "id": "9da125c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def StanModel(stan_file: str, stan_code: str) -> CmdStanModel:\n",
        "    \"\"\"Load or compile a Stan model\"\"\"\n",
        "    stan_src = f\"{stan_file}.stan\"\n",
        "\n",
        "    if not os.path.isfile(stan_file):  \n",
        "        open(stan_src, 'w').write(stan_code)  # Write Stan code if needed\n",
        "        return CmdStanModel(stan_file=stan_src, cpp_options={'STAN_THREADS': 'true', 'parallel_chains': 4})\n",
        "    \n",
        "    return CmdStanModel(stan_file=stan_src, exe_file=stan_file)"
      ],
      "id": "be842096",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem\n",
        "\n",
        "### Background\n",
        "\n",
        "Imagine that you are a statistician or data scientist working as an independent contractor. One of your clients is a company that owns many residential buildings throughout New York City. The property manager explains that they are concerned about the number of cockroach complaints that they receive from their buildings. Previously the company has offered monthly visits from a pest inspector as a solution to this problem. While this is the default solution of many property managers in NYC, the tenants are rarely home when the inspector visits, and so the manager reasons that this is a relatively expensive solution that is currently not very effective.\n",
        "\n",
        "One alternative to this problem is to deploy long term bait stations. In this alternative, child and pet safe bait stations are installed throughout the apartment building. Cockroaches obtain quick acting poison from these stations and distribute it throughout the colony. The manufacturer of these bait stations provides some indication of the space-to-bait efficacy, but the manager suspects that this guidance was not calculated with NYC roaches in mind. NYC roaches, the manager rationalizes, have more hustle than traditional roaches; and NYC buildings are built differently than other common residential buildings in the US. This is particularly important as the unit cost for each bait station per year is quite high.\n",
        "\n",
        "### The Goal\n",
        "\n",
        "The manager wishes to employ your services to help them to find the optimal number of roach bait stations they should place in each of their buildings in order to minimize the number of cockroach complaints while also keeping expenditure on pest control affordable.\n",
        "\n",
        "A subset of the company's buildings have been randomly selected for an experiment:\n",
        "\n",
        "-   At the beginning of each month, a pest inspector randomly places a number of bait stations throughout the building, without knowledge of the current cockroach levels in the building\n",
        "-   At the end of the month, the manager records the total number of cockroach complaints in that building.\n",
        "-   The manager would like to determine the optimal number of traps ($\\textrm{traps}$) that balances the lost revenue ($R$) that complaints ($\\textrm{complaints}$) generate with the all-in cost of maintaining the traps ($\\textrm{TC}$).\n",
        "\n",
        "Fortunately, Bayesian data analysis provides a coherent framework for us to tackle this problem.\n",
        "\n",
        "Formally, we are interested in finding:\n",
        "\n",
        "$$\n",
        "\\arg\\max_{\\textrm{traps} \\in \\mathbb{N}} \\mathbb{E}_{\\text{complaints}}[R(\\textrm{complaints}(\\textrm{traps})) - \\textrm{TC}(\\textrm{traps})]\n",
        "$$\n",
        "\n",
        "The property manager would also, if possible, like to learn how these results generalize to buildings they haven't treated so they can understand the potential costs of pest control at buildings they are acquiring as well as for the rest of their building portfolio.\n",
        "\n",
        "As the property manager has complete control over the number of traps set, the random variable contributing to this expectation is the number of complaints given the number of traps. We will model the number of complaints as a function of the number of traps.\n",
        "\n",
        "## The Data\n",
        "\n",
        "The data provided to us is in a file called `pest_data.csv`. Let's load the data and see what the structure is:\n"
      ],
      "id": "860c7a55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pest_data = pd.read_csv('data/pest_data.csv', sep=',', header=0, parse_dates=['date'])\n",
        "pest_data.head()"
      ],
      "id": "46a46066",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have access to the following fields:\n",
        "\n",
        "-   `complaints`: Number of complaints per building per month\n",
        "-   `building_id`: The unique building identifier\n",
        "-   `traps`: The number of traps used per month per building\n",
        "-   `date`: The date at which the number of complaints are recorded\n",
        "-   `live_in_super`: An indicator for whether the building as a live-in super\n",
        "-   `age_of_building`: The age of the building\n",
        "-   `total_sq_foot`: The total square footage of the building\n",
        "-   `average_tenant_age`: The average age of the tenants per building\n",
        "-   `monthly_average_rent`: The average monthly rent per building\n",
        "-   `floors`: The number of floors per building\n",
        "\n",
        "First, let's see how many buildings we have data for:\n"
      ],
      "id": "ce7c656a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_buildings = len(pest_data.building_id.unique())\n",
        "N_buildings"
      ],
      "id": "8e8deb57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And make some plots of the raw data:\n"
      ],
      "id": "455d572d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "sns.histplot(pest_data.complaints, discrete=True)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "37486aec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "sns.stripplot(data=pest_data, x='traps', y='complaints', hue='live_in_super', jitter=True)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "a2dbc37e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "import matplotlib.dates as mdates\n",
        "plt.clf()\n",
        "\n",
        "g = sns.FacetGrid(data=pest_data, col='building_id', hue='live_in_super', col_wrap=2, height=2, sharex=False)\n",
        "g.map_dataframe(sns.lineplot, 'date', 'traps', linestyle='dashed', color='k', label='Number of Traps')\n",
        "g.map_dataframe(sns.lineplot, x='date', y='complaints', marker='o', label='Number of Complaints')\n",
        "\n",
        "for ax in g.axes.flat:\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))  # Show only abbreviated month names (e.g., Jan, Feb)\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Set ticks at the start of each month\n",
        "\n",
        "line_labels = [\n",
        "    plt.Line2D([0], [0], color='k', linestyle='dashed', label=\"Number of Traps\"),\n",
        "    plt.Line2D([0], [0], marker='o', color='k', label=\"Number of Complaints\", linestyle='solid')\n",
        "]\n",
        "g.add_legend(handles=line_labels, title='Legend', loc='upper right')   \n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "2efc663b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first question we'll look at is just whether the number of complaints per building per month is associated with the number of bait stations per building per month, ignoring the temporal and across-building variation (we'll come back to those sources of variation later in the document). That requires only two variables, $\\textrm{complaints}$ and $\\textrm{traps}$. How should we model the number of complaints?\n",
        "\n",
        "## Bayesian Workflow\n",
        "\n",
        "-   Exploratory data analysis\n",
        "-   Prior predictive checking\n",
        "-   Model fitting and algorithm diagnostics\n",
        "-   Posterior predictive checking\n",
        "-   Model comparison (e.g., via cross-validation)\n",
        "\n",
        "## Modeling count data: Poisson distribution\n",
        "\n",
        "We already know some rudimentary information about what we should expect. The number of complaints over a month should be either zero or an integer. The property manager tells us that it is possible but unlikely that number of complaints in a given month is zero. Occasionally there are a very large number of complaints in a single month. A common way of modeling this sort of skewed, single bounded count data is as a Poisson random variable. One concern about modeling the outcome variable as Poisson is that the data may be over-dispersed, but we'll start with the Poisson model and then check whether over-dispersion is a problem by comparing our model's predictions to the data.\n",
        "\n",
        "### Model\n",
        "\n",
        "Given that we have chosen a Poisson regression, we define the likelihood to be the Poisson probability mass function over the number bait stations placed in the building, denoted below as `traps`. This model assumes that the mean and variance of the outcome variable `complaints` (number of complaints) is the same. We'll investigate whether this is a good assumption after we fit the model.\n",
        "\n",
        "For building $b = 1,\\dots,10$ at time (month) $t = 1,\\dots,12$, we have:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{complaints}_{b,t} & \\sim \\textrm{Poisson}(\\lambda_{b,t}) \\\\\n",
        "\\lambda_{b,t} & = \\exp{(\\eta_{b,t})} \\\\\n",
        "\\eta_{b,t} &= \\alpha + \\beta \\, \\textrm{traps}_{b,t}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Let's encode this probability model in a Stan program.\n",
        "\n",
        "### Writing our first Stan model\n"
      ],
      "id": "8fa3f59a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "simple_poisson_regression = '''\n",
        "functions {\n",
        "  /*\n",
        "  * Alternative to poisson_log_rng() that \n",
        "  * avoids potential numerical problems during warmup\n",
        "  */\n",
        "  int poisson_log_safe_rng(real eta) {\n",
        "    real pois_rate = exp(eta);\n",
        "    if (pois_rate >= exp(20.79))\n",
        "      return -9;\n",
        "    return poisson_rng(pois_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "    array[N] int<lower=0> complaints;\n",
        "    vector<lower=0>[N] traps;\n",
        "}\n",
        "parameters {\n",
        "    real alpha;\n",
        "    real beta;\n",
        "}\n",
        "model {\n",
        "  // weakly informative priors:\n",
        "  // we expect negative slope on traps and a positive intercept,\n",
        "  // but we will allow ourselves to be wrong\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  \n",
        "  // poisson_log(eta) is more efficient and stable alternative to poisson(exp(eta))\n",
        "  complaints ~ poisson_log(alpha + beta * traps);\n",
        "} \n",
        "generated quantities {\n",
        "    array[N] int y_rep;\n",
        "\n",
        "    for (n in 1:N) {\n",
        "        y_rep[n] = poisson_log_safe_rng(alpha + beta * traps[n]);\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "simple_poisson_regression_model = StanModel('../stan_models/simple_poisson_regression', simple_poisson_regression)"
      ],
      "id": "d46d6232",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making sure our code is right\n",
        "\n",
        "However, before we fit the model to real data, we should check that our model works well with simulated data. We'll simulate data according to the model and then check that we can sufficiently recover the parameter values used in the simulation.\n"
      ],
      "id": "98f8c01f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simple_poisson_regression_dgp_model = '''\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "    real<lower=0> mean_traps;\n",
        "}\n",
        "model {\n",
        "} \n",
        "generated quantities {\n",
        "    array[N] int traps;\n",
        "    array[N] int complaints;\n",
        "    real alpha = normal_rng(log(4), .1);\n",
        "    real beta = normal_rng(-0.25, .1);\n",
        "\n",
        "    for (n in 1:N)  {\n",
        "        traps[n] = poisson_rng(mean_traps);\n",
        "        complaints[n] = poisson_log_rng(alpha + beta * traps[n]);\n",
        "    }\n",
        "}\n",
        "'''"
      ],
      "id": "bed13047",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will do the *compilation* and *fitting* in two stages to demonstrate what is happening under the hood.\n",
        "\n",
        "First we will compile the Stan program (`simple_poisson_regression_dgp.stan`) that will generate the fake data.\n"
      ],
      "id": "a743e2cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "simple_poisson_regression_dgp = StanModel('../stan_models/simple_poisson_regression_dgp', simple_poisson_regression_dgp_model)"
      ],
      "id": "09359237",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can simulate the data by calling the `sample()` method on the `CmdStanModel` object instantiated above. The method requires that we pass input data in the form of a dictionary. The names must match the names used in the `data` block of the Stan program.\n"
      ],
      "id": "99148cc9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_dgp = simple_poisson_regression_dgp.sample(\n",
        "    data={'N': len(pest_data.traps), \n",
        "          'mean_traps': pest_data.traps.mean()},\n",
        "    chains=1,\n",
        "    iter_sampling=1,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "samps_dgp = az.from_cmdstanpy(fitted_model_dgp)"
      ],
      "id": "f2f8c261",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit the model to the fake data\n",
        "\n",
        "In order to pass the fake data to our Stan program, we need to arrange the data into a dictionary. The keys must match the names used in the `data` block of the Stan program.\n"
      ],
      "id": "48d5c0ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_dat_fake = {\n",
        "    'N': len(pest_data.traps),\n",
        "    'traps': samps_dgp.posterior.traps.sel(chain=0, draw=0).to_numpy().astype(np.int16),\n",
        "    'complaints': samps_dgp.posterior.complaints.sel(chain=0, draw=0).to_numpy().astype(np.int16)\n",
        "}"
      ],
      "id": "f2339123",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the simulated data we fit the model to see if we can recover the `alpha` and `beta` parameters used in the simulation.\n"
      ],
      "id": "25cdb966"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fit_model_P = simple_poisson_regression_model.sample(data=stan_dat_fake, show_progress=False)\n",
        "posterior_alpha_beta = az.extract(az.from_cmdstanpy(fit_model_P), 'posterior', var_names=['alpha', 'beta']).to_dataframe()[['alpha', 'beta']]"
      ],
      "id": "e6b04a49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assess parameter recovery\n"
      ],
      "id": "79d98b6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "true_alpha = samps_dgp.posterior.alpha.sel(chain=0, draw=0).to_numpy()\n",
        "true_beta = samps_dgp.posterior.beta.sel(chain=0, draw=0).to_numpy()\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
        "sns.histplot(posterior_alpha_beta, x='alpha', bins=30, ax=ax[0])\n",
        "ax[0].axvline(true_alpha, color='r', linewidth=2)\n",
        "sns.histplot(posterior_alpha_beta, x='beta', bins=30, ax=ax[1])\n",
        "ax[1].axvline(true_beta, color='r', linewidth=2)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "13e66f11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We don't do a great job recovering the parameters here simply because we're simulating so few observations that the posterior uncertainty remains rather large, but it looks at least *plausible* ($\\alpha$ and $\\beta$ are contained within the histograms). If we did the simulation with many more observations the parameters would be estimated much more precisely.\n",
        "\n",
        "We should also check if the `y_rep` datasets (in-sample predictions) that we coded in the `generated quantities` block are similar to the `y` (complaints) values we conditioned on when fitting the model.\n",
        "\n",
        "Here is a plot of the density estimate of the observed data compared to 200 of the `y_rep` datasets:\n"
      ],
      "id": "fbe759e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_rep = az.from_cmdstanpy(fit_model_P).posterior.y_rep\n",
        "\n",
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0,4), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_fake['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,12)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e9e07897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the plot above we have the kernel density estimate of the observed data ($y$, thicker curve) and 200 simulated data sets ($y_{rep}$, thin curves) from the posterior predictive distribution. If the model fits the data well, as it does here, there is little difference between the observed dataset and the simulated datasets.\n",
        "\n",
        "Another plot we can make for count data is a rootogram. This is a plot of the expected counts (continuous line) vs the observed counts (blue histogram). We can see the model fits well because the observed histogram matches the expected counts relatively well.\n"
      ],
      "id": "3ae76b1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "vals, counts = np.unique(y_rep.to_dataframe()['y_rep'].to_numpy(), return_counts=True)\n",
        "hist_range = (min(vals), max(vals))\n",
        "az.plot_dist(stan_dat_fake['complaints'], kind='hist', ax=ax, \n",
        "             hist_kwargs={'bins': np.arange(hist_range[0], hist_range[1] + 1, 1), 'range': hist_range})\n",
        "\n",
        "secax = ax.twinx()\n",
        "secax.plot(vals, counts, color='red', linestyle='dashed', marker='o')\n",
        "\n",
        "ax.set_xlim(hist_range[0]-0.5, hist_range[1]+0.5)\n",
        "\n",
        "secax.set_yticklabels([])\n",
        "secax.yaxis.set_ticks([])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "12e0a20a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit with real data\n",
        "\n",
        "To fit the model to the actual observed data we'll first create a dictionary to pass to Stan using the variables in the `pest_data` data frame:\n"
      ],
      "id": "0b3cc90b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_dat_simple = {\n",
        "    'N': len(pest_data.traps),\n",
        "    'complaints': pest_data.complaints,\n",
        "    'traps': pest_data.traps\n",
        "}"
      ],
      "id": "69178770",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fit_P_real_data = simple_poisson_regression_model.sample(data=stan_dat_simple, show_progress=False)\n",
        "az_fit_P_real_data = az.from_cmdstanpy(fit_P_real_data, posterior_predictive='y_rep')"
      ],
      "id": "d62ca1cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = fit_P_real_data.summary(percentiles=(2.5, 25, 50, 75, 97.5))\n",
        "summary[~summary.index.str.contains('y_rep')]"
      ],
      "id": "3ad8cc04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the posterior distributions:\n"
      ],
      "id": "68a1e8d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.ticker as mticker\n",
        "plt.clf()\n",
        "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
        "az.plot_dist(az_fit_P_real_data.posterior.alpha, kind='hist', ax=ax[0])\n",
        "az.plot_dist(az_fit_P_real_data.posterior.beta, kind='hist', ax=ax[1])\n",
        "for a in ax:\n",
        "    a.xaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "50581f80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we expected, it appears the number of bait stations set in a building is associated with the number of complaints about cockroaches that were made in the following month. However, we still need to consider how well the model fits.\n",
        "\n",
        "### Posterior predictive checking\n"
      ],
      "id": "b9079f93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_rep = az_fit_P_real_data.posterior_predictive.y_rep\n",
        "\n",
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_simple['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,25)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "bd8768af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As opposed to when we fit the model to simulated data above, here the simulated datasets is not as dispersed as the observed data and don't seem to capture the rate of zeros in the observed data. The Poisson model may not be sufficient for this data.\n",
        "\n",
        "Let's explore this further by looking directly at the proportion of zeros in the real data and predicted data.\n"
      ],
      "id": "1e0beae2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.histplot(np.mean(y_rep == 0, axis=2).to_numpy().flatten(), bins=20, ax=ax)\n",
        "ax.axvline(np.mean(stan_dat_simple['complaints'] == 0), linewidth=2, color='r')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "dfcb0f60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows the observed proportion of zeros (thick vertical line) and a histogram of the proportion of zeros in each of the simulated data sets. It is clear that the model does not capture this feature of the data well at all.\n",
        "\n",
        "This next plot is a plot of the standardized residuals of the observed vs predicted number of complaints.\n"
      ],
      "id": "03a38a63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_y_rep = y_rep.mean(dim=['chain', 'draw'])\n",
        "std_resid = (stan_dat_simple['complaints'] - mean_y_rep) / np.sqrt(mean_y_rep)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(mean_y_rep, std_resid, 'o', alpha=0.5)\n",
        "plt.axhline(2, linestyle='--')\n",
        "plt.axhline(-2, linestyle='--')\n",
        "plt.xlabel('mean_y_rep')\n",
        "plt.ylabel('std_resid')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "12f4dc47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see here, it looks as though we have more positive residuals than negative, which indicates that the model tends to underestimate the number of complaints that will be received.\n",
        "\n",
        "Below another useful plot to compare the observed vs expected number of complaints. This is a plot of the expected counts (continuous line) vs the observed counts (black histogram):\n"
      ],
      "id": "ab9179f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "vals, counts = np.unique(y_rep, return_counts=True)\n",
        "hist_range = (min(vals), max(vals))\n",
        "az.plot_dist(stan_dat_simple['complaints'], kind='hist', ax=ax, \n",
        "             hist_kwargs={'bins': np.arange(hist_range[0], hist_range[1] + 1, 1), 'range': hist_range})\n",
        "\n",
        "secax = ax.twinx()\n",
        "secax.plot(vals, counts, color='red', linestyle='dashed', marker='o')\n",
        "\n",
        "ax.set_xlim(hist_range[0]-0.5, hist_range[1]+0.5)\n",
        "\n",
        "secax.set_yticklabels([])\n",
        "secax.yaxis.set_ticks([])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "88e6a2c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model was fitting well these would be relatively similar, however in this figure we can see the number of complaints is underestimated if there are few complaints, over-estimated for medium numbers of complaints, and underestimated if there are a large number of complaints.\n",
        "\n",
        "We can also view how the predicted number of complaints varies with the number of traps. From this we can see that the model doesn't seem to fully capture the data.\n"
      ],
      "id": "a1ae892b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(stan_dat_simple['traps'], stan_dat_simple['complaints'], 'o', fillstyle='none')\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, trap in enumerate(stan_dat_simple['traps']):\n",
        "    grouped_indices[trap].append(idx)\n",
        "\n",
        "grouped_array = {trap: y_rep[:, :, idx_list].to_numpy().flatten() for trap, idx_list in grouped_indices.items()}\n",
        "\n",
        "for traps_x, complaints_y in grouped_array.items():\n",
        "    inner_prob = np.array([np.quantile(complaints_y, 0.75), np.quantile(complaints_y, 0.25)])  # az.hdi(complaints_y, hdi_prob=0.5)\n",
        "    outer_prob = np.array([np.quantile(complaints_y, 0.95), np.quantile(complaints_y, 0.05)])  # az.hdi(complaints_y, hdi_prob=0.9)\n",
        "    ax.plot(np.full(inner_prob.shape, traps_x), inner_prob, 'k', linewidth=2)\n",
        "    ax.plot(np.full(outer_prob.shape, traps_x), outer_prob, 'k', linewidth=0.5)\n",
        "    ax.plot(traps_x, np.mean(complaints_y), 'ko')\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "c5344508",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specifically, the model doesn't capture the tails of the observed data very well.\n",
        "\n",
        "## Expanding the model: multiple predictors\n",
        "\n",
        "Modeling the relationship between complaints and bait stations is the simplest model. We can expand the model, however, in a few ways that will be beneficial for our client. Moreover, the manager has told us that they expect there are a number of other reasons that one building might have more roach complaints than another.\n",
        "\n",
        "### Interpretability\n",
        "\n",
        "Currently, our model's mean parameter is a rate of complaints per 30 days, but we're modeling a process that occurs over an area as well as over time. We have the square footage of each building, so if we add that information into the model, we can interpret our parameters as a rate of complaints per square foot per 30 days.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{complaints}_{b,t} & \\sim \\textrm{Poisson}(\\textrm{sq. foot}_b\\,\\lambda_{b,t}) \\\\\n",
        "\\lambda_{b,t} & = \\exp{(\\eta_{b,t} )} \\\\\n",
        "\\eta_{b,t} &= \\alpha + \\beta \\, \\textrm{traps}_{b,t}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The term $\\text{sq. foot}$ is called an **exposure term**. If we log the term, we can put it in $\\eta_{b,t}$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{complaints}_{b,t} & \\sim \\textrm{Poisson}(\\lambda_{b,t}) \\\\\n",
        "\\lambda_{b,t} & = \\exp{(\\eta_{b,t} )} \\\\\n",
        "\\eta_{b,t} &= \\alpha + \\beta \\, \\textrm{traps}_{b,t} + \\textrm{log sq. foot}_b\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "A quick test shows us that there appears to be a relationship between the square footage of the building and the number of complaints received:\n"
      ],
      "id": "1341f7f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "plt.clf()\n",
        "sns.regplot(x=np.log(pest_data.total_sq_foot),y=np.log1p(pest_data.complaints))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "76fd8997",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the property manager's intuition, we include two extra pieces of information we know about the building - the (log of the) square floor space and whether there is a live in super or not - into both the simulated and real data.\n"
      ],
      "id": "b03094b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_dat_simple['log_sq_foot'] = np.log(pest_data.total_sq_foot/1e4)\n",
        "stan_dat_simple['live_in_super'] = pest_data.live_in_super"
      ],
      "id": "165d21e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stan program for Poisson multiple regression\n",
        "\n",
        "Now we need a new Stan model that uses multiple predictors.\n"
      ],
      "id": "7984e089"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "multiple_poisson_regression = '''\n",
        "functions {\n",
        "    /*\n",
        "    * Alternative to poisson_log_rng() that \n",
        "    * avoids potential numerical problems during warmup\n",
        "    */\n",
        "    int poisson_log_safe_rng(real eta) {\n",
        "    real pois_rate = exp(eta);\n",
        "    if (pois_rate >= exp(20.79))\n",
        "      return -9;\n",
        "    return poisson_rng(pois_rate);\n",
        "    }\n",
        "}\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "    array[N] int<lower=0> complaints;\n",
        "    vector<lower=0>[N] traps;\n",
        "    vector<lower=0,upper=1>[N] live_in_super;\n",
        "    vector[N] log_sq_foot;  // exposure term\n",
        "}\n",
        "parameters {\n",
        "    real alpha;\n",
        "    real beta;\n",
        "    real beta_super;\n",
        "}\n",
        "model {\n",
        "    beta ~ normal(-0.25, 1);\n",
        "    beta_super ~ normal(-0.5, 1);\n",
        "    alpha ~ normal(log(4), 1);\n",
        "    \n",
        "    complaints ~ poisson_log(alpha + beta * traps + beta_super * live_in_super + log_sq_foot);\n",
        "} \n",
        "generated quantities {\n",
        "    array[N] int y_rep;\n",
        "    \n",
        "    for (n in 1:N) \n",
        "    y_rep[n] = poisson_log_safe_rng(alpha + beta * traps[n] + beta_super * live_in_super[n] + log_sq_foot[n]);\n",
        "}\n",
        "'''\n",
        "\n",
        "multiple_poisson_regression_model = StanModel('../stan_models/multiple_poisson_regression', multiple_poisson_regression)"
      ],
      "id": "e7309ccc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulate fake data with multiple predictors\n"
      ],
      "id": "09bc8ef6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "multiple_poisson_regression_dgp = '''\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "}\n",
        "model {\n",
        "} \n",
        "generated quantities {\n",
        "    vector[N] log_sq_foot;\n",
        "    array[N] int live_in_super;\n",
        "    array[N] int traps;\n",
        "    array[N] int complaints;\n",
        "    \n",
        "    real alpha = normal_rng(log(4), .1);\n",
        "    real beta = normal_rng(-0.25, .1);\n",
        "    real beta_super = normal_rng(-0.5, .1);\n",
        "    \n",
        "    for (n in 1:N) {\n",
        "    log_sq_foot[n] = normal_rng(1.5, .1);\n",
        "    live_in_super[n] = bernoulli_rng(0.5);\n",
        "    traps[n] = poisson_rng(8);\n",
        "    complaints[n] = poisson_log_rng(alpha + log_sq_foot[n] \n",
        "                               + beta * traps[n] \n",
        "                               + beta_super * live_in_super[n]);\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "multiple_poisson_regression_dgp_model = StanModel('../stan_models/multiple_poisson_regression_dgp', multiple_poisson_regression_dgp)"
      ],
      "id": "f689bc9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit / sample the prior predictive check model:\n"
      ],
      "id": "87b36be6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_dgp = multiple_poisson_regression_dgp_model.sample(\n",
        "    data={'N': len(pest_data.traps)},\n",
        "    chains = 1,\n",
        "    iter_sampling = 1,\n",
        "    show_progress = False,\n",
        "    fixed_param = True\n",
        ")\n",
        "\n",
        "samps_dgp = az.from_cmdstanpy(fitted_model_dgp)"
      ],
      "id": "3e2c731b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now pop that simulated data into a dictionary ready for Stan.\n"
      ],
      "id": "615ea581"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_dat_fake = {\n",
        "    'N': len(pest_data.traps),\n",
        "    'complaints': samps_dgp.posterior.complaints.sel(chain=0, draw=0).to_numpy().astype(int),\n",
        "    'traps': samps_dgp.posterior.traps.sel(chain=0, draw=0),\n",
        "    'live_in_super': samps_dgp.posterior.live_in_super.sel(chain=0, draw=0),\n",
        "    'log_sq_foot': samps_dgp.posterior.log_sq_foot.sel(chain=0, draw=0)\n",
        "}"
      ],
      "id": "29b49432",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then compile and fit the model we wrote for the multiple regression.\n"
      ],
      "id": "6783df83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fit_model_P_mult = multiple_poisson_regression_model.sample(data=stan_dat_fake, show_progress=False)\n",
        "posterior_alpha_beta = az.extract(az.from_cmdstanpy(fit_model_P_mult), 'posterior', var_names=['alpha', 'beta', 'beta_super']).to_dataframe()[['alpha', 'beta', 'beta_super']]"
      ],
      "id": "1a2bd9b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then compare these parameters to the true parameters:\n"
      ],
      "id": "dcfd74d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "true_alpha = samps_dgp.posterior.alpha.sel(chain=0, draw=0).to_numpy()\n",
        "true_beta = samps_dgp.posterior.beta.sel(chain=0, draw=0).to_numpy()\n",
        "true_beta_super = samps_dgp.posterior.beta_super.sel(chain=0, draw=0).to_numpy()\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots(1,3, figsize=(8,4))\n",
        "sns.histplot(posterior_alpha_beta, x='alpha', bins=30, ax=ax[0])\n",
        "ax[0].axvline(true_alpha, color='r', linewidth=2)\n",
        "sns.histplot(posterior_alpha_beta, x='beta', bins=30, ax=ax[1])\n",
        "ax[1].axvline(true_beta, color='r', linewidth=2)\n",
        "sns.histplot(posterior_alpha_beta, x='beta_super', bins=30, ax=ax[2])\n",
        "ax[2].axvline(true_beta_super, color='r', linewidth=2)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "806ca306",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've recovered the parameters sufficiently well, so we've probably coded the Stan program correctly and we're ready to fit the real data.\n",
        "\n",
        "### Fit the real data\n",
        "\n",
        "Now let's use the real data and explore the fit.\n"
      ],
      "id": "2b959ec9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fit_model_P_mult_real = multiple_poisson_regression_model.sample(data=stan_dat_simple, show_progress=False)\n",
        "az_fit_P_mult_real_data = az.from_cmdstanpy(fit_model_P_mult_real, posterior_predictive='y_rep')\n",
        "y_rep = az_fit_P_mult_real_data.posterior_predictive.y_rep"
      ],
      "id": "d9948917",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_simple['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,25)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "355c03c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This again looks like we haven't captured the smaller counts very well, nor have we captured the larger counts.\n"
      ],
      "id": "28454a60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.histplot(np.mean(y_rep == 0, axis=2).to_numpy().flatten(), bins=20, ax=ax)\n",
        "ax.axvline(np.mean(stan_dat_simple['complaints'] == 0), linewidth=2, color='r')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "c26c4b8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're still severely underestimating the proportion of zeros in the data. Ideally this vertical line would fall somewhere within the histogram.\n",
        "\n",
        "We can also plot uncertainty intervals for the predicted complaints for different numbers of traps.\n"
      ],
      "id": "a9390c3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(stan_dat_simple['traps'], stan_dat_simple['complaints'], 'o', fillstyle='none')\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, trap in enumerate(stan_dat_simple['traps']):\n",
        "    grouped_indices[trap].append(idx)\n",
        "\n",
        "grouped_array = {trap: y_rep[:, :, idx_list].to_numpy().flatten() for trap, idx_list in grouped_indices.items()}\n",
        "\n",
        "for traps_x, complaints_y in grouped_array.items():\n",
        "    inner_prob = np.array([np.quantile(complaints_y, 0.75), np.quantile(complaints_y, 0.25)])  # az.hdi(complaints_y, hdi_prob=0.5)\n",
        "    outer_prob = np.array([np.quantile(complaints_y, 0.95), np.quantile(complaints_y, 0.05)])  # az.hdi(complaints_y, hdi_prob=0.9)\n",
        "    ax.plot(np.full(inner_prob.shape, traps_x), inner_prob, 'k', linewidth=2)\n",
        "    ax.plot(np.full(outer_prob.shape, traps_x), outer_prob, 'k', linewidth=0.5)\n",
        "    ax.plot(traps_x, np.mean(complaints_y), 'ko')\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "fb5a6a6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we've increased the tails a bit more at the larger numbers of traps but we still have some large observed numbers of complaints that the model would consider extremely unlikely events.\n",
        "\n",
        "## Modeling count data with the Negative Binomial\n",
        "\n",
        "When we considered modelling the data using a Poisson, we saw that the model didn't appear to fit as well to the data as we would like. In particular the model under-predicted low and high numbers of complaints, and over-predicted the medium number of complaints. This is one indication of **over-dispersion**, where the variance is larger than the mean. A Poisson model doesn't fit over-dispersed count data very well because the same parameter $\\lambda$, controls both the expected counts and the variance of these counts. The natural alternative to this is the **negative binomial model**:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{complaints}_{b,t} & \\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi) \\\\\n",
        "\\lambda_{b,t} & = \\exp{(\\eta_{b,t})} \\\\\n",
        "\\eta_{b,t} &= \\alpha + \\beta \\, {\\rm traps}_{b,t} + \\beta_{\\rm super} \\, {\\rm super}_{b} + \\text{log_sq_foot}_{b}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "In Stan the negative binomial mass function we'll use is called $\\texttt{neg_binomial_2_log}(\\text{ints} \\, y, \\text{reals} \\, \\eta, \\text{reals} \\, \\phi)$ in Stan. Like the `poisson_log` function, this negative binomial mass function that is parameterized in terms of its log-mean, $\\eta$, but it also has a precision $\\phi$ such that\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[y] \\, = \\lambda = \\exp(\\eta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}[y] = \\lambda + \\lambda^2/\\phi = \\exp(\\eta) + \\exp(\\eta)^2 / \\phi.\n",
        "$$\n",
        "\n",
        "As $\\phi$ gets larger the term $\\lambda^2 / \\phi$ approaches zero and so the variance of the negative-binomial approaches $\\lambda$, i.e., the negative-binomial gets closer and closer to the Poisson.\n",
        "\n",
        "### Stan program for negative-binomial regression\n"
      ],
      "id": "086f124e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "multiple_NB_regression = '''\n",
        "functions {\n",
        "    /*\n",
        "    * Alternative to neg_binomial_2_log_rng() that \n",
        "    * avoids potential numerical problems during warmup\n",
        "    */\n",
        "    int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "    }\n",
        "}\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "    vector<lower=0>[N] traps;\n",
        "    vector<lower=0,upper=1>[N] live_in_super;\n",
        "    vector[N] log_sq_foot;\n",
        "    array[N] int<lower=0> complaints;\n",
        "}\n",
        "parameters {\n",
        "    real alpha;\n",
        "    real beta;\n",
        "    real beta_super;\n",
        "    real<lower=0> inv_phi;  // Enforce positivity\n",
        "}\n",
        "transformed parameters {\n",
        "    real phi = inv(inv_phi);\n",
        "}\n",
        "model {\n",
        "    alpha ~ normal(log(4), 1);\n",
        "    beta ~ normal(-0.25, 1);\n",
        "    beta_super ~ normal(-0.5, 1);\n",
        "    inv_phi ~ normal(0, 1); // Alternative: normal(0.1, 0.1) // Alternative: exponential(1)\n",
        "    \n",
        "    complaints ~ neg_binomial_2_log(alpha + beta * traps + beta_super * live_in_super\n",
        "                                  + log_sq_foot, phi);\n",
        "} \n",
        "generated quantities {\n",
        "    array[N] int y_rep;\n",
        "    for (n in 1:N) \n",
        "    y_rep[n] = neg_binomial_2_log_safe_rng(alpha + beta * traps[n] + beta_super * live_in_super[n]\n",
        "                                       + log_sq_foot[n], phi);\n",
        "}\n",
        "'''\n",
        "\n",
        "multiple_NB_regression_model = StanModel('../stan_models/multiple_NB_regression', multiple_NB_regression)"
      ],
      "id": "7307918f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fake data fit: Multiple NB regression\n"
      ],
      "id": "59cfb66c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "multiple_NB_regression_dgp = '''\n",
        "data {\n",
        "    int<lower=1> N;\n",
        "}\n",
        "model {\n",
        "} \n",
        "generated quantities {\n",
        "    vector[N] log_sq_foot;\n",
        "    array[N] int live_in_super;\n",
        "    array[N] int traps;\n",
        "    array[N] int complaints;\n",
        "    real alpha = normal_rng(log(4), .1);\n",
        "    real beta = normal_rng(-0.25, .1);\n",
        "    real beta_super = normal_rng(-0.5, .1);\n",
        "    real inv_phi = abs(normal_rng(0,1));\n",
        "    \n",
        "    for (n in 1:N) {\n",
        "    log_sq_foot[n] = normal_rng(1.5, .1);\n",
        "    live_in_super[n] = bernoulli_rng(0.5);\n",
        "    traps[n] = poisson_rng(8);\n",
        "    complaints[n] = neg_binomial_2_log_rng(alpha + log_sq_foot[n] \n",
        "                               + beta * traps[n] \n",
        "                               + beta_super * live_in_super[n], inv(inv_phi));\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "multiple_NB_regression_dpg_model = StanModel('../stan_models/multiple_NB_regression_dgp', multiple_NB_regression_dgp)"
      ],
      "id": "46ae8f02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to generate one draw from the fake data model so we can use the data to fit our model and compare the known values of the parameters to the posterior density of the parameters.\n"
      ],
      "id": "13746ae8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_dgp_NB = multiple_NB_regression_dpg_model.sample(\n",
        "    data={'N': len(pest_data.traps)},\n",
        "    chains = 1,\n",
        "    iter_sampling = 1,\n",
        "    show_progress = False,\n",
        "    fixed_param=True\n",
        ")\n",
        "\n",
        "samps_dgp_NB = az.from_cmdstanpy(fitted_model_dgp_NB)"
      ],
      "id": "25e07214",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary to feed into the Stan model.\n"
      ],
      "id": "66922b7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_dat_fake_NB = {\n",
        "    'N': len(pest_data.traps),\n",
        "    'traps': samps_dgp_NB.posterior.traps.sel(chain=0, draw=0),\n",
        "    'complaints': samps_dgp_NB.posterior.complaints.sel(chain=0, draw=0).to_numpy().astype(int),\n",
        "    'live_in_super': samps_dgp_NB.posterior.live_in_super.sel(chain=0, draw=0),\n",
        "    'log_sq_foot': samps_dgp_NB.posterior.log_sq_foot.sel(chain=0, draw=0)\n",
        "}"
      ],
      "id": "16be148e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run our NB regression over the fake data and extract the samples to examine posterior predictive checks and to check whether we've sufficiently recovered our known parameters, $\\text{alpha}$ $\\texttt{beta}$.\n"
      ],
      "id": "6d76f86d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_NB = multiple_NB_regression_model.sample(data=stan_dat_fake_NB, show_progress=False)\n",
        "posterior_alpha_beta_NB = (\n",
        "    az.extract(\n",
        "        az.from_cmdstanpy(fitted_model_NB), 'posterior'\n",
        "    ).to_dataframe()[['alpha', 'beta', 'beta_super', 'inv_phi']]\n",
        ")"
      ],
      "id": "60ef6362",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construct the vector of true values from your simulated dataset and compare to the recovered parameters.\n"
      ],
      "id": "6f040a9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "true_params = ['alpha', 'beta', 'beta_super', 'inv_phi']\n",
        "\n",
        "plt.clf()\n",
        "_, axes = plt.subplots(2,2, figsize=(6,4))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    sns.histplot(posterior_alpha_beta_NB, x=true_params[i], bins=30, ax=ax)\n",
        "    ax.axvline(samps_dgp_NB.posterior[true_params[i]].sel(chain=0, draw=0).to_numpy(), color='r', linewidth=2)\n",
        "    ax.set(ylabel='')\n",
        "    ax.yaxis.set_ticks([])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "d6c8fbba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit to real data and check the fit\n"
      ],
      "id": "6c285fc6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_NB = multiple_NB_regression_model.sample(data=stan_dat_simple, show_progress=False)\n",
        "az_fitted_model_NB_data = az.from_cmdstanpy(fitted_model_NB, posterior_predictive='y_rep')\n",
        "y_rep = az_fitted_model_NB_data.posterior_predictive.y_rep"
      ],
      "id": "334f2a96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at our predictions vs. the data.\n"
      ],
      "id": "97646e2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_simple['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "0a68f526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears that our model now captures both the number of small counts better as well as the tails.\n",
        "\n",
        "Let's check if the negative binomial model does a better job capturing the number of zeros:\n"
      ],
      "id": "f017bcff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.histplot(np.mean(y_rep == 0, axis=2).to_numpy().flatten(), bins=25, ax=ax)\n",
        "ax.axvline(np.mean(stan_dat_simple['complaints'] == 0), linewidth=2, color='r')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "b38124b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These look OK, but let's look at the standardized residual plot.\n"
      ],
      "id": "06598983"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_inv_phi = az_fitted_model_NB_data.posterior.inv_phi.mean(dim=['chain', 'draw'])\n",
        "mean_y_rep = y_rep.mean(dim=['chain', 'draw'])\n",
        "std_resid = (stan_dat_simple['complaints'] - mean_y_rep) / np.sqrt(mean_y_rep + mean_y_rep**2 * mean_inv_phi)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(mean_y_rep, std_resid, 'o', alpha=0.5)\n",
        "plt.axhline(2, linestyle='--')\n",
        "plt.axhline(-2, linestyle='--')\n",
        "plt.xlabel('mean_y_rep')\n",
        "plt.ylabel('std_resid')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e8cfe4a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks OK, but we still have some very large *standardized* residuals. This might be because we are currently ignoring that the data are clustered by buildings, and that the probability of roach issue may vary substantially across buildings.\n"
      ],
      "id": "f22589c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "vals, counts = np.unique(y_rep, return_counts=True)\n",
        "hist_range = (min(vals), max(vals))\n",
        "az.plot_dist(stan_dat_simple['complaints'], kind='hist', ax=ax, \n",
        "             hist_kwargs={'bins': np.arange(hist_range[0], hist_range[1] + 1, 1), 'range': hist_range})\n",
        "\n",
        "secax = ax.twinx()\n",
        "secax.plot(vals, counts, color='red', linewidth=1.5)\n",
        "\n",
        "ax.set_xlim(hist_range[0]-0.5, hist_range[1]+0.5)\n",
        "\n",
        "secax.set_yticklabels([])\n",
        "secax.yaxis.set_ticks([])\n",
        "\n",
        "ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "c1a92635",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rootogram now looks much more plausible. We can tell this because now the expected number of complaints matches much closer to the observed number of complaints. However, we still have some larger counts that appear to be outliers for the model.\n",
        "\n",
        "Check predictions by number of traps:\n"
      ],
      "id": "d4f61b1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(stan_dat_simple['traps'], stan_dat_simple['complaints'], 'o', fillstyle='none')\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, trap in enumerate(stan_dat_simple['traps']):\n",
        "    grouped_indices[trap].append(idx)\n",
        "\n",
        "grouped_array = {trap: y_rep[:, :, idx_list].to_numpy().flatten() for trap, idx_list in grouped_indices.items()}\n",
        "\n",
        "for traps_x, complaints_y in grouped_array.items():\n",
        "    inner_prob = np.array([np.quantile(complaints_y, 0.75), np.quantile(complaints_y, 0.25)])  # az.hdi(complaints_y, hdi_prob=0.5)\n",
        "    outer_prob = np.array([np.quantile(complaints_y, 0.95), np.quantile(complaints_y, 0.05)])  # az.hdi(complaints_y, hdi_prob=0.9)\n",
        "    ax.plot(np.full(inner_prob.shape, traps_x), inner_prob, 'k', linewidth=2)\n",
        "    ax.plot(np.full(outer_prob.shape, traps_x), outer_prob, 'k', linewidth=0.5)\n",
        "    ax.plot(traps_x, np.mean(complaints_y), 'ko')\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "417e6063",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We haven't used the fact that the data are clustered by building yet. A posterior predictive check might elucidate whether it would be a good idea to add the building information into the model.\n"
      ],
      "id": "d7308c50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, building_id in enumerate(pest_data['building_id']):\n",
        "    grouped_indices[building_id].append(idx)\n",
        "\n",
        "grouped_array = {bldg_id: y_rep[:, :, idx_list].mean(axis=2).to_numpy().flatten() for bldg_id, idx_list in grouped_indices.items()}\n",
        "\n",
        "_, axes = plt.subplots(2, 5, figsize=(7,4))\n",
        "\n",
        "for ax, key in zip(axes.flat, sorted(grouped_array.keys())):\n",
        "    sns.histplot(grouped_array[key], bins=30, ax=ax)\n",
        "    ax.axvline(pest_data[pest_data['building_id']==key].complaints.mean(), color='r', linewidth=2)\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.set(ylabel='', title=f\"Bulding = {key}\")\n",
        "    ax.yaxis.set_ticks([])\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "2e4596ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're getting plausible predictions for most building means but some are estimated better than others and some have larger uncertainties than we might expect. If we explicitly model the variation across buildings we may be able to get much better estimates.\n",
        "\n",
        "### Garbage Collect\n"
      ],
      "id": "9f97671f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exceptions = ['pest_data', 'exceptions', 'active_variables']\n",
        "\n",
        "active_variables = [\n",
        "    var for var, value in globals().items()\n",
        "    if not var.startswith('_')   # Exclude variables that start with \"_\"\n",
        "    and var not in exceptions    # Exclude variables in the exceptions list\n",
        "    and isinstance(value, (CmdStanModel, cmdstanpy.CmdStanMCMC, plt.Axes, \n",
        "                           az.InferenceData, pd.DataFrame, pd.Series, \n",
        "                           dict, list, int, float, str, tuple, plt.Figure, defaultdict,\n",
        "                           np.ndarray, np.int64, np.float32))  # Remove these types only\n",
        "]\n",
        "\n",
        "for var in active_variables:\n",
        "    del globals()[var]\n",
        "del active_variables, exceptions, var, _, g, y_rep, mean_inv_phi, mean_y_rep\n",
        "\n",
        "gc.collect()"
      ],
      "id": "207ea3de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical modeling\n",
        "\n",
        "### Modeling varying intercepts for each building\n",
        "\n",
        "Let's add a hierarchical intercept parameter, $\\alpha_b$ at the building level to our model.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{complaints}_{b,t} &\\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi) \\\\\n",
        "\\lambda_{b,t}  &= \\exp{(\\eta_{b,t})} \\\\\n",
        "\\eta_{b,t} &= \\mu_b + \\beta \\, {\\rm traps}_{b,t} + \\beta_{\\rm super}\\, {\\rm super}_b + \\text{log_sq_foot}_b \\\\\n",
        "\\mu_b &\\sim \\text{Normal}(\\alpha, \\sigma_{\\mu})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "In our Stan model, $\\mu_b$ is the $b$-th element of the vector $\\texttt{mu}$ which has one element per building.\n",
        "\n",
        "One of our predictors varies only by building, so we can rewrite the above model more efficiently like so:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\eta_{b,t} &= \\mu_b + \\beta \\, {\\rm traps}_{b,t} + \\text{log_sq_foot}_b\\\\\n",
        "\\mu_b &\\sim \\text{Normal}(\\alpha +  \\beta_{\\text{super}} \\, \\text{super}_b , \\sigma_{\\mu})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We have more information at the building level as well, like the average age of the residents, the average age of the buildings, and the average per-apartment monthly rent so we can add that data into a matrix called `building_data`, which will have one row per building and four columns:\n",
        "\n",
        "-   `live_in_super`\n",
        "-   `age_of_building`\n",
        "-   `average_tentant_age`\n",
        "-   `monthly_average_rent`\n",
        "\n",
        "We'll write the Stan model like:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\eta_{b,t} &= \\alpha_b + \\beta \\, {\\rm traps} + \\text{log_sq_foot}\\\\\n",
        "\\mu &\\sim \\text{Normal}(\\alpha + \\texttt{building_data} \\, \\zeta, \\,\\sigma_{\\mu})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Prepare building data for hierarchical\n",
        "\n",
        "We'll need to do some more data prep before we can fit our models. Firstly to use the building variable in Stan we will need to transform it from a factor variable to an integer variable.\n"
      ],
      "id": "2e4ea0b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_months = len(pest_data['date'].unique())\n",
        "N_buildings = len(pest_data['building_id'].unique())\n",
        "\n",
        "# Add some IDs for building and month\n",
        "pest_data['building_fac'] = pd.Categorical(pest_data['building_id'], categories=pest_data['building_id'].unique())\n",
        "pest_data['building_idx'] = pest_data['building_fac'].cat.codes + 1  # Convert to integer index (1-based)\n",
        "pest_data['ids'] = list(range(1, N_months + 1)) * N_buildings\n",
        "pest_data['mo_idx'] = pest_data['date'].dt.month  # Extract month from date column\n",
        "\n",
        "# Center and rescale the building specific data\n",
        "building_data = pest_data[[\n",
        "    'building_idx',\n",
        "    'live_in_super',\n",
        "    'age_of_building',\n",
        "    'total_sq_foot',\n",
        "    'average_tenant_age',\n",
        "    'monthly_average_rent'\n",
        "]].drop_duplicates().sort_values('building_idx').drop(columns=['building_idx'])\n",
        "\n",
        "# Centering without scaling (subtract mean but don't divide by std)\n",
        "building_data = building_data - building_data.mean()\n",
        "\n",
        "# Scale by constants\n",
        "building_data['age_of_building'] /= 10\n",
        "building_data['total_sq_foot'] /= 10000\n",
        "building_data['average_tenant_age'] /= 10\n",
        "building_data['monthly_average_rent'] /= 1000\n",
        "\n",
        "# Convert to NumPy matrix\n",
        "building_data_matrix = building_data.to_numpy()\n",
        "\n",
        "# Make data dictionary for Stan\n",
        "stan_dat_hier = {\n",
        "  'complaints': pest_data.complaints,\n",
        "  'traps': pest_data.traps,\n",
        "  'N': len(pest_data.traps),\n",
        "  'J': N_buildings,\n",
        "  'M': N_months,\n",
        "  'log_sq_foot': np.log(pest_data.total_sq_foot/1e4),\n",
        "  'building_data': np.delete(building_data_matrix, 2, axis=1),\n",
        "  'mo_idx': pest_data.mo_idx,\n",
        "  'K': 4,\n",
        "  'building_idx': pest_data.building_idx\n",
        "  }"
      ],
      "id": "0ecca245",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile and fit the hierarchical model\n",
        "\n",
        "Let's code up and compile the model.\n"
      ],
      "id": "cb4a94cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "hier_NB_regression = '''\n",
        "functions {\n",
        "  /*\n",
        "  * Alternative to neg_binomial_2_log_rng() that \n",
        "  * avoids potential numerical problems during warmup\n",
        "  */\n",
        "  int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "  int<lower=1> N;                     \n",
        "  array[N] int<lower=0> complaints;\n",
        "  vector<lower=0>[N] traps;\n",
        "  \n",
        "  // 'exposure'\n",
        "  vector[N] log_sq_foot;  \n",
        "  \n",
        "  // building-level data\n",
        "  int<lower=1> K;\n",
        "  int<lower=1> J;\n",
        "  array[N] int <lower=1, upper=J> building_idx;\n",
        "  matrix[J,K] building_data;\n",
        "}\n",
        "parameters {\n",
        "  real<lower=0> inv_phi;   // 1/phi (easier to think about prior for 1/phi instead of phi)\n",
        "  real beta;               // coefficient on traps\n",
        "  \n",
        "  vector[J] mu;            // buildings-specific intercepts\n",
        "  real<lower=0> sigma_mu;  // sd of building-specific intercepts\n",
        "  real alpha;              // intercept of model for mu\n",
        "  vector[K] zeta;          // coefficients on building-level predictors in model for mu \n",
        "}\n",
        "transformed parameters {\n",
        "  real phi = inv(inv_phi);\n",
        "}\n",
        "model {\n",
        "  mu ~ normal(alpha + building_data * zeta, sigma_mu);\n",
        "  sigma_mu ~ normal(0, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  zeta ~ normal(0, 1);  // could also use informative priors on the different elements\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  inv_phi ~ normal(0, 1);\n",
        "  \n",
        "  complaints ~ neg_binomial_2_log(mu[building_idx] + beta * traps + log_sq_foot, phi);\n",
        "} \n",
        "generated quantities {\n",
        "  array[N] int y_rep;\n",
        "  for (n in 1:N) {\n",
        "    real eta_n = mu[building_idx[n]] + beta * traps[n] + log_sq_foot[n];\n",
        "    y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi);\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "hier_NB_regression_model = StanModel('../stan_models/hier_NB_regression', hier_NB_regression)"
      ],
      "id": "8cb45d34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model to data.\n"
      ],
      "id": "29b76d87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_NB_hier = hier_NB_regression_model.sample(data=stan_dat_hier, \n",
        "                                                       chains=4, \n",
        "                                                       parallel_chains=4,\n",
        "                                                       iter_sampling=4000, \n",
        "                                                       seed=[1034992638, 1034992638, 1034992638, 1034992638],\n",
        "                                                       show_progress=False)\n",
        "az_samps_hier_NB = az.from_cmdstanpy(fitted_model_NB_hier, posterior_predictive='y_rep', observed_data=stan_dat_hier)"
      ],
      "id": "b9a5cd42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostics\n",
        "\n",
        "We get a bunch of warnings from Stan about divergent transitions, which is an indication that there may be regions of the posterior that have not been explored by the Markov chains.\n",
        "\n",
        "Divergences are discussed in more detail in the course slides as well as the **bayesplot** [MCMC diagnostics vignette](http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html) and [*A Conceptual Introduction to Hamiltonian Monte Carlo*](https://arxiv.org/abs/1701.02434).\n",
        "\n",
        "In this example we will see that we have divergent transitions because we need to reparameterize our model - i.e., we will retain the overall structure of the model, but transform some of the parameters so that it is easier for Stan to sample from the parameter space. Before we go through exactly how to do this reparameterization, we will first go through what indicates that this is something that reparameterization will resolve. We will go through:\n",
        "\n",
        "1.  Examining the fitted parameter values, including the effective sample size\n",
        "2.  Traceplots and scatterplots that reveal particular patterns in locations of the divergences.\n",
        "\n",
        "Let us first run the `diagnose` and `summary` method on the model object `fitted_model_NB_hier`:\n"
      ],
      "id": "638b7624"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(fitted_model_NB_hier.diagnose())"
      ],
      "id": "b15cd758",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This reports that 32% of the samples ended with a divergence.\n",
        "\n",
        "Now, let's extract the fits from the model.\n"
      ],
      "id": "394759a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = fitted_model_NB_hier.summary(percentiles=(5, 95)).round(2)\n",
        "summary[~summary.index.str.contains('y_rep')]"
      ],
      "id": "5b6647a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the effective samples are quite low for many of the parameters relative to the total number of samples. This alone isn't indicative of the need to reparameterize, but it indicates that we should look further at the trace plots and pairs plots. First let's look at the traceplots to see if the divergent transitions form a pattern.\n"
      ],
      "id": "41786753"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "az.plot_trace(az_samps_hier_NB, var_names='sigma_mu', divergences='bottom')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "9cea6ec9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks as if the divergent parameters, the the black bars underneath the traceplots correspond to samples where the sampler gets stuck at one parameter value for $\\sigma_\\mu$.\n"
      ],
      "id": "1636a73b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_mu = az_samps_hier_NB.posterior.mu.sel(mu_dim_0=3).to_numpy().flatten()\n",
        "plot_sigma_mu = np.log(az_samps_hier_NB.posterior.sigma_mu.to_numpy().flatten())\n",
        "divergent_filter = az_samps_hier_NB.sample_stats.diverging.to_numpy().flatten()\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.scatterplot(x=plot_mu, y=plot_sigma_mu, alpha=0.2, ax=ax)\n",
        "sns.scatterplot(x=plot_mu[divergent_filter], y=plot_sigma_mu[divergent_filter], color='r', alpha=0.5, ax=ax)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "4a32143c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we have here is a cloud-like shape, with most of the divergences clustering towards the bottom. We'll see a bit later that we actually want this to look more like a funnel than a cloud, but the divergences are indicating that the sampler can't explore the narrowing neck of the funnel.\n",
        "\n",
        "One way to see why we should expect some version of a funnel is to look at some simulations from the prior, which we can do without MCMC and thus with no risk of sampling problems:\n"
      ],
      "id": "87242fb2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_sims = 1000\n",
        "log_sigma = np.random.normal(loc=0, scale=1, size=N_sims)\n",
        "theta = np.random.normal(loc=0, scale=np.exp(log_sigma))\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.scatterplot(x=theta, y=log_sigma, alpha=0.2, ax=ax)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "8efedbe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, if the data is at all informative we shouldn't expect the posterior to look exactly like the prior. But unless the data is incredibly informative about the parameters and the posterior concentrates away from the narrow neck of the funnel, the sampler is going to have to confront the funnel geometry. (See the [Visual MCMC Diagnostics](http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html) vignette for more on this.)\n",
        "\n",
        "Another way to look at the divergences is via a parallel coordinates plot:\n"
      ],
      "id": "ff09bef7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "param_mat = np.zeros((11, 4000*4))\n",
        "param_mat[0,:] = az_samps_hier_NB.posterior.sigma_mu.to_numpy().flatten()\n",
        "for i in range(10):\n",
        "    param_mat[i+1,:] = az_samps_hier_NB.posterior.mu.sel(mu_dim_0=i).to_numpy().flatten()\n",
        "\n",
        "divergent_mat = np.zeros((11, np.sum(divergent_filter)))\n",
        "for i in range(11):\n",
        "    divergent_mat[i,:] = param_mat[i, divergent_filter]\n",
        "\n",
        "custom_labels = [\"sigma_mu\", \"mu[1]\", \"mu[2]\", \"mu[3]\", \"mu[4]\", \"mu[5]\",\n",
        "                 \"mu[6]\", \"mu[7]\", \"mu[8]\", \"mu[9]\", \"mu[10]\"]\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(param_mat, 'k', alpha=0.015)\n",
        "plt.plot(divergent_mat, 'r', alpha=0.025)\n",
        "plt.xlim(0,10)\n",
        "plt.xticks(np.arange(11), custom_labels)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "a31d24f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we see evidence that our problems concentrate when $\\texttt{sigma_mu}$ is small.\n",
        "\n",
        "### Reparameterize and recheck diagnostics\n",
        "\n",
        "Instead, we should use the non-centered parameterization for $\\mu_b$. We define a vector of auxiliary variables in the parameters block, $\\texttt{mu_raw}$ that is given a $\\text{Normal}(0, 1)$ prior in the model block. We then make $\\texttt{mu}$ a transformed parameter:\n",
        "\n",
        "We can reparameterize the random intercept $\\mu_b$, which is distributed:\n",
        "\n",
        "$$\n",
        "\\mu_b \\sim \\text{Normal}(\\alpha + \\texttt{building_data} \\, \\zeta, \\sigma_{\\mu})\n",
        "$$\n",
        "\n",
        "```         \n",
        "transformed parameters {\n",
        "  vector[J] mu;\n",
        "  mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "}\n",
        "```\n",
        "\n",
        "This gives $\\texttt{mu}$ a $\\text{Normal}(\\alpha + \\texttt{building_data}\\, \\zeta, \\sigma_\\mu)$ distribution, but it decouples the dependence of the density of each element of $\\texttt{mu}$ from $\\texttt{sigma_mu}$ ($\\sigma_\\mu$). `hier_NB_regression_ncp` uses the non-centered parameterization for $\\texttt{mu}$. We will examine the effective sample size of the fitted model to see whether we've fixed the problem with our reparameterization.\n",
        "\n",
        "Compile the model.\n"
      ],
      "id": "a7550c38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "hier_NB_regression_ncp = '''\n",
        "functions {\n",
        "    /*\n",
        "    * Alternative to neg_binomial_2_log_rng() that \n",
        "    * avoids potential numerical problems during warmup\n",
        "    */\n",
        "    int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "    }\n",
        "}\n",
        "data {\n",
        "    int<lower=1> N;                     \n",
        "    array[N] int<lower=0> complaints;\n",
        "    vector<lower=0>[N] traps;\n",
        "    \n",
        "    // 'exposure'\n",
        "    vector[N] log_sq_foot;\n",
        "    \n",
        "    // building-level data\n",
        "    int<lower=1> K;\n",
        "    int<lower=1> J;\n",
        "    array[N] int<lower=1, upper=J> building_idx;\n",
        "    matrix[J,K] building_data;\n",
        "}\n",
        "parameters {\n",
        "    real<lower=0> inv_phi;   // 1/phi (easier to think about prior for 1/phi instead of phi)\n",
        "    real beta;               // coefficient on traps\n",
        "    \n",
        "    vector[J] mu_raw;        // N(0,1) params for non-centered parameterization of building intercepts \n",
        "    real<lower=0> sigma_mu;  // sd of building-specific intercepts\n",
        "    real alpha;              // intercept of model for mu\n",
        "    vector[K] zeta;          // coefficients on building-level predictors in model for mu \n",
        "}\n",
        "transformed parameters {\n",
        "    real phi = inv(inv_phi);\n",
        "    \n",
        "    // non-centered parameterization\n",
        "    vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "}\n",
        "model {\n",
        "    mu_raw ~ normal(0, 1);   // implies mu ~ normal(alpha + building_data * zeta, sigma_mu)\n",
        "    sigma_mu ~ normal(0, 1);\n",
        "    alpha ~ normal(log(4), 1);\n",
        "    zeta ~ normal(0, 1);\n",
        "    beta ~ normal(-0.25, 1);\n",
        "    inv_phi ~ normal(0, 1);\n",
        "    \n",
        "    complaints ~ neg_binomial_2_log(mu[building_idx] + beta * traps + log_sq_foot, phi);\n",
        "} \n",
        "generated quantities {\n",
        "    array[N] int y_rep;\n",
        "    for (n in 1:N) {\n",
        "        real eta_n = mu[building_idx[n]] + beta * traps[n] + log_sq_foot[n];\n",
        "        y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi);\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "hier_NB_regression_ncp_model = StanModel('../stan_models/hier_NB_regression_ncp', hier_NB_regression_ncp)"
      ],
      "id": "f3d0bb4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model to the data.\n"
      ],
      "id": "f4a6bb1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "fitted_model_NB_hier_ncp = hier_NB_regression_ncp_model.sample(\n",
        "    data=stan_dat_hier,\n",
        "    chains=4,\n",
        "    parallel_chains=4,\n",
        "    adapt_delta=0.95,\n",
        "    show_progress=False\n",
        ")\n",
        "az_samps_hier_NB_hier_ncp = az.from_cmdstanpy(fitted_model_NB_hier_ncp, posterior_predictive='y_rep', observed_data=stan_dat_hier)"
      ],
      "id": "500fc1dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examining the fit of the new model\n"
      ],
      "id": "01bf928e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(fitted_model_NB_hier_ncp.diagnose())"
      ],
      "id": "d0549d20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "summary = fitted_model_NB_hier.summary(percentiles=(5, 95)).round(2)\n",
        "summary[~summary.index.str.contains(r'^(y_rep|zeta)', regex=True)]"
      ],
      "id": "4aef6118",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has improved the effective sample sizes of $\\texttt{mu}$. We extract the parameters to run our usual posterior predictive checks.\n"
      ],
      "id": "5462b8e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_mu = az_samps_hier_NB_hier_ncp.posterior.mu.sel(mu_dim_0=3).to_numpy().flatten()\n",
        "plot_sigma_mu = np.log(az_samps_hier_NB_hier_ncp.posterior.sigma_mu.to_numpy().flatten())\n",
        "divergent_filter = az_samps_hier_NB_hier_ncp.sample_stats.diverging.to_numpy().flatten()\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.scatterplot(x=plot_mu, y=plot_sigma_mu, alpha=0.2, ax=ax)\n",
        "sns.scatterplot(x=plot_mu[divergent_filter], y=plot_sigma_mu[divergent_filter], color='r', alpha=0.5, ax=ax)\n",
        "ax.set(xlabel='mu[4]', ylabel='log(sigma_mu)')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "8627509e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "az.plot_parallel(az_samps_hier_NB_hier_ncp, \n",
        "                 var_names=['sigma_mu', 'mu'])\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e19887fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_rep = az_samps_hier_NB_hier_ncp.posterior_predictive.y_rep\n",
        "\n",
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_hier['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e7ba2a44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks quite nice. If we've captured the building-level means well, then the posterior distribution of means by building should match well with the observed means of the quantity of building complaints by month.\n"
      ],
      "id": "3b32a676"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, building_id in enumerate(pest_data['building_id']):\n",
        "    grouped_indices[building_id].append(idx)\n",
        "\n",
        "grouped_array = {bldg_id: y_rep[:, :, idx_list].mean(axis=2).to_numpy().flatten() for bldg_id, idx_list in grouped_indices.items()}\n",
        "\n",
        "_, axes = plt.subplots(2, 5, figsize=(7,4))\n",
        "\n",
        "for ax, key in zip(axes.flat, sorted(grouped_array.keys())):\n",
        "    sns.histplot(grouped_array[key], bins=30, ax=ax)\n",
        "    ax.axvline(pest_data[pest_data['building_id']==key].complaints.mean(), color='r', linewidth=2)\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.set(ylabel='', title=f\"Bulding = {key}\")\n",
        "    ax.yaxis.set_ticks([])\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e7e8ae25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We weren't doing terribly with the building-specific means before, but now they are all well-captured by our model. The model is also able to do a decent job estimating within-building variability:\n"
      ],
      "id": "067c4e3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, building_id in enumerate(pest_data['building_id']):\n",
        "    grouped_indices[building_id].append(idx)\n",
        "\n",
        "grouped_array = {bldg_id: y_rep[:, :, idx_list].std(axis=2).to_numpy().flatten() for bldg_id, idx_list in grouped_indices.items()}\n",
        "\n",
        "_, axes = plt.subplots(2, 5, figsize=(7,4))\n",
        "\n",
        "for ax, key in zip(axes.flat, sorted(grouped_array.keys())):\n",
        "    sns.histplot(grouped_array[key], bins=30, ax=ax)\n",
        "    ax.axvline(pest_data[pest_data['building_id']==key].complaints.std(), color='r', linewidth=2)\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.set(ylabel='', title=f\"Bulding = {key}\")\n",
        "    ax.yaxis.set_ticks([])\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "1460ffc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predictions by number of traps:\n"
      ],
      "id": "a9dc8579"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(stan_dat_hier['traps'], stan_dat_hier['complaints'], 'o', fillstyle='none')\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, trap in enumerate(stan_dat_hier['traps']):\n",
        "    grouped_indices[trap].append(idx)\n",
        "\n",
        "grouped_array = {trap: y_rep[:, :, idx_list].to_numpy().flatten() for trap, idx_list in grouped_indices.items()}\n",
        "\n",
        "for traps_x, complaints_y in grouped_array.items():\n",
        "    inner_prob = np.array([np.quantile(complaints_y, 0.75), np.quantile(complaints_y, 0.25)])\n",
        "    outer_prob = np.array([np.quantile(complaints_y, 0.95), np.quantile(complaints_y, 0.05)])\n",
        "    ax.plot(np.full(inner_prob.shape, traps_x), inner_prob, 'k', linewidth=2)\n",
        "    ax.plot(np.full(outer_prob.shape, traps_x), outer_prob, 'k', linewidth=0.5)\n",
        "    ax.plot(traps_x, np.mean(complaints_y), 'ko')\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "1914edd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Standardized residuals:\n"
      ],
      "id": "7b093592"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_inv_phi = az_samps_hier_NB_hier_ncp.posterior.inv_phi.mean(dim=['chain', 'draw'])\n",
        "mean_y_rep = y_rep.mean(dim=['chain', 'draw'])\n",
        "std_resid = (stan_dat_hier['complaints'] - mean_y_rep) / np.sqrt(mean_y_rep + mean_y_rep**2 * mean_inv_phi)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(mean_y_rep, std_resid, 'o', alpha=0.5)\n",
        "plt.axhline(2, linestyle='--')\n",
        "plt.axhline(-2, linestyle='--')\n",
        "plt.xlabel('mean_y_rep')\n",
        "plt.ylabel('std_resid')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "bcb728f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rootogram:\n"
      ],
      "id": "9c0d8d47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "vals, counts = np.unique(y_rep, return_counts=True)\n",
        "hist_range = (min(vals), max(vals))\n",
        "az.plot_dist(stan_dat_hier['complaints'], kind='hist', ax=ax, \n",
        "             hist_kwargs={'bins': np.arange(hist_range[0], hist_range[1] + 1, 1), 'range': hist_range})\n",
        "\n",
        "secax = ax.twinx()\n",
        "secax.plot(vals, counts, color='red', linewidth=1.5)\n",
        "\n",
        "ax.set_xlim(hist_range[0]-0.5, hist_range[1]+0.5)\n",
        "\n",
        "secax.set_yticklabels([])\n",
        "secax.yaxis.set_ticks([])\n",
        "\n",
        "ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=5))\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "d5c84fd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Varying intercepts *and* varying slopes\n",
        "\n",
        "We've gotten some new data that extends the number of time points for which we have observations for each building. This will let us explore how to expand the model a bit more with varying *slopes* in addition to the varying intercepts and also, later, also model temporal variation.\n"
      ],
      "id": "8031d72b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import rdata\n",
        "stan_dat_hier = rdata.read_rds('data/pest_data_longer_stan_dat.RDS')\n",
        "stan_dat_hier['N'] = stan_dat_hier['N'][0].astype(int)\n",
        "stan_dat_hier['J'] = stan_dat_hier['J'][0].astype(int)\n",
        "stan_dat_hier['K'] = stan_dat_hier['K'][0].astype(int)\n",
        "stan_dat_hier['M'] = stan_dat_hier['M'][0].astype(int)\n",
        "stan_dat_hier['M_forward'] = stan_dat_hier['M_forward'][0].astype(int)\n",
        "stan_dat_hier['complaints'] = stan_dat_hier['complaints'].astype(int)\n",
        "stan_dat_hier['building_idx'] = stan_dat_hier['building_idx'].astype(int)"
      ],
      "id": "4b03158a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perhaps if the levels of complaints differ by building, the coefficient for the effect of traps on building does too. We can add this to our model and observe the fit.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{complaints}_{b,t} &\\sim \\text{Neg-Binomial}(\\lambda_{b,t}, \\phi)  \\\\\n",
        "\\lambda_{b,t} &= \\exp{(\\eta_{b,t})}\\\\\n",
        "\\eta_{b,t} &= \\mu_b + \\kappa_b \\, \\texttt{traps}_{b,t} + \\text{log_sq_foot}_b \\\\\n",
        "\\mu_b &\\sim \\text{Normal}(\\alpha + \\texttt{building_data} \\, \\zeta, \\sigma_{\\mu}) \\\\\n",
        "\\kappa_b &\\sim \\text{Normal}(\\beta + \\texttt{building_data} \\, \\gamma, \\sigma_{\\kappa})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Let's compile the model.\n"
      ],
      "id": "4581af04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "hier_NB_regression_ncp_slopes_mod = '''\n",
        "functions {\n",
        "  /*\n",
        "  * Alternative to neg_binomial_2_log_rng() that \n",
        "  * avoids potential numerical problems during warmup\n",
        "  */\n",
        "  int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "  int<lower=1> N;                     \n",
        "  array[N] int<lower=0> complaints;\n",
        "  vector<lower=0>[N] traps;        \n",
        "  \n",
        "  // 'exposure'\n",
        "  vector[N] log_sq_foot;  \n",
        "  \n",
        "  // building-level data\n",
        "  int<lower=1> K;\n",
        "  int<lower=1> J;\n",
        "  array[N] int<lower=1, upper=J> building_idx;\n",
        "  matrix[J,K] building_data;\n",
        "}\n",
        "parameters {\n",
        "  real<lower=0> inv_phi;     // 1/phi (easier to think about prior for 1/phi instead of phi)\n",
        "  \n",
        "  vector[J] mu_raw;        // N(0,1) params for non-centered param of building-specific intercepts\n",
        "  real<lower=0> sigma_mu;  // sd of buildings-specific intercepts\n",
        "  real alpha;              // 'global' intercept\n",
        "  vector[K] zeta;          // coefficients on building-level predictors in model for mu\n",
        "  \n",
        "  vector[J] kappa_raw;       // N(0,1) params for non-centered param of building-specific slopes\n",
        "  real<lower=0> sigma_kappa; // sd of buildings-specific slopes\n",
        "  real beta;                 // 'global' slope on traps variable\n",
        "  vector[K] gamma;           // coefficients on building-level predictors in model for kappa\n",
        "}\n",
        "transformed parameters {\n",
        "  real phi = inv(inv_phi);\n",
        "  \n",
        "  // non-centered parameterization of building-specific intercepts and slopes\n",
        "  vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "  vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw;\n",
        "}\n",
        "model {\n",
        "  inv_phi ~ normal(0, 1);\n",
        "  \n",
        "  kappa_raw ~ normal(0,1) ;\n",
        "  sigma_kappa ~ normal(0, 1);\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  gamma ~ normal(0, 1);\n",
        "  \n",
        "  mu_raw ~ normal(0,1) ;\n",
        "  sigma_mu ~ normal(0, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  zeta ~ normal(0, 1);\n",
        "  \n",
        "  complaints ~ neg_binomial_2_log(mu[building_idx] + kappa[building_idx] .* traps  + log_sq_foot,\n",
        "                                  phi);\n",
        "} \n",
        "generated quantities {\n",
        "  array[N] int y_rep;\n",
        "  for (n in 1:N) {\n",
        "    real eta_n = mu[building_idx[n]] + kappa[building_idx[n]] * traps[n] + log_sq_foot[n];\n",
        "    y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi);\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "hier_NB_regression_ncp_slopes_mod_model = StanModel('../stan_models/hier_NB_regression_ncp_slopes_mod', hier_NB_regression_ncp_slopes_mod)"
      ],
      "id": "1a2f5d71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model to data and extract the posterior draws needed for our posterior predictive checks.\n"
      ],
      "id": "c99adaba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "keys_to_remove = {'pred_mat', 'log_sq_foot_pred', 'M_forward', 'M'}\n",
        "filtered_stan_dat_hier = {k: v for k, v in stan_dat_hier.items() if k not in keys_to_remove}\n",
        "\n",
        "fitted_model_NB_hier_slopes = hier_NB_regression_ncp_slopes_mod_model.sample(\n",
        "    data=filtered_stan_dat_hier,\n",
        "    chains=4,\n",
        "    parallel_chains=4,\n",
        "    adapt_delta=0.95,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "az_samps_NB_hier_slopes = az.from_cmdstanpy(fitted_model_NB_hier_slopes, posterior_predictive='y_rep', observed_data=filtered_stan_dat_hier)"
      ],
      "id": "61d04a23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see if the model infers building-to-building differences in, we can plot a histogram of our marginal posterior distribution for `sigma_kappa`.\n"
      ],
      "id": "df25e210"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "sns.histplot(az_samps_NB_hier_slopes.posterior.sigma_kappa.to_numpy().flatten(), binwidth=0.005)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "11e4ca73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = fitted_model_NB_hier_slopes.summary(percentiles=(5, 95)).round(2)\n",
        "filter_indices = r'^(kappa(\\[\\d+\\])?|beta|alpha|phi|sigma_mu|sigma_kappa|mu(\\[\\d+\\])?)$'\n",
        "summary[summary.index.str.match(filter_indices, na=False)]"
      ],
      "id": "ccc45bba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "sns.histplot(az_samps_NB_hier_slopes.posterior.beta.to_numpy().flatten(), binwidth=0.005)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "79bb9edf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the model can't specifically rule out zero from the posterior, it does have mass at small non-zero numbers, so we should leave in the hierarchy over $\\texttt{kappa}$. Plotting the marginal data density again, we can see the model still looks well calibrated.\n"
      ],
      "id": "3d07688f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_rep = az_samps_NB_hier_slopes.posterior_predictive.y_rep\n",
        "\n",
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_hier['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "ce08ea83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time varying effects and structured priors\n",
        "\n",
        "We haven't looked at how cockroach complaints change over time. Let's look at whether there's any pattern over time.\n"
      ],
      "id": "0516430a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grouped_indices = defaultdict(list)\n",
        "for idx, month_num in enumerate(stan_dat_hier['mo_idx']):\n",
        "    if month_num > 12: continue\n",
        "    grouped_indices[month_num].append(idx)\n",
        "    \n",
        "grouped_array = {bldg_id: y_rep[:, :, idx_list].mean(axis=2).to_numpy().flatten() for bldg_id, idx_list in grouped_indices.items()}\n",
        "\n",
        "_, axes = plt.subplots(3, 4, figsize=(7,4))\n",
        "\n",
        "for ax, key in zip(axes.flat, sorted(grouped_array.keys())):\n",
        "    sns.histplot(grouped_array[key], bins=30, ax=ax)\n",
        "    ax.axvline(stan_dat_hier['complaints'][grouped_indices[key]].mean(), color='r', linewidth=2)\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.set(ylabel='', title=f\"Bulding = {key}\", xlim=(0, 12))\n",
        "    ax.yaxis.set_ticks([])\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "3280dcf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We might augment our model with a log-additive monthly effect, $\\texttt{mo}_t$.\n",
        "\n",
        "$$\n",
        "\\eta_{b,t} = \\mu_b + \\kappa_b \\, \\texttt{traps}_{b,t} + \\texttt{mo}_t + \\text{log_sq_foot}_b\n",
        "$$\n",
        "\n",
        "We have complete freedom over how to specify the prior for $\\texttt{mo}_t$. There are several competing factors for how the number of complaints might change over time. It makes sense that there might be more roaches in the environment during the summer, but we might also expect that there is more roach control in the summer as well. Given that we're modeling complaints, maybe after the first sighting of roaches in a building, residents are more vigilant, and thus complaints of roaches would increase.\n",
        "\n",
        "This can be a motivation for using an autoregressive prior for our monthly effects. The model is as follows:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\texttt{mo}_t \\sim \\text{Normal}(\\rho \\, \\texttt{mo}_{t-1}, \\sigma_\\texttt{mo}) \\\\\n",
        "\\equiv \\\\\n",
        "\\texttt{mo}_t = \\rho \\, \\texttt{mo}_{t-1} +\\epsilon_t , \\quad \\epsilon_t \\sim \\text{Normal}(0, \\sigma_\\texttt{mo}) \\\\\n",
        "\\quad \\rho \\in [-1,1]\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "This equation says that the monthly effect in month $t$ is directly related to the last month's monthly effect. Given the description of the process above, it seems like there could be either positive or negative associations between the months, but there should be a bit more weight placed on positive $\\rho$s, so we'll put an informative prior that pushes the parameter $\\rho$ towards 0.5.\n",
        "\n",
        "Before we write our prior, however, we have a problem: Stan doesn't implement any densities that have support on $[-1,1]$. We can use variable transformation of a raw variable defined on $[0,1]$ to to give us a density on $[-1,1]$. Specifically,\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\rho_{\\text{raw}} \\in [0, 1] \\\\\n",
        "\\rho = 2 \\times \\rho_{\\text{raw}} - 1\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Then we can put a beta prior on $\\rho_\\text{raw}$ to push our estimate towards 0.5.\n",
        "\n",
        "One further wrinkle is that we have a prior for $\\texttt{mo}_t$ that depends on $\\texttt{mo}_{t-1}$. That is, we are working with the *conditional* distribution of $\\texttt{mo}_t$ given $\\texttt{mo}_{t-1}$. But what should we do about the prior for $\\texttt{mo}_1$, for which we don't have a previous time period in the data?\n",
        "\n",
        "We need to work out the *marginal* distribution of the first observation. Thankfully we can use the fact that AR models are stationary, so $\\text{Var}(\\texttt{mo}_t) = \\text{Var}(\\texttt{mo}_{t-1})$ and $\\mathbb{E}(\\texttt{mo}_t) = \\mathbb{E}(\\texttt{mo}_{t-1})$ for all $t$. Therefore the marginal distribution of $\\texttt{mo}_1$ is the same as the marginal distribution of any $\\texttt{mo}_t$.\n",
        "\n",
        "First we derive the marginal variance of $\\texttt{mo}_{t}$.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Var}(\\texttt{mo}_t) &= \\text{Var}(\\rho \\texttt{mo}_{t-1} + \\epsilon_t)  \\\\\n",
        "\\text{Var}(\\texttt{mo}_t) &= \\text{Var}(\\rho \\texttt{mo}_{t-1}) + \\text{Var}(\\epsilon_t)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where the second line holds by independence of $\\epsilon_t$ and $\\epsilon_{t-1})$. Then, using the fact that $Var(cX) = c^2Var(X)$ for a constant $c$ and the fact that, by stationarity, $\\textrm{Var}(\\texttt{mo}_{t-1}) = \\textrm{Var}(\\texttt{mo}_{t})$, we then obtain:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Var}(\\texttt{mo}_t) &= \\rho^2 \\text{Var}( \\texttt{mo}_{t})  + \\sigma_\\texttt{mo}^2 \\\\\n",
        "\\text{Var}(\\texttt{mo}_t) &= \\frac{\\sigma_\\texttt{mo}^2}{1 - \\rho^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "For the mean of $\\texttt{mo}_t$ things are a bit simpler:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}(\\texttt{mo}_t) &= \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1} + \\epsilon_t) \\\\\n",
        "\\mathbb{E}(\\texttt{mo}_t) &= \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1}) + \\mathbb{E}(\\epsilon_t) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Since $\\mathbb{E}(\\epsilon_t) = 0$ by assumption we have\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}(\\texttt{mo}_t) &= \\mathbb{E}(\\rho \\, \\texttt{mo}_{t-1})  + 0\\\\\n",
        "\\mathbb{E}(\\texttt{mo}_t) &= \\rho \\, \\mathbb{E}(\\texttt{mo}_{t}) \\\\\n",
        "\\mathbb{E}(\\texttt{mo}_t) - \\rho \\mathbb{E}(\\texttt{mo}_t) &= 0  \\\\\n",
        "\\mathbb{E}(\\texttt{mo}_t) &= 0/(1 - \\rho)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "which for $\\rho \\neq 1$ yields $\\mathbb{E}(\\texttt{mo}_{t}) = 0$.\n",
        "\n",
        "We now have the marginal distribution for $\\texttt{mo}_{t}$, which, in our case, we will use for $\\texttt{mo}_1$. The full AR(1) specification is then:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_1 &\\sim \\text{Normal}\\left(0, \\frac{\\sigma_\\texttt{mo}}{\\sqrt{1 - \\rho^2}}\\right) \\\\\n",
        "\\texttt{mo}_t &\\sim \\text{Normal}\\left(\\rho \\, \\texttt{mo}_{t-1}, \\sigma_\\texttt{mo}\\right) \\forall t > 1\n",
        "\\end{align*}\n",
        "$$\n"
      ],
      "id": "d8c289fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "hier_NB_regression_ncp_slopes_mod_mos = '''\n",
        "functions {\n",
        "  /*\n",
        "  * Alternative to neg_binomial_2_log_rng() that \n",
        "  * avoids potential numerical problems during warmup\n",
        "  */\n",
        "  int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "  int<lower=1> N;   \n",
        "  array[N] int<lower=0> complaints;\n",
        "  vector<lower=0>[N] traps;                \n",
        "  \n",
        "  // 'exposure'\n",
        "  vector[N] log_sq_foot;  \n",
        "  \n",
        "  // building-level data\n",
        "  int<lower=1> K;\n",
        "  int<lower=1> J;\n",
        "  array[N] int<lower=1, upper=J> building_idx;\n",
        "  matrix[J,K] building_data;\n",
        "  \n",
        "  // month \n",
        "  int<lower=1> M; \n",
        "  array[N] int<lower=1,upper=M> mo_idx;\n",
        "}\n",
        "parameters {\n",
        "  real<lower=0> inv_phi;   // 1/phi (easier to think about prior for 1/phi instead of phi)\n",
        "  \n",
        "  vector[J] mu_raw;        // N(0,1) params for non-centered param of building-specific intercepts\n",
        "  real<lower=0> sigma_mu;  // sd of buildings-specific intercepts\n",
        "  real alpha;              // 'global' intercept\n",
        "  vector[K] zeta;          // coefficients on building-level predictors in model for mu\n",
        "  \n",
        "  vector[J] kappa_raw;       // N(0,1) params for non-centered param of building-specific slopes\n",
        "  real<lower=0> sigma_kappa; // sd of buildings-specific slopes\n",
        "  real beta;                 // 'global' slope on traps variable\n",
        "  vector[K] gamma;           // coefficients on building-level predictors in model for kappa\n",
        "  \n",
        "  \n",
        "  vector[M] mo_raw;               // N(0,1) params for non-centered param of AR(1) process\n",
        "  real<lower=0> sigma_mo;         // sd of month-specific parameters\n",
        "  real<lower=0,upper=1> rho_raw;  // used to construct AR(1) coefficient\n",
        "}\n",
        "transformed parameters {\n",
        "  real phi = inv(inv_phi);\n",
        "  \n",
        "  // non-centered parameterization of building-specific intercepts and slopes\n",
        "  vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "  vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw;\n",
        "  \n",
        "  // AR(1) process priors\n",
        "  real rho = 2.0 * rho_raw - 1.0;\n",
        "  vector[M] mo = sigma_mo * mo_raw;\n",
        "  mo[1] /= sqrt(1 - rho^2);\n",
        "  for (m in 2:M) {\n",
        "    mo[m] += rho * mo[m-1];\n",
        "  }\n",
        "}\n",
        "model {\n",
        "  inv_phi ~ normal(0, 1);\n",
        "  \n",
        "  kappa_raw ~ normal(0,1) ;\n",
        "  sigma_kappa ~ normal(0, 1);\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  gamma ~ normal(0, 1);\n",
        "  \n",
        "  mu_raw ~ normal(0,1) ;\n",
        "  sigma_mu ~ normal(0, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  zeta ~ normal(0, 1);\n",
        "  \n",
        "  mo_raw ~ normal(0,1);\n",
        "  sigma_mo ~ normal(0, 1);\n",
        "  rho_raw ~ beta(10, 5);\n",
        "  \n",
        "  complaints ~ neg_binomial_2_log(mu[building_idx] + kappa[building_idx] .* traps \n",
        "                                 + mo[mo_idx] + log_sq_foot, phi);\n",
        "}\n",
        "generated quantities {\n",
        "  array[N] int y_rep;\n",
        "  for (n in 1:N) {\n",
        "    real eta_n = \n",
        "      mu[building_idx[n]] + kappa[building_idx[n]] * traps[n] + mo[mo_idx[n]] + log_sq_foot[n];\n",
        "    y_rep[n] = neg_binomial_2_log_safe_rng(eta_n, phi);\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "comp_model_NB_hier_mos = StanModel('../stan_models/hier_NB_regression_ncp_slopes_mod_mos', hier_NB_regression_ncp_slopes_mod_mos)"
      ],
      "id": "d404d7b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "keys_to_remove = {'pred_mat', 'log_sq_foot_pred', 'M_forward'}\n",
        "filtered_stan_dat_hier = {k: v for k, v in stan_dat_hier.items() if k not in keys_to_remove}\n",
        "\n",
        "fitted_model_NB_hier_mos = comp_model_NB_hier_mos.sample(\n",
        "    data=filtered_stan_dat_hier,\n",
        "    chains=4,\n",
        "    parallel_chains=4,\n",
        "    adapt_delta=0.9,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "az_samps_NB_hier_mos = az.from_cmdstanpy(fitted_model_NB_hier_mos, posterior_predictive='y_rep', observed_data=filtered_stan_dat_hier)"
      ],
      "id": "bc12b556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the interest of brevity, we won't go on expanding the model, though we certainly could. What other information would help us understand the data generating process better? What other aspects of the data generating process might we want to capture that we're not capturing now?\n",
        "\n",
        "As usual, we run through our posterior predictive checks.\n"
      ],
      "id": "1de3b6ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_rep = az_samps_NB_hier_mos.posterior_predictive.y_rep\n",
        "\n",
        "plt.clf()\n",
        "for i in range(200):\n",
        "    sns.kdeplot(y_rep.sel(chain=slice(0), draw=i).to_dataframe(), x='y_rep', color='k', alpha=0.2)\n",
        "sns.kdeplot(stan_dat_hier['complaints'], color='r', linewidth=2)\n",
        "plt.xlim(0,)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "21f71ec4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grouped_indices = defaultdict(list)\n",
        "for idx, month_num in enumerate(stan_dat_hier['mo_idx']):\n",
        "    if month_num > 12: continue\n",
        "    grouped_indices[month_num].append(idx)\n",
        "    \n",
        "grouped_array = {bldg_id: y_rep[:, :, idx_list].mean(axis=2).to_numpy().flatten() for bldg_id, idx_list in grouped_indices.items()}\n",
        "\n",
        "_, axes = plt.subplots(3, 4, figsize=(7,4))\n",
        "\n",
        "for ax, key in zip(axes.flat, sorted(grouped_array.keys())):\n",
        "    sns.histplot(grouped_array[key], bins=20, ax=ax)\n",
        "    ax.axvline(stan_dat_hier['complaints'][grouped_indices[key]].mean(), color='r', linewidth=2)\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.set(ylabel='', title=f\"Bulding = {key}\")\n",
        "    ax.yaxis.set_ticks([])\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "18701b02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, our monthly random intercept has captured a monthly pattern across all the buildings. We can also compare the prior and posterior for the autoregressive parameter to see how much we've learned. Here are two different ways of comparing the prior and posterior visually:\n"
      ],
      "id": "eedaef02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.special as special\n",
        "\n",
        "# 1) compare draws from prior and draws from posterior\n",
        "prior_rho = 2 * np.random.beta(a=10, b=5, size=4000) - 1\n",
        "posterior_rho = az_samps_NB_hier_mos.posterior.rho.to_numpy().flatten()\n",
        "\n",
        "plt.clf()\n",
        "sns.histplot(prior_rho, binwidth=0.025)\n",
        "sns.histplot(posterior_rho, binwidth=0.025)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "3758314a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) overlay prior density curve on posterior draws\n",
        "def gen_rho_prior(x):\n",
        "    alpha, beta = 10, 5\n",
        "    a, c = -1, 1\n",
        "    lp = (\n",
        "        (alpha - 1) * np.log(x - a) +\n",
        "        (beta - 1) * np.log(c - x) -\n",
        "        (alpha + beta - 1) * np.log(c - a) -\n",
        "        special.betaln(3, 4)\n",
        "    )\n",
        "    return np.exp(lp)\n",
        "\n",
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "sns.histplot(posterior_rho, binwidth=0.025, ax=ax)\n",
        "x = np.linspace(-0.99,0.99)\n",
        "secax = ax.twinx()\n",
        "secax.plot(x, gen_rho_prior(x), color='k')\n",
        "secax.set(ylim=(0,0.03))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "f819efd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = fitted_model_NB_hier_mos.summary(percentiles=(5, 95)).round(2)\n",
        "filter_indices = r'^(gamma(\\[\\d+\\])?|rho|sigma_mu|sigma_kappa)$'\n",
        "summary[summary.index.str.match(filter_indices, na=False)]"
      ],
      "id": "c19f767f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(stan_dat_hier['traps'], stan_dat_hier['complaints'], 'o', fillstyle='none')\n",
        "\n",
        "grouped_indices = defaultdict(list)\n",
        "for idx, trap in enumerate(stan_dat_hier['traps']):\n",
        "    grouped_indices[trap].append(idx)\n",
        "\n",
        "grouped_array = {trap: y_rep[:, :, idx_list].to_numpy().flatten() for trap, idx_list in grouped_indices.items()}\n",
        "\n",
        "for traps_x, complaints_y in grouped_array.items():\n",
        "    inner_prob = az.hdi(complaints_y, hdi_prob=0.95)\n",
        "    outer_prob = az.hdi(complaints_y, hdi_prob=0.99)\n",
        "    ax.plot(np.full(inner_prob.shape, traps_x), inner_prob, 'k', linewidth=2)\n",
        "    ax.plot(np.full(outer_prob.shape, traps_x), outer_prob, 'k', linewidth=0.5)\n",
        "    ax.plot(traps_x, np.mean(complaints_y), 'ko')\n",
        "ax.set(xlabel='Number of traps', ylabel='Number of complaints')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "914b5399",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks as if our model finally generates a reasonable posterior predictive distribution for all numbers of traps, and appropriately captures the tails of the data generating process.\n",
        "\n",
        "## Using our model: Cost forecasts\n",
        "\n",
        "Our model seems to be fitting well, so now we will go ahead and use the model to help us make a decision about how many traps to put in our buildings. We'll make a forecast for 6 months forward.\n"
      ],
      "id": "c5838cc9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "hier_NB_regression_ncp_slopes_mod_mos_predict = '''\n",
        "functions {\n",
        "  /*\n",
        "  * Alternative to neg_binomial_2_log_rng() that \n",
        "  * avoids potential numerical problems during warmup\n",
        "  */\n",
        "  int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real gamma_rate = gamma_rng(phi, phi / exp(eta));\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "      \n",
        "    return poisson_rng(gamma_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "  int<lower=1> N;                     \n",
        "  array[N] int<lower=0> complaints;\n",
        "  vector<lower=0>[N] traps;\n",
        "  \n",
        "  // 'exposure'\n",
        "  vector[N] log_sq_foot;\n",
        "  \n",
        "  // building-level data\n",
        "  int<lower=1> K;\n",
        "  int<lower=1> J;\n",
        "  array[N] int<lower=1, upper=J> building_idx;\n",
        "  matrix[J,K] building_data;\n",
        "  \n",
        "  // month \n",
        "  int<lower=1> M; \n",
        "  array[N] int<lower=1,upper=M> mo_idx;\n",
        "  \n",
        "  // for use in generated quantities\n",
        "  int<lower=1> M_forward;\n",
        "  vector[J] log_sq_foot_pred;\n",
        "}\n",
        "transformed data {\n",
        "  // We'll make predictions for 0, 1, 2, ..., 20 traps (can go further too)\n",
        "  int N_hypo_traps = 21;\n",
        "  array[N_hypo_traps] int hypo_traps;\n",
        "  for (i in 1:N_hypo_traps)\n",
        "    hypo_traps[i] = i - 1;\n",
        "}\n",
        "parameters {\n",
        "  real<lower=0> inv_phi;   // 1/phi (easier to think about prior for 1/phi instead of phi)\n",
        "  \n",
        "  vector[J] mu_raw;        // N(0,1) params for non-centered param of building-specific intercepts\n",
        "  real<lower=0> sigma_mu;  // sd of buildings-specific intercepts\n",
        "  real alpha;              // 'global' intercept\n",
        "  vector[K] zeta;          // coefficients on building-level predictors in model for mu\n",
        "  \n",
        "  vector[J] kappa_raw;       // N(0,1) params for non-centered param of building-specific slopes\n",
        "  real<lower=0> sigma_kappa; // sd of buildings-specific slopes\n",
        "  real beta;                 // 'global' slope on traps variable\n",
        "  vector[K] gamma;           // coefficients on building-level predictors in model for kappa\n",
        "  \n",
        "  vector[M] mo_raw;               // N(0,1) params for non-centered param of AR(1) process\n",
        "  real<lower=0> sigma_mo;         // sd of month-specific parameters\n",
        "  real<lower=0,upper=1> rho_raw;  // used to construct AR(1) coefficient\n",
        "}\n",
        "transformed parameters {\n",
        "  real phi = inv(inv_phi);\n",
        "  \n",
        "  // non-centered parameterization of building-specific intercepts and slopes\n",
        "  vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "  vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw;\n",
        "  \n",
        "  // AR(1) process priors\n",
        "  real rho = 2.0 * rho_raw - 1.0;\n",
        "  vector[M] mo = sigma_mo * mo_raw;\n",
        "  mo[1] /= sqrt(1 - rho^2);\n",
        "  for (m in 2:M) {\n",
        "    mo[m] += rho * mo[m-1];\n",
        "  }\n",
        "}\n",
        "model {\n",
        "  inv_phi ~ normal(0, 1);\n",
        "  \n",
        "  kappa_raw ~ normal(0,1) ;\n",
        "  sigma_kappa ~ normal(0, 1);\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  gamma ~ normal(0, 1);\n",
        "  \n",
        "  mu_raw ~ normal(0,1) ;\n",
        "  sigma_mu ~ normal(0, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  zeta ~ normal(0, 1);\n",
        "  \n",
        "  mo_raw ~ normal(0,1);\n",
        "  sigma_mo ~ normal(0, 1);\n",
        "  rho_raw ~ beta(10, 5);\n",
        "  \n",
        "\n",
        "  { \n",
        "  // within a local block we can declare new temporary variables  \n",
        "  vector[N] eta = mu[building_idx] + kappa[building_idx] .* traps + mo[mo_idx] + log_sq_foot;\n",
        "  complaints ~ neg_binomial_2_log(eta, phi);\n",
        "  \n",
        "  /* \n",
        "  alternatively we can use a 'target +=' statement, which is equivalent to \n",
        "  above except that using '~' drops constants that don't affect inferences:\n",
        "  \n",
        "  target += neg_binomial_2_log_lpmf(complaints | eta, phi);\n",
        "  */\n",
        "  }\n",
        "  \n",
        "}\n",
        "generated quantities {\n",
        "  // we'll predict number of complaints and revenue lost for each building\n",
        "  // at each hypothetical number of traps for M_forward months in the future\n",
        "  array[J,N_hypo_traps] int y_pred;\n",
        "  matrix[J,N_hypo_traps] rev_pred;\n",
        "  \n",
        "  for (j in 1:J) {\n",
        "    for (i in 1:N_hypo_traps) {\n",
        "      array[M_forward] int y_pred_by_month;\n",
        "      vector[M_forward] mo_forward;\n",
        "      \n",
        "      mo_forward[1] = normal_rng(rho * mo[M], sigma_mo);\n",
        "      for (m in 2:M_forward) \n",
        "        mo_forward[m] = normal_rng(rho * mo_forward[m-1], sigma_mo); \n",
        "        \n",
        "      for (m in 1:M_forward) {\n",
        "        real eta = mu[j] + kappa[j] * hypo_traps[i] + mo_forward[m] + log_sq_foot_pred[j];\n",
        "        y_pred_by_month[m] = neg_binomial_2_log_safe_rng(eta, phi);\n",
        "      }\n",
        "      \n",
        "      y_pred[j,i] = sum(y_pred_by_month);\n",
        "      \n",
        "      // were were told every 10 complaints has additional exterminator cost of $100, \n",
        "      // so $10 lose per complaint.\n",
        "      rev_pred[j,i] = y_pred[j,i] * (-10.0);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "comp_rev = StanModel('../stan_models/hier_NB_regression_ncp_slopes_mod_mos_predict', hier_NB_regression_ncp_slopes_mod_mos_predict)"
      ],
      "id": "6fc02485",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An important input to the revenue model is how much revenue is lost due to each complaint. The client has a policy that for every 10 complaints, they'll call an exterminator costing the client \\$100, so that'll amount to \\$10 per complaint.\n"
      ],
      "id": "c352528f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "rev_model = comp_rev.sample(\n",
        "    data=stan_dat_hier,\n",
        "    chains=4,\n",
        "    parallel_chains=4,\n",
        "    adapt_delta=0.9,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "az_samps_rev_model = az.from_cmdstanpy(rev_model, posterior_predictive=['rev_pred', 'y_pred'], observed_data=stan_dat_hier)"
      ],
      "id": "bc33db61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we've generated our revenue curves for the buildings. These charts will give us precise quantification of our uncertainty around our revenue projections at any number of traps for each building.\n",
        "\n",
        "A key input to our analysis will be the cost of installing bait stations. We're simulating the number of complaints we receive over the course of a year, so we need to understand the cost associated with maintaining each bait station over the course of a year. There's the cost attributed to the raw bait station, which is the plastic housing and the bait material, a peanut-buttery substance that's injected with insecticide. The cost of maintaining one bait station for a year plus monthly replenishment of the bait material is about \\$20.\n"
      ],
      "id": "4058057f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_traps = np.arange(20+1)\n",
        "costs = N_traps * 10"
      ],
      "id": "6fd609b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also need labor for maintaining the bait stations, which need to be serviced every two months. If there are fewer than five traps, our in-house maintenance staff can manage the stations (about one hour of work every two months at \\$20/hour), but above five traps we need to hire outside pest control to help out. They're a bit more expensive, so we've put their cost at \\$30 /hour. Each five traps should require an extra person-hour of work, so that's factored in as well. The marginal person-person hours above five traps are at the higher pest-control-labor rate.\n"
      ],
      "id": "3aa2ff30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_months_forward = 12\n",
        "N_months_labor = N_months_forward // 2\n",
        "hourly_rate_low = 20\n",
        "hourly_rate_high = 30\n",
        "costs += (\n",
        "    ((N_traps < 5) & (N_traps > 0)) * (N_months_labor * hourly_rate_low) +\n",
        "    ((N_traps >= 5) & (N_traps < 10)) * (N_months_labor * (hourly_rate_low + 1 * hourly_rate_high)) +\n",
        "    ((N_traps >= 10) & (N_traps < 15)) * (N_months_labor * (hourly_rate_low + 2 * hourly_rate_high)) +\n",
        "    (N_traps >= 15) * (N_months_labor * (hourly_rate_low + 3 * hourly_rate_high))\n",
        ")"
      ],
      "id": "158f7363",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot curves with number of traps on the x-axis and profit/loss forecasts and uncertainty intervals on the y-axis.\n"
      ],
      "id": "2d71220c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# total and mean profit\n",
        "tot_profit = az_samps_rev_model.posterior_predictive.rev_pred - costs\n",
        "mean_profit = tot_profit.median(dim=['chain', 'draw'])\n",
        "\n",
        "# lower and upper ends of 50% central interval\n",
        "lower_profit = tot_profit.quantile(q=0.25, dim=['chain', 'draw'])\n",
        "upper_profit = tot_profit.quantile(q=0.75, dim=['chain', 'draw'])\n",
        "\n",
        "profit_df = pd.DataFrame({\n",
        "    'profit': mean_profit.to_numpy().flatten(),\n",
        "    'lower': lower_profit.to_numpy().flatten(),\n",
        "    'upper': upper_profit.to_numpy().flatten(),\n",
        "    'traps': np.tile(N_traps, N_buildings),\n",
        "    'building': np.repeat(np.arange(1, N_buildings+1), np.max(N_traps)+1)\n",
        "})"
      ],
      "id": "4194356c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "\n",
        "fig, axes = plt.subplots(5, 2, figsize=(5,11), sharex=True)\n",
        "\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "  data = profit_df[profit_df['building'] == idx+1]\n",
        "  profit_y = data['profit']\n",
        "  traps_x = data['traps']\n",
        "  ax.plot(traps_x, profit_y)\n",
        "  ax.fill_between(traps_x, y1=data['lower'], y2=data['upper'], alpha=0.2)\n",
        "\n",
        "fig.supxlabel('Traps')\n",
        "fig.supylabel('Profit')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "48ba66a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can can see that the optimal number of bait stations differs by building.\n",
        "\n",
        "### Exercise for the reader\n",
        "\n",
        "-   How would we build a revenue curve for a new building?\n",
        "\n",
        "-   Let's say our utility function is revenue. If we wanted to maximize expected revenue, we can take expectations at each station count for each building, and choose the trap numbers that maximizes expected revenue. This will be called a maximum revenue strategy. How can we generate the distribution of portfolio revenue (i.e. the sum of revenue across all the buildings) under the maximum revenue strategy from the the draws of `rev_pred` we already have?\n",
        "\n",
        "## Gaussian process instead of AR(1)\n",
        "\n",
        "### Joint density for AR(1) process\n",
        "\n",
        "We can derive the joint distribution for the AR(1) process before we move to the Gaussian process (GP) which will give us a little more insight into what a GP is. Remember that we've specified the AR(1) prior as:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_1 &\\sim \\text{Normal}\\left(0, \\frac{\\sigma_\\texttt{mo}}{\\sqrt{1 - \\rho^2}}\\right) \\\\\n",
        "\\texttt{mo}_t &\\sim \\text{Normal}\\left(\\rho \\, \\texttt{mo}_{t-1}, \\sigma_\\texttt{mo}\\right) \\forall t > 1\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Rewriting our process in terms of the errors will make the derivation of the joint distribution clearer\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_1 &\\sim \\text{Normal}\\left(0, \\frac{\\sigma_\\texttt{mo}}{\\sqrt{1 - \\rho^2}}\\right) \\\\\n",
        "\\texttt{mo}_t &= \\rho \\, \\texttt{mo}_{t-1} + \\sigma_\\texttt{mo}\\epsilon_t  \\\\\n",
        "\\epsilon_t &\\sim \\text{Normal}\\left(0, 1\\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Given that our first term $\\texttt{mo}_1$ is normally distributed, and subsequent terms are sums of normal random variables, we can see that jointly the vector, `mo`, with the $t$-th element equally the scalar $\\texttt{mo}_t$, is multivariate normal, with mean zero (which we derived above). More formally, if we have a vector $x \\in \\mathbb{R}^M$ which is multivariate normal, $x \\sim \\text{MultiNormal}(0, \\Sigma)$ and we left-multiply $x$ by a nonsingular matrix $L \\in \\mathbb{R}^{M\\times M}$, $y = Lx \\sim \\text{MultiNormal}(0, L\\Sigma L^T)$. We can use this fact to show that our vector `mo` is jointly multivariate normal.\n",
        "\n",
        "Just as before with the noncentered parameterization, we'll be taking a vector $\\texttt{mo_raw} \\in \\mathbb{R}^M$ in which each element is univariate $\\text{Normal}(0,1)$ and transforming it into `mo`, but instead of doing the transformation with scalar transformations like in the section **Time varying effects and structured priors**, we'll do it with linear algebra operations. The trick is that by specifying each element of $\\texttt{mo_raw}$ to be distributed $\\text{Normal}(0,1)$ we are implicitly defining $\\texttt{mo_raw} \\sim \\text{MultiNormal}(0, I_M)$, where $I_M$ is the identity matrix of dimension $M \\times M$. Then we do a linear transformation using a matrix $L$ and assign the result to `mo` like $\\texttt{mo} = L\\times\\texttt{mo_raw}$ so $\\texttt{mo} \\sim \\text{MultiNormal}(0, LI_M L^T)$ and $LI_M L^T = LL^T$.\n",
        "\n",
        "Consider the case where we have three elements in `mo` and we want to make figure out the form for $L$.\n",
        "\n",
        "The first element of `mo` is fairly straightforward, because it mirrors our earlier parameterization of the AR(1) prior. The only difference is that we're explicitly adding the last two terms of `mo_raw` into the equation so we can use matrix algebra for our transformation.\n",
        "\n",
        "$$\n",
        "\\texttt{mo}_1 = \\frac{\\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + 0 \\times \\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3\\\\\n",
        "$$\n",
        "\n",
        "The second element is a bit more complicated:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_2 &= \\rho \\texttt{mo}_1 + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3  \\\\\n",
        " &= \\rho \\left(\\frac{\\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1\\right) + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3  \\\\[5pt]\n",
        " &= \\frac{\\rho \\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3  \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "While the third element will involve all three terms\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_3 &= \\rho \\, \\texttt{mo}_2 + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_3 \\\\\n",
        " &= \\rho \\left(\\frac{\\rho \\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2\\right) + \\sigma_{\\texttt{mo}} \\texttt{mo_raw}_3  \\\\[5pt]\n",
        "&= \\frac{\\rho^2 \\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + \\rho \\, \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 +  \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_3  \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Writing this all together:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\texttt{mo}_1 &= \\frac{\\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + 0 \\times \\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3\\\\[3pt]\n",
        "\\texttt{mo}_2 &= \\frac{\\rho \\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 + 0 \\times \\texttt{mo_raw}_3  \\\\[3pt]\n",
        "\\texttt{mo}_3 &= \\frac{\\rho^2 \\sigma_{\\texttt{mo}}}{\\sqrt{1 - \\rho^2}} \\times \\texttt{mo_raw}_1 + \\rho \\, \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_2 +  \\sigma_{\\texttt{mo}}\\,\\texttt{mo_raw}_3  \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Separating this into a matrix of coefficients $L$ and the vector `mo_raw`:\n",
        "\n",
        "$$\n",
        "\\texttt{mo} = \\begin{bmatrix} \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & 0 & 0 \\\\\n",
        "              \\rho \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\sigma_\\texttt{mo} & 0 \\\\\n",
        "              \\rho^2 \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\rho \\,\\sigma_\\texttt{mo}  & \\sigma_\\texttt{mo}\n",
        "              \\end{bmatrix} \\times \\texttt{mo_raw}\n",
        "$$\n",
        "\n",
        "If we multiply $L$ on the right by its transpose $L^T$, we'll get expressions for the covariance matrix of our multivariate random vector `mo`:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & 0 & 0 \\\\\n",
        "\\rho \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\sigma_\\texttt{mo} & 0 \\\\\n",
        "\\rho^2 \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\rho \\,\\sigma_\\texttt{mo}  & \\sigma_\\texttt{mo}\n",
        "\\end{bmatrix} \\times\n",
        "\\begin{bmatrix} \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\rho \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} & \\rho^2 \\sigma_\\texttt{mo} / \\sqrt{1 - \\rho^2} \\\\\n",
        "0 & \\sigma_\\texttt{mo} & \\rho \\,\\sigma_\\texttt{mo} \\\\\n",
        "0 & 0  & \\sigma_\\texttt{mo}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which results in:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \\sigma^2_\\texttt{mo} / (1 - \\rho^2) & \\rho \\, \\sigma^2_\\texttt{mo} / (1 - \\rho^2) &  \\rho^2 \\, \\sigma^2_\\texttt{mo} / (1 - \\rho^2)\\\\\n",
        "\\rho \\, \\sigma^2_\\texttt{mo} / (1 - \\rho^2)  & \\sigma^2_\\texttt{mo} / (1 - \\rho^2)  & \\rho \\, \\sigma^2_\\texttt{mo} / (1 - \\rho^2) \\\\\n",
        "\\rho^2 \\sigma^2_\\texttt{mo} / (1 - \\rho^2) & \\rho \\, \\sigma^2_\\texttt{mo} / (1 - \\rho^2) & \\sigma^2_\\texttt{mo} / (1 - \\rho^2)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can simplify this result by dividing the matrix by $\\sigma^2_\\texttt{mo} / (1 - \\rho^2)$ to get\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 & \\rho  &  \\rho^2 \\\\\n",
        "\\rho  & 1  & \\rho  \\\\\n",
        "\\rho^2 & \\rho & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This should generalize to higher dimensions pretty easily. We could replace the `mo` variable and its definition (inside `transformed parameters` block) in Stan code in `hier_NB_regression_ncp_slopes_mod_mos.stan` with the following:\n",
        "\n",
        "```         \n",
        "vector[M] mo;\n",
        "{\n",
        "  matrix[M,M] A = rep_matrix(0, M, M);\n",
        "  A[1,1] = sigma_mo / sqrt(1 - rho^2);\n",
        "  for (m in 2:M)\n",
        "    A[m,1] = rho^(m-1) * sigma_mo / sqrt(1 - rho^2);\n",
        "  for (m in 2:M) {\n",
        "    A[m,m] = sigma_mo;\n",
        "    for (i in (m + 1):M)\n",
        "      A[i,m] = rho^(i-m) * sigma_mo;\n",
        "  }\n",
        "  mo = A * mo_raw;\n",
        "}\n",
        "```\n",
        "\n",
        "It's important to the note that the existing block of Stan code is doing the exact same calculations but more efficiently.\n",
        "\n",
        "### Cholesky decomposition\n",
        "\n",
        "Note that if we only knew the covariance matrix of our process, say a matrix called $\\Sigma$, and we had a way of decomposing $\\Sigma$ into $L L^T$ we wouldn't need to write out the equation for the vector. Luckily, there is a matrix decomposition called the **Cholesky decomposition** that does just that. The Stan function for the composition is called `cholesky_decompose`. Instead of writing out the explicit equation, we could do the following:\n",
        "\n",
        "```         \n",
        "vector[M] mo;\n",
        "{\n",
        "  mo = cholesky_decompose(Sigma) * mo_raw;\n",
        "}\n",
        "```\n",
        "\n",
        "provided we've defined `Sigma` appropriately elsewhere in the transformed parameter block. Note that the matrix $L$ is lower-triangular (i.e. all elements in the upper right-hand triangle of the matrix are zero).\n",
        "\n",
        "We've already derived the covariance matrix `Sigma` for the three-dimensional AR(1) process above by explicitly calculating $L L^T$, but we can do so using the rules of covariance and the way our process is defined. We already know that each element of $\\texttt{mo}_t$ has marginal variance $\\sigma^2_\\texttt{mo} / (1 - \\rho^2)$, but we don't know the covariance of $\\texttt{mo}_t$ and $\\texttt{mo}_{t+h}$. We can do so recursively. First we derive the covariance for two elements of $\\texttt{mo}_t$ separated by one month:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Cov}(\\texttt{mo}_{t+1},\\texttt{mo}_{t}) &= \\text{Cov}(\\rho \\, \\texttt{mo}_{t} + \\sigma_\\texttt{mo}\\epsilon_{t+1},\\texttt{mo}_{t}) \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+1},\\texttt{mo}_{t}) &= \\rho \\text{Cov}(\\texttt{mo}_{t},\\texttt{mo}_{t}) + \\sigma_\\texttt{mo}\\text{Cov}(\\epsilon_{t+1},\\texttt{mo}_{t}) \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+1},\\texttt{mo}_{t}) &= \\rho \\text{Var}(\\texttt{mo}_t) + 0\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Then we define the covariance for $\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t})$ in terms of $\\text{Cov}(\\texttt{mo}_{t+h-1},\\texttt{mo}_{t})$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\text{Cov}(\\rho \\, \\texttt{mo}_{t+h-1} + \\sigma_\\texttt{mo}\\epsilon_{t+h},\\texttt{mo}_{t}) \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\rho \\, \\text{Cov}(\\texttt{mo}_{t+h-1},\\texttt{mo}_{t}) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Which we can use to recursively get at the covariance we need:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\rho \\, \\text{Cov}(\\texttt{mo}_{t+h-1},\\texttt{mo}_{t}) \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\rho \\,( \\rho \\, \\text{Cov}(\\texttt{mo}_{t+h-2},\\texttt{mo}_{t}) )\\\\\n",
        "\\dots \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\rho^h \\, \\text{Cov}(\\texttt{mo}_{t},\\texttt{mo}_{t}) \\\\\n",
        "\\text{Cov}(\\texttt{mo}_{t+h},\\texttt{mo}_{t}) &= \\rho^h \\, \\sigma_\\texttt{mo}^2/(1 - \\rho^2) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Writing this in Stan code to replace the `mo` variable and its definition in `hier_NB_regression_ncp_slopes_mod_mos.stan` we would get:\n",
        "\n",
        "```         \n",
        "vector[M] mo;\n",
        "{\n",
        "  matrix[M,M] Sigma;\n",
        "  for (m in 1:M) {\n",
        "    Sigma[m,m] = 1.0;\n",
        "    for (i in (m + 1):M) {\n",
        "      Sigma[i,m] = rho^(i - m);\n",
        "      Sigma[m,i] = Sigma[i,m];\n",
        "    }\n",
        "  }\n",
        "  Sigma = Sigma * sigma_mo^2 / (1 - rho^2);\n",
        "  mo = cholesky_decompose(Sigma) * mo_raw;\n",
        "}\n",
        "```\n",
        "\n",
        "### Extension to Gaussian processes\n",
        "\n",
        "The prior we defined for `mo` is strictly speaking a Gaussian process. It is a stochastic process that is distributed as jointly multivariate normal for any finite value of $M$. Formally, we could write the above prior for `mo` like so:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\sigma_\\texttt{mo} &\\sim \\text{Normal}(0, 1) \\\\\n",
        "  \\rho &\\sim \\text{GenBeta}(-1,1,10, 5) \\\\\n",
        "  \\texttt{mo}_t &\\sim \\text{GP}\\left( 0, K(t | \\sigma_\\texttt{mo},\\rho) \\right) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The notation $K(t | \\sigma_\\texttt{mo},\\rho)$ defines the covariance matrix of the process over the domain $t$, which is months.\n",
        "\n",
        "In other words:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(\\texttt{mo}_t,\\texttt{mo}_{t+h}) = k(t, t+h | \\sigma_\\texttt{mo}, \\rho)\n",
        "$$\n",
        "\n",
        "We've already derived the covariance for our process. What if we want to use a different definition of `Sigma`?\n",
        "\n",
        "As the above example shows defining a proper covariance matrix will yield a proper multivariate normal prior on a parameter. We need a way of defining a proper covariance matrix. These are symmetric positive definite matrices. It turns out there is a class of functions that define proper covariance matrices, called **kernel functions**. These functions are applied elementwise to construct a covariance matrix, $K$:\n",
        "\n",
        "$$\n",
        "K_{[t,t+h]} = k(t, t+h | \\theta)\n",
        "$$\n",
        "\n",
        "where $\\theta$ are the hyperparameters that define the behavior of covariance matrix.\n",
        "\n",
        "One such function is called the **exponentiated quadratic function**, and it happens to be implemented in Stan as `gp_exp_quad_cov`. The function is defined as:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  k(t, t+h | \\theta) &= \\alpha^2  \\exp \\left( - \\dfrac{1}{2\\ell^2} ((t+h) - t)^2 \\right) \\\\\n",
        "  &= \\alpha^2  \\exp \\left( - \\dfrac{h^2}{2\\ell^2} \\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The exponentiated quadratic kernel has two components to theta, $\\alpha$, the marginal standard deviation of the stochastic process $f$ and $\\ell$, the process length-scale.\n",
        "\n",
        "The length-scale defines how quickly the covariance decays between time points, with large values of $\\ell$ yielding a covariance that decays slowly, and with small values of $\\ell$ yielding a covariance that decays rapidly. It can be seen interpreted as a measure of how nonlinear the `mo` process is in time.\n",
        "\n",
        "The marginal standard deviation defines how large the fluctuations are on the output side, which in our case is the number of roach complaints per month across all buildings. It can be seen as a scale parameter akin to the scale parameter for our building-level hierarchical intercept, though it now defines the scale of the monthly deviations.\n",
        "\n",
        "This kernel's defining quality is its smoothness; the function is infinitely differentiable. That will present problems for our example, but if we add some noise the diagonal of our covariance matrix, the model will fit well.\n",
        "\n",
        "$$\n",
        "k(t, t+h | \\theta) = \\alpha^2  \\exp \\left( - \\dfrac{h^2}{2\\ell^2} \\right) + \\text{if } h = 0, \\, \\sigma^2_\\texttt{noise} \\text{ else } 0\n",
        "$$\n",
        "\n",
        "### Compiling the GP model\n"
      ],
      "id": "e313a50a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hier_NB_regression_ncp_slopes_mod_mos_gp = '''\n",
        "functions {\n",
        "  int neg_binomial_2_log_safe_rng(real eta, real phi) {\n",
        "    real phi_div_exp_eta;\n",
        "    real gamma_rate;\n",
        "    phi_div_exp_eta = phi/exp(eta);\n",
        "    gamma_rate = gamma_rng(phi, phi_div_exp_eta);\n",
        "    if (gamma_rate >= exp(20.79))\n",
        "      return -9;\n",
        "    return poisson_rng(gamma_rate);\n",
        "  }\n",
        "}\n",
        "data {\n",
        "  int<lower=1> N;\n",
        "  int<lower=1> M;\n",
        "  int<lower=1> K;\n",
        "  array[N] int complaints;\n",
        "  vector[N] traps;\n",
        "  int<lower=1> J;\n",
        "  array[N] int<lower=1, upper=J> building_idx;\n",
        "  matrix[J,K] building_data;\n",
        "  vector[N] log_sq_foot;\n",
        "  array[N] int<lower=1> mo_idx;\n",
        "}\n",
        "transformed data {\n",
        "  array[M] real mo_gp_vec;\n",
        "  for (m in 1:M)\n",
        "    mo_gp_vec[m] = m;\n",
        "}\n",
        "parameters {\n",
        "  real alpha;\n",
        "  real<lower=0> sigma_mu;\n",
        "  real<lower=0> sigma_kappa;\n",
        "  vector[J] mu_raw;\n",
        "  vector[J] kappa_raw;\n",
        "  real beta;\n",
        "  real<lower=0> inv_phi;\n",
        "  vector[K] zeta;\n",
        "  vector[K] gamma;\n",
        "  // GP prior parameters\n",
        "  vector[M] gp_raw;\n",
        "  real<lower=0> gp_len;\n",
        "  real<lower=0> sigma_gp;\n",
        "  real<lower=0> sigma_noise;\n",
        "  vector[M] mo_noise_raw;\n",
        "}\n",
        "transformed parameters {\n",
        "  vector[J] mu = alpha + building_data * zeta + sigma_mu * mu_raw;\n",
        "  vector[J] kappa = beta + building_data * gamma + sigma_kappa * kappa_raw;\n",
        "  vector[M] mo_noise = sigma_noise * mo_noise_raw;\n",
        "  real phi = inv(inv_phi);\n",
        "  vector[M] gp_exp_quad;\n",
        "  vector[M] gp;\n",
        "  {\n",
        "    matrix[M, M] C = gp_exp_quad_cov(mo_gp_vec, sigma_gp, gp_len);\n",
        "    real var_noise = square(sigma_noise);\n",
        "    matrix[M, M] L_C;\n",
        "    for (m in 1:M)\n",
        "      C[m,m] += 1e-12;\n",
        "    L_C = cholesky_decompose(C);\n",
        "    gp_exp_quad = L_C * gp_raw;\n",
        "  }\n",
        "  \n",
        "  // gp is sum of monthly noise and the smoothly varying process\n",
        "  gp = mo_noise + gp_exp_quad;\n",
        "}\n",
        "model {\n",
        "  beta ~ normal(-0.25, 1);\n",
        "  mu_raw ~ normal(0,1);\n",
        "  kappa_raw ~ normal(0,1);\n",
        "  sigma_mu ~ normal(0, 1);\n",
        "  sigma_kappa ~ normal(0, 1);\n",
        "  alpha ~ normal(log(4), 1);\n",
        "  zeta ~ normal(0, 1);\n",
        "  gamma ~ normal(0, 1);\n",
        "  inv_phi ~ normal(0, 1);\n",
        "  \n",
        "  // GP priors\n",
        "  gp_raw ~ normal(0, 1);\n",
        "  gp_len ~ gamma(10, 2);\n",
        "  sigma_gp ~ normal(0, 1);\n",
        "  \n",
        "  sigma_noise ~ normal(0, 1);\n",
        "  mo_noise_raw ~ normal(0, 1);\n",
        "  \n",
        "  complaints ~ neg_binomial_2_log(mu[building_idx] + kappa[building_idx] .* traps \n",
        "                                 + gp[mo_idx] + log_sq_foot, phi);\n",
        "} \n",
        "generated quantities {\n",
        "  array[N] int y_rep;\n",
        "\n",
        "  for (n in 1:N) \n",
        "    y_rep[n] = neg_binomial_2_log_safe_rng(mu[building_idx[n]] + kappa[building_idx[n]] * traps[n]\n",
        "                                          + gp[mo_idx[n]] + log_sq_foot[n],\n",
        "                                          phi);\n",
        "}\n",
        "'''\n",
        "\n",
        "comp_model_NB_hier_gp = StanModel('../stan_models/hier_NB_regression_ncp_slopes_mod_mos_gp', hier_NB_regression_ncp_slopes_mod_mos_gp)"
      ],
      "id": "792048ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting the GP model to data\n"
      ],
      "id": "d15cd938"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fitted_model_NB_hier_gp = comp_model_NB_hier_gp.sample(\n",
        "    data=stan_dat_hier,\n",
        "    chains=4,\n",
        "    parallel_chains=4,\n",
        "    adapt_delta=0.9,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "samps_gp = az.from_cmdstanpy(fitted_model_NB_hier_gp, posterior_predictive='y_rep', observed_data=stan_dat_hier)"
      ],
      "id": "e12716c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examining the fit\n",
        "\n",
        "Let's look at the prior vs. posterior for the GP length scale parameter:\n"
      ],
      "id": "103d195f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "sns.kdeplot(samps_gp.posterior.gp_len.to_numpy().flatten(), label='Posterior')\n",
        "sns.kdeplot(np.random.gamma(10, 1/2, 4000), label='Prior')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "40886aed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the plot above it only looks like we learned a small amount, however we can see a bigger difference between the prior and posterior if we consider how much we learned about the ratio of `sigma_gp` to the length scale `gp_len`:\n"
      ],
      "id": "1009aa99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "sns.kdeplot(samps_gp.posterior.sigma_gp.to_numpy().flatten() / samps_gp.posterior.gp_len.to_numpy().flatten(), label='Posterior')\n",
        "sns.kdeplot(np.abs(np.random.normal(size=4000)) / np.random.gamma(10, 1/2, 4000), label='Prior')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "983249eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a classic problem with Gaussian processes. Marginally, the length-scale parameter isn't very well-identified by the data, but jointly the length-scale and the marginal standard deviation are well-identified.\n",
        "\n",
        "And let's compare the estimates for the time varying parameters between the AR(1) and GP. In this case the posterior mean of the time trend is essentially the same for the AR(1) and GP priors but the 50% uncertainty intervals are narrower for the AR(1):\n"
      ],
      "id": "465fcb2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib.lines import Line2D \n",
        "\n",
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(np.arange(1,36+1), \n",
        "            hdi_data=az.hdi(samps_gp.posterior.gp, hdi_prob=0.5), \n",
        "            smooth=False, \n",
        "            color='C1',\n",
        "            ax=ax)\n",
        "az.plot_hdi(np.arange(1,36+1), \n",
        "            hdi_data=az.hdi(az_samps_NB_hier_mos.posterior.mo, hdi_prob=0.5), \n",
        "            smooth=False, \n",
        "            color='C0',\n",
        "            ax=ax)\n",
        "\n",
        "legend_handles = [\n",
        "    Line2D([0], [0], color='C1', lw=2, label=\"GP\"),\n",
        "    Line2D([0], [0], color='C0', lw=2, label=\"AR1\")\n",
        "]\n",
        "\n",
        "ax.legend(handles=legend_handles, loc='lower right')\n",
        "ax.set(xlabel='Time', ylabel='m')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "2fb2cc09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The way we coded the GP also lets us plot a decomposition of the GP into a monthly noise component (`mo_noise` in the Stan code) and the underlying smoothly varying trend (`gp_exp_quad` in the Stan code):\n"
      ],
      "id": "37bd2548"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "time = np.arange(1,36+1)\n",
        "az.plot_hdi(time, \n",
        "            hdi_data=az.hdi(samps_gp.posterior.gp_exp_quad, hdi_prob=0.5), \n",
        "            smooth=False, \n",
        "            color='C1',\n",
        "            ax=ax)\n",
        "az.plot_hdi(time, \n",
        "            hdi_data=az.hdi(samps_gp.posterior.mo_noise, hdi_prob=0.5), \n",
        "            smooth=False, \n",
        "            color='C0',\n",
        "            ax=ax)\n",
        "\n",
        "ax.plot(time, samps_gp.posterior.mo_noise.mean(dim=['chain', 'draw']))\n",
        "ax.plot(time, samps_gp.posterior.gp_exp_quad.mean(dim=['chain', 'draw']))\n",
        "\n",
        "legend_handles = [\n",
        "    Line2D([0], [0], color='C1', lw=2, label=\"Smooth Trend\"),\n",
        "    Line2D([0], [0], color='C0', lw=2, label=\"Monthly Noise\")\n",
        "]\n",
        "\n",
        "ax.legend(handles=legend_handles, loc='lower right')\n",
        "ax.set(xlabel='Time', ylabel='m')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "25fb7b4b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "main",
      "language": "python",
      "display_name": "main",
      "path": "/Users/rehabnaeem/Library/Jupyter/kernels/main"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}