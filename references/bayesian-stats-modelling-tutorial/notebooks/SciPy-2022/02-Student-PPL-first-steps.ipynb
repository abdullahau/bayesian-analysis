{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7455101",
   "metadata": {},
   "source": [
    "# Parameter estimation and hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe95357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "from ipywidgets import interact\n",
    "import arviz as az\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4986782",
   "metadata": {},
   "source": [
    "## Learning Objectives of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bceb679",
   "metadata": {},
   "source": [
    "1. Understand what priors, likelihoods and posteriors are;\n",
    "2. Use random sampling for parameter estimation to appreciate the relationship between sample size & the posterior distribution, along with the effect of the prior;\n",
    "3. Use probabilistic programming for parameter estimation;\n",
    "4. Use probabilistic programming for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36a7b7",
   "metadata": {},
   "source": [
    "## 1. From Bayes Theorem to Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc9b18",
   "metadata": {},
   "source": [
    "Let's say that we flip a biased coin several times and we want to estimate the probability of heads from the number of heads we saw. Statistical intuition tells us that our best estimate of $p(heads)=$ number of heads divided by total number of flips.\n",
    "\n",
    "However, \n",
    "\n",
    "1. It doesn't tell us how certain we can be of that estimate and\n",
    "2. This type of intuition doesn't extend to even slightly more complex examples.\n",
    "\n",
    "Bayesian inference helps us here. We can calculate the probability of a particular $p=p(H)$ given data $D$ by setting $A$ in Bayes Theorem equal to $p$ and $B$ equal to $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a94e6",
   "metadata": {},
   "source": [
    "\n",
    "$$P(p|D) = \\frac{P(D|p)P(p)}{P(D)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefaf145",
   "metadata": {},
   "source": [
    "In this equation, we call $P(p)$ the prior (distribution), $P(D|p)$ the likelihood and $P(p|D)$ the posterior (distribution). The intuition behind the nomenclature is as follows: the prior is the distribution containing our knowledge about $p$ prior to the introduction of the data $D$ & the posterior is the distribution containing our knowledge about $p$ after considering the data $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef0379",
   "metadata": {},
   "source": [
    "**Note** that we're _overloading_ the term _probability_ here. In fact, we have 3 distinct usages of the word:\n",
    "- The probability $p$ of seeing a head when flipping a coin;\n",
    "- The resulting binomial probability distribution $P(D|p)$ of seeing the data $D$, given $p$;\n",
    "- The prior & posterior probability distributions of $p$, encoding our _uncertainty_ about the value of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc8b36",
   "metadata": {},
   "source": [
    "**Key concept:** We only need to know the posterior distribution $P(p|D)$ up to multiplication by a constant at the moment: this is because we really only care about the values of $P(p|D)$ relative to each other – for example, what is the most likely value of $p$? To answer such questions, we only need to know what $P(p|D)$ is proportional to, as a function of $p$. Thus we don’t currently need to worry about the term $P(D)$. In fact,\n",
    "\n",
    "$$P(p|D) \\propto P(D|p)P(p) $$\n",
    "\n",
    "**Note:** What is the prior? Really, what do we know about $p$ before we see any data? Well, as it is a probability, we know that $0\\leq p \\leq1$. If we haven’t flipped any coins yet, we don’t know much else: so it seems logical that all values of $p$ within this interval are equally likely, i.e., $P(p)=1$, for $0\\leq p \\leq1$. This is known as an uninformative prior because it contains little information (there are other uninformative priors we may use in this situation, such as the Jeffreys prior, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we’ll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn’t matter so much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4473f",
   "metadata": {},
   "source": [
    "\n",
    "**Essential remark:** we get the whole distribution of $P(p|D)$, not merely a point estimate plus errors bars, such as [95% confidence intervals](http://andrewgelman.com/2018/07/04/4th-july-lets-declare-independence-95/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50058d7",
   "metadata": {},
   "source": [
    "## 2. Bayesian parameter estimation I: flip those coins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71043234",
   "metadata": {},
   "source": [
    "Now let's generate some coin flips and try to estimate $p(H)$. Two notes:\n",
    "- given data $D$ consisting of $n$ coin tosses & $k$ heads, the likelihood function is given by $L:=P(D|p) \\propto p^k(1-p)^{n-k}$;\n",
    "- given a uniform prior, the posterior is proportional to the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773686af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(p=0.6, N=0):\n",
    "    \"\"\"Plot the posterior given a uniform prior; Bernoulli trials\n",
    "    with probability p; sample size N\"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    # Flip coins \n",
    "    n_successes = rng.binomial(N, p)\n",
    "    # X-axis for PDF\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    # Prior\n",
    "    prior = 1\n",
    "    # Compute posterior, given the likelihood (analytic form)\n",
    "    posterior = x**n_successes*(1-x)**(N-n_successes)*prior\n",
    "    posterior /= np.max(posterior)  # so that peak always at 1\n",
    "    plt.plot(x, posterior)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55623b",
   "metadata": {},
   "source": [
    "* Now use the great ipywidget interact to check out the posterior as you generate more and more data (you can also vary $p$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24503e17",
   "metadata": {},
   "source": [
    "**Notes for discussion:**\n",
    "\n",
    "* as you generate more and more data, your posterior gets narrower, i.e. you get more and more certain of your estimate.\n",
    "* you need more data to be certain of your estimate when $p=0.5$, as opposed to when $p=0$ or $p=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3da81b",
   "metadata": {},
   "source": [
    "### The choice of the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eac036",
   "metadata": {},
   "source": [
    "You may have noticed that we needed to choose a prior and that, in the small to medium data limit, this choice can affect the posterior. We'll briefly introduce several types of priors and then you'll use one of them for the example above to see the effect of the prior:\n",
    "\n",
    "- **Informative priors** express specific, definite information about a variable, for example, if we got a coin from the mint, we may use an informative prior with a peak at $p=0.5$ and small variance. \n",
    "- **Weakly informative priors** express partial information about a variable, such as a peak at $p=0.5$ (if we have no reason to believe the coin is biased), with a larger variance.\n",
    "- **Uninformative priors** express no information about a variable, except what we know for sure, such as knowing that $0\\leq p \\leq1$.\n",
    "\n",
    "Now you may think that the _uniform distribution_ is uninformative, however, what if I am thinking about this question in terms of the probability $p$ and Eric Ma is thinking about it in terms of the _odds ratio_ $r=\\frac{p}{1-p}$? Eric rightly feels that he has no prior knowledge as to what this $r$ is and thus chooses the uniform prior on $r$.\n",
    "\n",
    "With a bit of algebra (transformation of variables), we can show that choosing the uniform prior on $p$ amounts to choosing a decidedly non-uniform prior on $r$ and vice versa. So Eric and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**, which is an uninformative prior that solves this problem. You can read more about the Jeffreys prior [here](https://en.wikipedia.org/wiki/Jeffreys_prior) & in your favourite Bayesian text book (Sivia gives a nice treatment). \n",
    "\n",
    "In the binomial (coin flip) case, the Jeffreys prior is given by $P(p) = \\frac{1}{\\sqrt{p(1-p)}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b704b9",
   "metadata": {},
   "source": [
    "#### Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4d581",
   "metadata": {},
   "source": [
    "* Create an interactive plot like the one above, except that it has two posteriors on it: one for the uniform prior, another for the Jeffries prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a638a78",
   "metadata": {},
   "source": [
    "**Question:** What happens to the posteriors as you generate more and more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d0c7f",
   "metadata": {},
   "source": [
    "# The Bayesian Workflow OR A Taste of Probabilistic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c0e43",
   "metadata": {},
   "source": [
    "## Bayesian parameter estimation using PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646506f",
   "metadata": {},
   "source": [
    "Now it's time to generalize our new skills and build on them by learning the basics of probabilistic programming using PyMC. To this end, let's recap the general workflow. \n",
    "\n",
    "In the basics of Bayesian model building, the steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters.\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "In the coin flipping example, the form of the sampling distribution of the data was Binomial (described by the likelihood) and the uncertainty around the unknown parameter $p$ captured by the prior (we used both the uniform and the Jeffreys priors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219c936",
   "metadata": {},
   "source": [
    "Now it is time to do the same using the **probabilistic programming language** PyMC. There's _loads of cool stuff_ about PyMC and this paradigm, two of which are\n",
    "- _probabililty distributions_ are first class citizens, in that we can assign them to variables and use them intuitively to mirror how we think about priors, likelihoods & posteriors.\n",
    "- PyMC calculates the posterior for us!\n",
    "\n",
    "Under the hood, PyMC will compute the posterior using a sampling based approach called Markov Chain Monte Carlo (MCMC) or Variational Inference. Check the [PyMC docs](https://docs.pymc.io/) for more on these. \n",
    "\n",
    "But now, it's time to bust out some MCMC and get sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61261a",
   "metadata": {},
   "source": [
    "### Parameter estimation I: click-through rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbcbe9",
   "metadata": {},
   "source": [
    "A common experiment in tech data science is to test a product change and see how it affects a metric that you're interested in. Say that I don't think enough people are clicking a button on my website & I hypothesize that it's because the button is a similar color to the background of the page. Then I can set up two pages and send some people to each: the first the original page, the second a page that is identical, except that it has a button that is of higher contrast and see if more people click through. This is commonly referred to as an A/B test and the metric of interest is click-through rate (CTR), what proportion of people click through. Before even looking at two rates, let's use PyMC to estimate one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65a818",
   "metadata": {},
   "source": [
    "First lets generate click-through data, given a CTR $p_a=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80856865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from IPython.display import display_png\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63addb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed207995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "rng = np.random.default_rng(42)\n",
    "p_a = 0.15\n",
    "N = 150\n",
    "n_successes_a = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab440e3f",
   "metadata": {},
   "source": [
    "Now it's time to build our probability model. Noticing that our model of having a constant CTR resulting in click or not is a biased coin flip,\n",
    "- the sampling distribution is binomial and we need to encode this in the likelihood;\n",
    "- there is a single parameter $p$ that we need to describe the uncertainty around, using a prior and we'll use a uniform prior for this.\n",
    "\n",
    "These are the ingredients for the model so let's now build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model of p_a\n",
    "with pm.Model() as model:\n",
    "    # Prior on p\n",
    "    prob = ___\n",
    "    # Binomial Likelihood\n",
    "    y = ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572d1e8",
   "metadata": {},
   "source": [
    "Now that we've done the hard work of building our model, \n",
    "let's visualize the model graph using `graphviz`.\n",
    "In our case, the graph will look simple\n",
    "because our model doesn't have too many moving parts.\n",
    "That said, being able to see the model graph is awesome\n",
    "and will introduce you with a tool\n",
    "that will help visualizing more complex models in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbcf30",
   "metadata": {},
   "source": [
    "It's now time to sample from the posterior using PyMC. You'll also then plot the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = ___\n",
    "    # We're also sampling the prior/posterior predictives,\n",
    "    # which we'll introduce and explain below\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, progressbar=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior samples of $p$, the click-through rate, having seen the data.\n",
    "with model:\n",
    "    ___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9624a26",
   "metadata": {},
   "source": [
    "#### Predictive checks and traceplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bbfb7",
   "metadata": {},
   "source": [
    "Let's now introduce 3 diagnostic tools, which will come in handy when building models:\n",
    "\n",
    "- prior predictive check,\n",
    "- posterior predictive check, and\n",
    "- traceplots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff36d2d",
   "metadata": {},
   "source": [
    "**Prior predictive checks** essentially generate samples of the data\n",
    "based on your specified priors without looking at the actual data.\n",
    "They help you check whether your priors are sensible distributions,\n",
    "or if they have the potential to generate wildly unreasonable data.\n",
    "Let's plot the prior predictive check ECDF for our model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0357ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior predictive samples for observed data.\n",
    "# Generate x & y data for ECDF\n",
    "x_ecdf, y_ecdf = ecdf(idata.prior_predictive[\"y\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');\n",
    "plt.xlabel(\"Number of Clicks\")\n",
    "plt.ylabel(\"Cumulative Fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963191fa",
   "metadata": {},
   "source": [
    "Now, we have to ask the question: Does this prior predictive check look sensible?\n",
    "This question is both qualitative and quantitative.\n",
    "Qualitatively, it looks extremely similar to a uniform distribution's CDF.\n",
    "Quantitatively, its lower bound $0$ and upper bound $150$ are what we expect.\n",
    "We know that we are sampling the number of click throughs\n",
    "out of a total number of 150 website landings\n",
    "so this prior distribution looks pretty good!\n",
    "\n",
    "Counterfactually, we would be concerned if there negative values _or_ values over $150$,\n",
    "as they would violate what we assume about our data-generating processes.\n",
    "In the next example, we'll see how red flags in our prior predictive check\n",
    "can lead us to alter our priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e389e",
   "metadata": {},
   "source": [
    "The *posterior* predictive check samples our posterior distribution of data so we would expect it to look something like our actual data. Let's check it out, in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83645f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x & y data for ECDF\n",
    "x_ecdf, y_ecdf = ecdf(idata.posterior_predictive[\"y\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none', color=\"black\")\n",
    "plt.axvline(x=n_successes_a, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b87fe",
   "metadata": {},
   "source": [
    "Notice that this looks somewhat Gaussian, centered around 21. Now our data is a single number, the number of clicks, so we would expect it to be around $21\\pm5$. Let's see what it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bdb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_successes_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00be7ba",
   "metadata": {},
   "source": [
    "This looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc42a22",
   "metadata": {},
   "source": [
    "Finally, let's now plot the *traceplot*, which plots our parameters as a function of iteration number and tells us about whether our sampling has converged. Essentially we want to make sure that the sampling procedure doesn't \"shoot off\" in any weird way, such as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4dd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../../images/badchains.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecb2f3",
   "metadata": {},
   "source": [
    "What we really want to see is a traceplot that doesn't exhibit drift but diffusion,\n",
    "that is, noise around a certain value.\n",
    "There are ways to frame this in terms of autocorrelation but,\n",
    "for the time being, the most important way to think about it is\n",
    "you want to see a traceplot that looks something like a \"hairy caterpillar\",\n",
    "which we do on the right in this case\n",
    "(on the left we have the samples of our distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b48633",
   "metadata": {},
   "source": [
    "### HANDS ON: Parameter estimation II -- the mean of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16208f",
   "metadata": {},
   "source": [
    "We'll now calculate the  posterior mean beak depth of Galapagos finches in a given species. First let's load the data and subset it with respect to species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6096ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../../data/finch_beaks_2012.csv')\n",
    "df_fortis = df_12.loc[df_12['species'] == 'fortis']\n",
    "df_scandens = df_12.loc[df_12['species'] == 'scandens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbb86f",
   "metadata": {},
   "source": [
    "To specify the full probability model, we need\n",
    "- a likelihood function for the data &\n",
    "- priors for all unknowns.\n",
    "\n",
    "What is the likelihood here? Let's plot the measurements below and see that they look approximately normal so we'll use a normal likelihood $y_i\\sim \\mathcal{N}(\\mu, \\sigma^2)$. The unknowns here are the mean $\\mu$ and standard deviation $\\sigma$ and we'll use weakly informative priors on both\n",
    "\n",
    "- a normal prior for $\\mu$ with mean $10$ and standard deviation $5$;\n",
    "- a lognormal prior for $\\sigma$ with $\\mu = 0$ and $\\sigma = 10$.\n",
    "\n",
    "There are biological reasons for these priors \n",
    "(**NOTE: not really now, with the lognormal prior? \n",
    "Need to explain what the biological reasons are otherwise**)\n",
    "also but we can also test that the posteriors are relatively robust to the choice of prior here due to the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_fortis['blength']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb457c3",
   "metadata": {},
   "source": [
    "Let's now build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior for mean & standard deviation\n",
    "    mu_1 = ___\n",
    "    sigma_1 = ___\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054341f3",
   "metadata": {},
   "source": [
    "Once again, we can also visualize our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = ___\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed81947",
   "metadata": {},
   "source": [
    "We now press our inference button and sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82870f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = ___\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, progressbar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e504c7",
   "metadata": {},
   "source": [
    "Let's now plot our *prior predictive check*, data, and *posterior predictive check*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5aea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ___\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaaad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(df_fortis['blength'].values)\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57810a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_ecdf, yp_ecdf = ___\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');\n",
    "plt.plot(xp_ecdf, yp_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639af30",
   "metadata": {},
   "source": [
    "Although the *posterior predictive check* passes the eyeball test (when comapring with the data), the *prior predictive check* does not! Looking at the x-axis on it, we see that we end up with values orders of magnitude larger than what a beak length could possibly be!\n",
    "\n",
    "This means that our priors aren't quite right yet.\n",
    "Looking at them, we can see that having a Lognormal prior for our variance\n",
    "means that we could end up with some pretty large values\n",
    "so let's try a prior that drops off more quickly (we say it has a _shorter tail_).\n",
    "We'll try an Exponential prior here as we know that these have shorter tails.\n",
    "We don't expect the learner to be aware of this\n",
    "but we hope to help build your intuition around this here\n",
    "and in the following case stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior for mean & standard deviation\n",
    "    mu_1 = ___\n",
    "    sigma_1 = ___\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = pm.Normal('y_1', mu=mu_1, sigma=sigma_1, \n",
    "    observed=df_fortis['blength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf69dcd-103e-49ec-8da3-a347f67558f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = ___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = ___\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, \n",
    "        progressbar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff586df",
   "metadata": {},
   "source": [
    "Let's now plot our *prior* and *posterior* predictive checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.prior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.posterior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a8b71",
   "metadata": {},
   "source": [
    "The *prior* predictive check is now looking a lot better!\n",
    "There is still the possibility of negative values, so that may still be an issue.\n",
    "Additionally, if it were still generating data that are too large,\n",
    "we could increase the exponent `lam`, among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56211f00",
   "metadata": {},
   "source": [
    "Now let's plot the posteriors of our two parameters, mean $\\mu$ and standard deviation $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    ___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491f749",
   "metadata": {},
   "source": [
    "We once again plot the traceplot, which plots our parameters as a function of iteration number, showing that our model has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56379dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc474cb4",
   "metadata": {},
   "source": [
    "## Bayesian hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ed0f7",
   "metadata": {},
   "source": [
    "After parameter estimation, a common scientific question involves hypothesis testing, so let's now see how Bayesian inference and probabilistic programming can be used for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e2ff9",
   "metadata": {},
   "source": [
    "### Bayesian hypothesis testing I: A/B tests on click through rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d196ea7",
   "metadata": {},
   "source": [
    "Assume we have a website and want to redesign the layout (*A*) and test whether the new layout (*B*) results in a higher click through rate. When people come to our website we randomly show them layout *A* or *B* and see how many people click through for each. First let's generate the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "p_b = 0.20\n",
    "N = 1000\n",
    "n_successes_a = np.sum(np.random.uniform(size=N) <= p_a)\n",
    "n_successes_b = np.sum(np.random.uniform(size=N) <= p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b29a9",
   "metadata": {},
   "source": [
    "Once again, we need to specify our models for $p_a$ and $p_b$. Each will be the same as the CTR example above\n",
    "- Binomial likelihoods\n",
    "- uniform priors on $p_a$ and $_p$.\n",
    "\n",
    "We also want to calculate the posterior of the difference $p_a-p_b$ and we do so using `pm.Deterministic()`, which specifies a deterministic random variable, i.e., one that is completely determined by the values it references, in the case $p_a$ & $p_b$.\n",
    "\n",
    "We'll now build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7586afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior on p\n",
    "    prob_a = pm.Uniform('p_a')\n",
    "    prob_b = pm.Uniform('p_b')\n",
    "    # Binomial Likelihood\n",
    "    y_a = pm.Binomial('y_a', n=N, p=prob_a, observed=n_successes_a)\n",
    "    y_b = pm.Binomial('y_b', n=N, p=prob_b, observed=n_successes_b)\n",
    "    diff_clicks = pm.Deterministic('diff_clicks', prob_a-prob_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73567f37",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71371207",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d03a4c",
   "metadata": {},
   "source": [
    "Sample from the posterior and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab14734",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n",
    "    az.plot_posterior(samples, kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ed198",
   "metadata": {},
   "source": [
    "We once again plot the traceplot, which plots our parameters as a function of iteration number, showing that our model has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caec48f",
   "metadata": {},
   "source": [
    "Note that, for the sake of brevity, we did not perform our predictive checks, but this is something that the excited learner can do, in this case, and for those below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978c507",
   "metadata": {},
   "source": [
    "### HANDS ON: Bayesian hypothesis testing II -- beak lengths difference between species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3ded7",
   "metadata": {},
   "source": [
    "In the following, we seek to determine whether the mean beak length of the Galapagos finches differs between species. For the mean of each species, we use the same model as in previous section:\n",
    "\n",
    "- Gaussian likelihood;\n",
    "- Normal prior for the means;\n",
    "- Uniform prior for the variances.\n",
    "\n",
    "We also calculate the difference between the means and, the _effect size_, which is the difference between the means divided by the pooled standard deviations = $\\sqrt{(\\sigma_1^2+\\sigma_2^2)/2}$. \n",
    "\n",
    "We then sample from the posteriors and plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f203140",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for means and variances\n",
    "    mu_1 = ___\n",
    "    sigma_1 = ___\n",
    "    mu_2 = ___\n",
    "    sigma_2 = ___\n",
    "    # Gaussian Likelihoods\n",
    "    y_1 = ___\n",
    "    y_2 = ___\n",
    "    # Calculate the effect size and its uncertainty.\n",
    "    diff_means = pm.Deterministic('diff_means', mu_1 - mu_2)\n",
    "    pooled_sd = pm.Deterministic('pooled_sd', \n",
    "                                 np.sqrt(np.power(sigma_1, 2) + \n",
    "                                         np.power(sigma_2, 2)) / 2)\n",
    "    effect_size = pm.Deterministic('effect_size', \n",
    "                                   diff_means / pooled_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76093759",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c654961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(samples, \n",
    "    var_names=['mu_1', 'mu_2', 'diff_means', 'effect_size'], kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33587c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf42b75",
   "metadata": {},
   "source": [
    "## Bayesian regression\n",
    "\n",
    "In the above, we have seen how to use the Bayesian workflow and probabilistic programming for both parameter estimation and comparison between groups. To now round out our first few Bayesian modeling tools, let's see how to use the Bayesian workflow and probabilistic programming for linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363de6d7",
   "metadata": {},
   "source": [
    "To this end, let's first generate some bivariate data, consisting of a linear relationship and a noise/error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "(m, b) = (25, 0.5)\n",
    "x = 100 * np.random.random(20)\n",
    "y = m * x + b\n",
    "\n",
    "# add scatter to points\n",
    "x = np.random.normal(x, 10)\n",
    "y = np.random.normal(y, 10)\n",
    "\n",
    "plt.plot(x, y, 'ok');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac9b33",
   "metadata": {},
   "source": [
    "Now recall that our Bayesian model building steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters.\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "The sampling distribution of the data is given by $y = mx + b + \\sigma$, where $m$ is the gradient, $b$ the $y$-intercept, and $\\sigma$ the error/noise term. These are the unknown parameters, which we need to describe our _uncertatinty_ around using priors and we choose:\n",
    "\n",
    "- Gaussian priors for $m$ and $b$,\n",
    "- A Half-Cauchy prior for $\\sigma$ (this is a distribution that has desirable properties for us, but isn't worth getting into the details of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Define priors\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n",
    "    b = pm.Normal(\"b\", 0, sigma=20)\n",
    "    m = pm.Normal(\"m\", 0, sigma=20)\n",
    "\n",
    "    # Define likelihood\n",
    "    likelihood = pm.Normal(\"y\", mu=b + m * x, sigma=sigma, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4efaac",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4fd47",
   "metadata": {},
   "source": [
    "Let's now plot the traceplot, which plots our parameters as a function of iteration number and here shows that our model has converged in a desirable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2810fff",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fb363",
   "metadata": {},
   "source": [
    "In this notebook, we've introduced the fundamentals of the principled Bayesian workflow and demonstrated its utility in answering the common scientific questions of parameter estimation and hypothesis testing. We've done so for two different types of data, the first an example of data from A/B testing, the second a dataset from basic research of Galapagos finch beak measurements. We also introduced Bayesian linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
