{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, we are going to look at the issue of model parametrization.\n",
    "\n",
    "The parametrization of a model has knock-on effects when sampling.\n",
    "In the example from the previous chapter,\n",
    "you saw how we had obtained \"divergences\" warnings\n",
    "when hitting Thomas Wiecki's Inference Button (tm).\n",
    "These divergences show up when the sampler has problems\n",
    "sampling from the posterior distribution.\n",
    "\n",
    "What kind of problems, you might ask?\n",
    "\n",
    "Well, we'll first have to take a detour into the \"shape\" of joint distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funneling down the river\n",
    "\n",
    "To illustrate what we mean by the \"shape\" of joint distributions,\n",
    "I am going to introduce you to **Neal's Funnel**,\n",
    "named after Professor Radford Neal,\n",
    "who proposed this as a _particularly extreme_ example\n",
    "of pathological likelihood geometries.\n",
    "Neal's funnel a slightly different form and more dimensions of $x$,\n",
    "but I have simplified the example here to make it easier to follow.\n",
    "\n",
    "In particular, we're going to look at the following joint distribution\n",
    "between two random variables, $x$ and $v$:\n",
    "\n",
    "$$v \\sim N(0, 1)$$\n",
    "$$x \\sim N(0, e^v)$$\n",
    "\n",
    "Let's see this simplified Neal's Funnel implemented in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "n_dims = 1\n",
    "for i in range(1000):\n",
    "    v = norm(0, 1).rvs(1)\n",
    "    x = norm(0, np.exp(v)).rvs(n_dims)\n",
    "    data.append(np.hstack([v, x]))\n",
    "data = pd.DataFrame(data)\n",
    "data.columns = [\"v\"] + [fr\"x_{i+1}\" for i in range(n_dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = data.plot(x=\"x_1\", y=\"v\", kind=\"scatter\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through why the joint distribution is shaped as it is.\n",
    "(In retrospect,\n",
    "this is one of those situations that makes a ton of sense\n",
    "only after one sees it.)\n",
    "\n",
    "In this joint distribution between $x_1$ and $v$,\n",
    "\n",
    "$$v \\sim N(0, 1)$$\n",
    "$$x \\sim N(0, e^v)$$\n",
    "\n",
    "when $v$ is negative, the variance term of $x$ is very small,\n",
    "because $e^v$ is a fractional number,\n",
    "and hence the numbers drawn have a very tight distribution around the mean $0$.\n",
    "When $v$ is positive, the variance term of $x$ grows exponentially,\n",
    "and so the numbers drawn are extremely variable.\n",
    "\n",
    "As you can see, the joint distribution samples are shaped like a funnel.\n",
    "Not only that, if you imagine the _density_ of points sampled,\n",
    "there is a third dimension that rises up from the screen to you.\n",
    "The contours look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neals_funnel_likelihood(v, x):\n",
    "    v_like = norm(0, 1).pdf(v)\n",
    "    x_like = norm(0, np.exp(v)).pdf(x)\n",
    "    return v_like * x_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm.autonotebook import tqdm\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "vs = np.linspace(-3, 3, 1000)\n",
    "xs = np.linspace(-20, 20, 1000)\n",
    "X, V = np.meshgrid(xs, vs)\n",
    "L = neals_funnel_likelihood(V, X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.contour(X, V, L, levels=10, cmap=\"viridis\");\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"v\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that _really_ looks like a funnel!\n",
    "As usual, $x$ and $v$ are plotted against each other,\n",
    "but now the likelihood of jointly drawing any two values of $(x, v)$\n",
    "are going to be really concentrated in the funnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling difficulties\n",
    "\n",
    "Now, I'd like to imagine you're an MCMC sampler.\n",
    "(To understand what goes on underneath the hood,\n",
    "check out the chapter on MCMC sampling.)\n",
    "Not a fancy one, just a simple one.\n",
    "The rule by which you propose a new value to sample\n",
    "is governed by a standard Gaussian distribution $N(0, 1)$.\n",
    "(That is your proposal distribution.)\n",
    "Your goal is to _most_ of the time sample new points\n",
    "that are within regions of high likelihood,\n",
    "occasionally allowing yourself to step out but not always.\n",
    "\n",
    "Now imagine that you fell into the funnel,\n",
    "such that you sampled $x=0.0003$ and $v=-2.89275$ on your last step.\n",
    "With a simple $N(0, 1)$ proposal distribution,\n",
    "most of the points you propose for $x$\n",
    "are going to fall outside of the region of high likelihood.\n",
    "You're going to be mostly stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative reparametrization\n",
    "\n",
    "A reparametrization of the model makes things a lot easier.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "$$v \\sim N(0, 3)$$\n",
    "$$x \\sim N(0, e^v)$$\n",
    "\n",
    "We switch to:\n",
    "\n",
    "$$\\hat{v} \\sim N(0, 1)$$\n",
    "$$ v = 3 \\hat{v}  + 0$$\n",
    "$$\\hat{x} \\sim N(0, 1)$$\n",
    "$$ x = \\hat{x}e^v + 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What manner of monstrosity is this?\n",
    "\n",
    "Fret not! What is going on here can be explained.\n",
    "\n",
    "One little trick we have up our sleeves is this:\n",
    "Every Gaussian distribution $N(\\mu, \\sigma)$\n",
    "can be generated by sampling from an $N(0, 1)$ distribution,\n",
    "multiplying by $\\sigma$, and adding $\\mu$!\n",
    "\n",
    "So in the above model,\n",
    "$\\hat{v}$ is sampled from a standard Gaussian,\n",
    "then multiplied by the variance term $3$ and added to the mean term $0$,\n",
    "thereby generating the original $v \\sim N(0, 3)$ distribution.\n",
    "\n",
    "Same goes for $x$:\n",
    "$\\hat{x}$ is sampled from a standard Gaussian,\n",
    "then multiplied by the variance term $e^v$ and added to the mean term $0$,\n",
    "thereby regenerating the original $x \\sim N(0, e^v)$ distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the effect?\n",
    "\n",
    "The effect of a reparametrization for the MCMC samplers used in PyMC3 is tremendous!\n",
    "\n",
    "Instead of having to sample from an $N(0, e^v)$ distribution for the random variable $x$,\n",
    "the sampler can instead propose new steps for $\\hat{x}$ in a standard $N(0, 1)$ space,\n",
    "which is less likely to generate rejected samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But the analogy feels a bit contrived...\n",
    "\n",
    "If you're feeling this way, you're not alone.\n",
    "\n",
    "Most data problems don't look _this_ pathological.\n",
    "However, it is an extremely illustrative example of what happens\n",
    "when an MCMC sampler rejects most of the proposed samples.\n",
    "The "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparametrizing Logits\n",
    "\n",
    "Let's see how all of those lessons can be applied in our ice cream shop problem!\n",
    "After all, in that prior chapter on hierarchical models,\n",
    "we did suffer from divergences in the sampling process,\n",
    "indicating to us that there may have been pathologies \n",
    "in the shape of the joint likelihoods.\n",
    "\n",
    "Let's start by re-loading the trace from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "from pyprojroot import here\n",
    "\n",
    "trace_original = az.from_netcdf(here() / \"data/ice_cream_shop_hierarchical_posterior.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_original, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trace plot, you might see some of the places where divergences happened.\n",
    "In particular, for owner 7, on one of the chains, the divergences are pretty obvious.\n",
    "That's where the sampler _really_ got stuck in a region of bad geometry.\n",
    "\n",
    "Here's another way to visualize why this _might_ be happening:\n",
    "let's go ahead and plot the $p$ terms and variance terms against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_posterior_pair(trace, rv1=\"p_owner\", rv2=\"logit_p_owner_scale\"):\n",
    "    locations = trace.posterior[rv1].to_dataframe().unstack(-1)\n",
    "    scales = trace.posterior[rv2].to_dataframe().unstack(-1)\n",
    "    \n",
    "    for i in range(9):\n",
    "        plt.scatter(locations[(rv1, i)], scales[(rv2, i)], alpha=0.3, label=f\"{i}\")\n",
    "    plt.xlabel(rv1)\n",
    "    plt.ylabel(rv2)\n",
    "    sns.despine()\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior_pair(trace_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the geometry isn't _so_ pathological,\n",
    "but a funnel-like shape can be observed for a subset of shop owners.\n",
    "(We have to keep in mind that these are not exact posteriors,\n",
    "but samples taken from what is otherwise a fairly biased sampling procedure\n",
    "because of the divergences.)\n",
    "Let's take a look at a reparametrized version of the same model that we wrote before.\n",
    "But first, just to jog your memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import ice_cream_hierarchical_model\n",
    "\n",
    "ice_cream_hierarchical_model??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The place that should look like we might be able to do a reparametrization\n",
    "is in the line that has the `logit_p_shop` RV defined.\n",
    "This is because it has a $\\mu$ and $\\sigma$ both defined as random variables.\n",
    "Perhaps that might be where the pathologies might lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.data import load_ice_cream\n",
    "data = load_ice_cream()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build reparametrized model\n",
    "\n",
    "Remember that the key idea here is to convert:\n",
    "\n",
    "$$N(\\mu, \\sigma)$$ \n",
    "\n",
    "into its equivalent:\n",
    "\n",
    "$$N(0, 1) * \\sigma + \\mu$$\n",
    "\n",
    "Take a look at the code below to see how it's done.\n",
    "I've done my best annotating the location where the reparametrization happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "n_owners = len(data[\"owner_idx\"].unique())\n",
    "with pm.Model() as model:\n",
    "    logit_p_overall = pm.Normal(\"logit_p_overall\", mu=0, sigma=1)\n",
    "    logit_p_owner_mean = pm.Normal(\n",
    "        \"logit_p_owner_mean\",\n",
    "        mu=logit_p_overall,\n",
    "        sigma=1,\n",
    "        shape=(n_owners,),\n",
    "    )\n",
    "    logit_p_owner_scale = pm.Exponential(\n",
    "        \"logit_p_owner_scale\", lam=1 / 5.0, shape=(n_owners,)\n",
    "    )\n",
    "    logit_p_shop_raw = pm.Normal(\n",
    "        \"logit_p_shop_raw\",\n",
    "        mu=0,\n",
    "        sigma=1,\n",
    "        shape=(len(data),),\n",
    "    )\n",
    "    \n",
    "    logit_p_shop = pm.Deterministic(\n",
    "        \"logit_p_shop\",\n",
    "        logit_p_shop_raw * logit_p_owner_scale[data[\"owner_idx\"]] + logit_p_owner_mean[data[\"owner_idx\"]],\n",
    "    )\n",
    "\n",
    "    p_overall = pm.Deterministic(\"p_overall\", pm.invlogit(logit_p_overall))\n",
    "    p_shop = pm.Deterministic(\"p_shop\", pm.invlogit(logit_p_shop))\n",
    "    p_owner = pm.Deterministic(\"p_owner\", pm.invlogit(logit_p_owner_mean))\n",
    "    like = pm.Binomial(\n",
    "        \"like\",\n",
    "        n=data[\"num_customers\"],\n",
    "        p=p_shop,\n",
    "        observed=data[\"num_favs\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000)\n",
    "    trace = az.from_pymc3(\n",
    "        trace,\n",
    "        coords={\n",
    "            \"p_shop_dim_0\": data[\"shopname\"],\n",
    "            \"logit_p_shop_transformed\": data[\"shopname\"],\n",
    "            \"logit_p_shop_dim_0\": data[\"shopname\"],\n",
    "            \"logit_p_owner_scale_dim_0\": data[\"owner_idx\"].sort_values().unique(),\n",
    "            \"p_owner_dim_0\": data[\"owner_idx\"].sort_values().unique(),\n",
    "            \"logit_p_owner_mean\": data[\"owner_idx\"].sort_values().unique(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect traces\n",
    "\n",
    "Let's now do a comparison of the traces.\n",
    "As with the prior chapter on hierarchical models,\n",
    "for simplicity's sake, we're going to look at only the owner $\\mu$ and $p$,\n",
    "rather than look at all of the shop's $\\mu$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"logit_p_owner_mean\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more divergences! Also, the traces look more like hairy caterpillars again. Compare that to the traces from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_original, var_names=[\"logit_p_owner_mean\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that the divergences are gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this mean for me?\n",
    "\n",
    "The long story cut short: Look for opportunities to reparametrize!\n",
    "\n",
    "In particular, if you run into a situation like we have\n",
    "where you have a Gaussian in your model,\n",
    "if divergences show up during sampling,\n",
    "this is an opportunity to try a non-centered parametrization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Stan User Guide on Reparametrizations](https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html)\n",
    "- [PyMC3 docs on diagnosing biased inferences with divergences](https://docs.pymc.io/notebooks/Diagnosing_biased_Inference_with_Divergences.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
