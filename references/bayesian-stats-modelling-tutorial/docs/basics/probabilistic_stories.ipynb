{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, we're going to get some practice writing data generating processes,\n",
    "and calculating joint likelihoods between our data and model,\n",
    "using the SciPy statistics library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating coin flips (again!)\n",
    "\n",
    "In this notebook, we are going to have some fun telling _probabilistic stories_.\n",
    "\n",
    "We're going to stick with coin flip simulations, because it's a very good \"simplest complex model\".\n",
    "\n",
    "Previously, we constructed coin flips where we knew the parameter $p$ (probability of heads) precisely.\n",
    "This time, though, we're going to construct a model of coin flips\n",
    "that no longer involves a fixed/known $p$,\n",
    "but instead involves a $p$ that is not precisely known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol\n",
    "\n",
    "If we have a $p$ that is not precisely known, we can set it up by instantiating a probability distribution for it, rather than a fixed value.\n",
    "\n",
    "How do we decide what distribution to use?\n",
    "Primarily, the criteria that should guide us is the _support_ of the distribution,\n",
    "that is, the range of values for which the probability distribution is valid.\n",
    "\n",
    "$p$ must be a value that is bounded between 0 and 1.\n",
    "As such, the choice of probability distribution for $p$ is most intuitively the Beta distribution,\n",
    "which provides a probability distribution over the interval $[0, 1]$.\n",
    "\n",
    "Taking that value drawn from the Beta, we can pass it into the Bernoulli distribution,\n",
    "and then draw an outcome (either 1 or 0).\n",
    "In doing so, we now have the makings of a __generative model__ for our coin flip data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating in code\n",
    "\n",
    "Let's see the algorithmic protocol above implemented in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as sts\n",
    "import numpy as np\n",
    "\n",
    "def coin_flip_generator() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Coin flip generator for a `p` that is not precisely known.\n",
    "    \"\"\"\n",
    "    p = sts.beta(a=10, b=10).rvs(1)\n",
    "    result = sts.bernoulli(p=p).rvs(1)\n",
    "    return result\n",
    "\n",
    "coin_flip_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph form\n",
    "\n",
    "If we visualize this model in graphical form,\n",
    "it would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import coin_flip_pgm\n",
    "\n",
    "coin_flip_pgm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, each node is a random variable. For example, `result` is the random variable that models outcomes. It accepts a parameter `p`, which itself is a random variable that does not depend on anything. At the same time, `p` depends on two parameters, $\\alpha$ and $\\beta$, which are fixed.\n",
    "\n",
    "The graphical form expresses _conditional dependence_ between random variables, that is to say, `result`'s draws depend on the value of `p` drawn. In math symbols, we would write this joint distribution between `p` and `result` as:\n",
    "\n",
    "$$P(p, result) = P(result | p)P(p)$$\n",
    "\n",
    "The `|` tells us that `results` is conditioned on, or depends on, the value of the random variable `p`.\n",
    "\n",
    "The graphical form is a definitely a simplified view, in that we don't show the exact probability distributions by which each random variable is distributed. That is what can make reading the diagrams a bit confusing at first, though with practice, things get much easier over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Information\n",
    "\n",
    "The astute eyes amongst you will notice \n",
    "that the Beta distribution has parameters of its own,\n",
    "so how do we instantiate that?\n",
    "Well, one thing we can do is bring in some _prior information_ to the problem.\n",
    "\n",
    "Is our mental model of this coin that it behaves like billions of other coins in circulation,\n",
    "in that it will generate outcomes with basically equal probability?\n",
    "Turns out, the Beta distribution can assign credibility in this highly opinionated fashion!\n",
    "And by doing so, we are injecting _prior information_\n",
    "by instantiating a Beta _prior distribution_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatSlider, interact, Checkbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha = FloatSlider(value=2, min=1.0, max=100, step=1, description=r'$\\alpha$')\n",
    "beta = FloatSlider(value=2, min=1.0, max=100, step=1, description=r'$\\beta$')\n",
    "equal = Checkbox(value=False, description=r\"set $\\beta$ to be equal to $\\alpha$\")\n",
    "\n",
    "@interact(alpha=alpha, beta=beta, equal=equal)\n",
    "def visualize_beta_distribution(alpha, beta, equal):\n",
    "    if equal:\n",
    "        beta = alpha\n",
    "    dist = sts.beta(a=alpha, b=beta)\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    ys = dist.pdf(xs)\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Likelihood\")\n",
    "    plt.plot(xs, ys)\n",
    "    plt.title(fr\"$\\alpha$={alpha}, $\\beta$={beta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you play around with the slider, notice how when you increase the $\\alpha$ and $\\beta$ sliders,\n",
    "the width of the probability distribution decreases,\n",
    "while the height of the maximum value increases,\n",
    "thus reflecting greater _certianty_ in what values for $p$ get drawn.\n",
    "Using this _prior distribution_ on $p$, we can express what we think is reasonable\n",
    "given _prior knowledge_ of our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justifying priors\n",
    "\n",
    "Some of you, at this point, might be wondering - is there an algorithmic protocol for justifying our priors too?\n",
    "Can we somehow \"just pass our priors into a machine and have it tell us if we're right or wrong\"?\n",
    "\n",
    "It's a great wish, but remains just that: wishful thinking.\n",
    "Just like the \"Aye Eye Drug\", one for which a disease is plugged in,\n",
    "and the target and molecule are spat out.\n",
    "(I also find it to not be an inspiring goal,\n",
    "as the fun of discovery is removed.)\n",
    "\n",
    "Rather, as with all modelling exercises,\n",
    "I advocate for human debate about the model.\n",
    "After all, humans are the ones taking action based on, and being affected by, the modelling exercise.\n",
    "There are a few questions we can ask to help us decide:\n",
    "\n",
    "- Are the prior assumptions something a _reasonable_ person would make?\n",
    "- Is there evidence that lie outside of our problem that can help us justify these priors?\n",
    "- Is there a _practical_ difference between two different priors?\n",
    "- In the limit of infinite data, do various priors converge? (We will see later how this convergence can happen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "It's time for some exercises to practice what we've learnt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Control prior distribution\n",
    "\n",
    "In this first exercise, I would like you to modify the `coin_flip_generator` function\n",
    "such that it allows a user to control what the prior distribution on $p$ should look like\n",
    "before returning outcomes drawn from the Bernoulli.\n",
    "\n",
    "Be sure to check that the values of `alpha` and `beta` are valid values, i.e. floats greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import coin_flip_generator_v2\n",
    "\n",
    "# Your answer below:\n",
    "# def coin_flip_generator_v2(alpha: float, beta: float) -> np.ndarray:\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate data\n",
    "\n",
    "Now, simulate data generated from your new coin flip generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from bayes_tutorial.solutions.simulation import generate_many_coin_flips\n",
    "\n",
    "# Your answer below:\n",
    "# def generate_many_coin_flips(n_draws: int, alpha: float, beta: float) -> List[int]:\n",
    "#     pass\n",
    "\n",
    "generate_many_coin_flips(50, alpha=5, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that written, we now have a \"data generating\" function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint likelihood\n",
    "\n",
    "Remember back in the first notebook how we wrote about evaluating the joint likelihood of multiple coin flip data\n",
    "against an assumed Bernoulli model?\n",
    "\n",
    "We wrote a function that looked something like the following:\n",
    "\n",
    "```python\n",
    "from scipy import stats as sts\n",
    "from typing import List\n",
    "\n",
    "def likelihood(data: List[int]):\n",
    "    c = sts.bernoulli(p=0.5)\n",
    "    return np.product(c.pmf(data))\n",
    "```\n",
    "\n",
    "Now, if $p$ is something that is not precisely known,\n",
    "then any \"guesses\" of $p$ will have to be subject to the Likelihood principle too,\n",
    "which means that we need to jointly evaluate the likelihood of $p$ and our data.\n",
    "\n",
    "Let's see that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_flip_joint_likelihood(data: List[int], p: float) -> float:\n",
    "    p_like = sts.beta(a=10, b=10).pdf(p)  # evaluate guesses of `p` against the prior distribution\n",
    "    data_like = sts.bernoulli(p=p).pmf(data)\n",
    "    \n",
    "    return np.product(data_like) * np.product(p_like)\n",
    "\n",
    "coin_flip_joint_likelihood([1, 1, 0, 1], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint _log_-likelihood\n",
    "\n",
    "Because we are dealing with decimal numbers,\n",
    "when multiplying them together,\n",
    "we might end up with underflow issues.\n",
    "As such, we often take the log of the likelihood.\n",
    "\n",
    "### Exercise: Implementing joint _log_-likelihood\n",
    "\n",
    "Doing this means we can use summations on our likelihood calculations,\n",
    "rather than products.\n",
    "\n",
    "Because of the rules of logarithms, what originally was:\n",
    "\n",
    "$$P(D|p)P(p)$$\n",
    "\n",
    "becomes:\n",
    "\n",
    "$$\\log(P(D|p)) + \\log(P(p))$$\n",
    "\n",
    "Also, if you think about the joint distribution of data,\n",
    "$P(D)$ is actually $P(D_1, D_2, ..., D_n)$ for $n$ data points,\n",
    "but because each is independent from one another, the joint distribution of $P(D)$ factorizes out to $P(D_1)P(D_2)...P(D_n)$. Taking the log then allows us to sum up the log of PMFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import coin_flip_joint_loglike\n",
    "\n",
    "# Your answer below:\n",
    "# def coin_flip_joint_loglike(data: List[int], p: float) -> float:\n",
    "#     pass\n",
    "\n",
    "coin_flip_joint_loglike([1, 1, 0, 1], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Confirm equality\n",
    "\n",
    "Now confirm that the joint log-likelihood is of the same value as the log of the joint likelihood,\n",
    "subject to machine precision error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(coin_flip_joint_likelihood([1, 1, 0, 1], 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Idea: Statistical Stories\n",
    "\n",
    "Before we can go into probabilistic programming,\n",
    "one has to know the skill of \"telling statistical stories\".\n",
    "\n",
    "In telling statistical stories, we are using probability distributions\n",
    "to represent the pieces of our problem that are difficult to precisely know.\n",
    "It is because they are difficult to precisely know\n",
    "that we use random variables, distributed by some probability distribution,\n",
    "as the modelling tool of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stories of probability distributions\n",
    "\n",
    "One skill that is necessary in knowing how to choose\n",
    "what probability distribution to associate with a random variable\n",
    "is to learn their \"distribution stories\".\n",
    "\n",
    "Here's an example, taken from [Justin Bois' excellent resource][jsbois],\n",
    "for the Bernoulli distribution:\n",
    "\n",
    "> A Bernoulli trial is an experiment that has two outcomes that can be encoded as success ($y=1$) or failure ($y=0$). The result $y$ of a Bernoulli trial is Bernoulli distributed.\n",
    "\n",
    "[jsbois]: http://bois.caltech.edu/dist_stories/t3b_probability_stories.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "A generally usable workflow for telling statistical stories\n",
    "is to work backwards from the data.\n",
    "Using our evergreen coin flip example, if we start with coin flip-like data,\n",
    "and have a hunch that our data are never going to be anything other than 0s and 1s,\n",
    "then we might use a Bernoulli to model the data.\n",
    "And then as we saw above, if we realize that we can't be precisely sure\n",
    "of the value $p$, then we model it using a Beta distribution.\n",
    "In many cases, knowing the distribution of $p$ is useful.\n",
    "\n",
    "One might ask, then, how about the parameters of the Beta distribution?\n",
    "Do we have to give _them_ distributions too?\n",
    "\n",
    "The answer is \"usually not\", as we consider them \"nuisance\" parameters:\n",
    "parameters that we need to have, but can't take action on even if we know something about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "To help you get familiar with this skill,\n",
    "I've designed a number of exercises below that will help you get some practice.\n",
    "Be sure to reference the [distribution stories][jsbois]\n",
    "for any probability distributions mentioned in here.\n",
    "\n",
    "[jsbois]: http://bois.caltech.edu/dist_stories/t3b_probability_stories.html\n",
    "\n",
    "As you embark on the exercises, always remember:\n",
    "\n",
    "![](https://memegenerator.net/img/instances/84732105/if-youre-uncertain-about-it-put-a-distribution-on-it.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate the number of car crashes per week at Brigham circle\n",
    "\n",
    "Brigham circle is a place in Boston near the Longwood Medical Area, and is notorious for car crashes. (I made the car crashes piece up.)\n",
    "\n",
    "Write down a statistical simulation that generates counts of car crashes per week at Brigham circle.\n",
    "\n",
    "Some hints that may help:\n",
    "\n",
    "- Count data are normally distributed by [Poisson][poisson] or [Negative Binomial][negbinom] distributions.\n",
    "- If you use the Poisson distribution, then its key parameter, the \"rate\" parameter, is a positive real number (positive floats). The [exponential distribution][expon] is a good choice.\n",
    "- If you use the negative binomial distribution, remember that it takes in one integer and one float parameter.\n",
    "- The official answer uses the Poisson distribution, and follows the following graphical form.\n",
    "\n",
    "[expon]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html\n",
    "[poisson]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html#scipy.stats.poisson\n",
    "[negbinom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html#scipy.stats.nbinom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import car_crash_pgm\n",
    "\n",
    "car_crash_pgm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_crash_generator():\n",
    "    \"\"\"Generate a \"per week\" car crash data point\"\"\"\n",
    "    rate = sts.expon(0.5).rvs()\n",
    "    crashes = sts.poisson(mu=rate).rvs()\n",
    "    return crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, simulate 10 draws from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[car_crash_generator() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Joint log-likelihood function for observed car crashes\n",
    "\n",
    "Now, write down the joint likelihood function for observed car crashes and its key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import car_crash_loglike\n",
    "\n",
    "# Uncomment the block below and fill in your answer.\n",
    "# def car_crash_loglike(rate: float, crashes: List[int]) -> float:\n",
    "#     \"\"\"Evaluate likelihood of per-week car crash data points.\"\"\"\n",
    "#     \n",
    "#     your answer goes here\n",
    "# \n",
    "#     return rate_like + crashes_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Evaluate joint log-likelihood of data and parameter guesses\n",
    "\n",
    "Now that you have a log likelihood function that was constructed from your priors,\n",
    "evaluate guesses of car crash rates against the following data.\n",
    "\n",
    "To best visualize this, make a plot of log likelihood on the y-axis against rate on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import car_crash_data, car_crash_loglike_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = car_crash_data()\n",
    "\n",
    "# Comment out the next line before filling in your answer\n",
    "car_crash_loglike_plot();\n",
    "\n",
    "# Your answer goes below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus exercise\n",
    "\n",
    "As a bonus exercise, add a few more data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate the heights of men in North and South Korea\n",
    "\n",
    "It is well-known that there is a height difference between adult men in North and South Korea,\n",
    "due to differences in nutrition (direct cause) resulting from government (mis-)management. \n",
    "\n",
    "Write two functions that simulates the data generating process for observed human male height in North and South Korea.\n",
    "Assume that South Korean men are somewhere in the vicinity of 180 cm on average,\n",
    "while North Korean mean are somwhere in the vicinity of 165 cm on average,\n",
    "but that this is not precisely known.\n",
    "\n",
    "Some guides to help:\n",
    "\n",
    "- Name the two functions `s_korea_generator()` and `n_korea_generator()`.\n",
    "- For height, a [Gaussian distribution][gaussian] is a _good enough_ model, even though strictly speaking it is positive-bound.\n",
    "- We should operate in the centimeter scale, as this scale places us in the hundreds range, which makes things easier to reason about.\n",
    "- Because the spread of heights might not be precisely known, we can model this uncertainty by placing an [exponential distribution][expon] over it, because scale parameters are positive-only distributed.\n",
    "- Assume that the mean height and the variance of the height distribution cannot be precisely known, which means you have to place a probability distribution over those parameters too.\n",
    "\n",
    "[gaussian]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm\n",
    "[expon]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html#scipy.stats.expon\n",
    "\n",
    "The graphical form would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import korea_pgm\n",
    "\n",
    "korea_pgm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import s_korea_generator, n_korea_generator\n",
    "\n",
    "# Your answer goes here.\n",
    "# def s_korea_generator():\n",
    "#     pass \n",
    "\n",
    "# def n_korea_generator():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_korea_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_korea_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the two are of the same structure, so you can probably merge them into one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korea_height_generator(mean_loc: float, mean_scale: float, scale_scale: float) -> float:\n",
    "    mean = sts.norm(loc=mean_loc, scale=mean_scale).rvs()\n",
    "    scale = sts.expon(scale=scale_scale).rvs()\n",
    "    height = sts.norm(loc=mean, scale=scale).rvs()\n",
    "    return height\n",
    "\n",
    "n_korea_height = korea_height_generator(mean_loc=165, mean_scale=3, scale_scale=1)\n",
    "s_korea_height = korea_height_generator(mean_loc=180, mean_scale=3, scale_scale=1)\n",
    "n_korea_height, s_korea_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Joint log-likelihood of heights\n",
    "\n",
    "Similar to the exercise above, calcualte the joint log-likelihood of heights with possible values of mean and scale evaluated against the prior distributions stated.\n",
    "\n",
    "To be a bit more precise, create one log-likelihood function for South Korean heights and one for North Korean heights, and then one for their combined joint likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import s_korea_height_loglike, n_korea_height_loglike, joint_height_loglike\n",
    "\n",
    "# Your answer for South Korean log likelihoods here\n",
    "# def s_korea_height_loglike(mean: float, scale: float, heights: List[int]) -> float:\n",
    "#     pass\n",
    "\n",
    "# Your answer for North Korean log likelihoods here\n",
    "# def n_korea_height_loglike(mean: float, scale: float, heights: List[int]) -> float:\n",
    "#     pass\n",
    "\n",
    "# Your answer for the combined joint likelihood of South and North Korean heights\n",
    "# def joint_height_loglike(s_mean: float, s_scale: float, n_mean: float, n_scale: float, s_heights: List[int], n_heights: List[int]) -> float:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Evaluate log-likelihood of true parameter guesses\n",
    "\n",
    "Now that you've got a log likelihood function written down,\n",
    "evaluate some guesses as to what the best \"mean\" and \"scale\" values are,\n",
    "given the data\n",
    "and the priors that you specified in your log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.simulation import s_korea_height_data, n_korea_height_data\n",
    "s_korea_heights = s_korea_height_data()\n",
    "n_korea_heights = n_korea_height_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_mean = FloatSlider(min=150, max=190, value=155, step=1)\n",
    "s_scale = FloatSlider(min=0.1, max=10, value=2, step=0.1)\n",
    "n_mean = FloatSlider(min=150, max=190, value=155, step=1)\n",
    "n_scale = FloatSlider(min=0.1, max=10, value=2, step=0.1)\n",
    "\n",
    "@interact(s_mean=s_mean, s_scale=s_scale, n_mean=n_mean, n_scale=n_scale)\n",
    "def evaluate_joint_likelihood(s_mean: float, s_scale: float, n_mean: float, n_scale: float) -> float:\n",
    "    return joint_height_loglike(s_mean, s_scale, n_mean, n_scale, s_korea_heights, n_korea_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the full uncertainty\n",
    "\n",
    "Exciting stuff ahead! Notice how it's super troublesome to manually slide sliders all over the place.\n",
    "Well, we're going to attempt to solve that  by using Monte Carlo simulation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, draw numbers uniformly in the regime of 130-210 for heights, and 1-6 for scales.\n",
    "def draw():\n",
    "    s_mean, n_mean = sts.uniform(130, 80).rvs(2)  # bounds are 150-190, rtfd\n",
    "    s_scale, n_scale = sts.uniform(1, 5).rvs(2)   # bounds are 2-8, rtfd\n",
    "    \n",
    "    return (s_mean, s_scale, n_mean, n_scale)\n",
    "\n",
    "# Then, set up 2000 draws\n",
    "params = np.array([draw() for _ in range(2000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we evaluate the log-likelihood.\n",
    "loglikes = []\n",
    "for param_set in params:\n",
    "    loglikes.append(evaluate_joint_likelihood(*param_set))\n",
    "loglikes = np.array(loglikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "param_df = pd.DataFrame(params)\n",
    "loglike_df = pd.DataFrame(loglikes)\n",
    "\n",
    "plotting_df = pd.concat([param_df, loglike_df], axis=1)\n",
    "plotting_df.columns = [\"s_mean\", \"s_scale\", \"n_mean\", \"n_scale\", \"loglike\"]\n",
    "plotting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "\n",
    "s_mean = params[:, 0]\n",
    "s_scale = params[:, 1]\n",
    "n_mean = params[:, 2]\n",
    "n_scale = params[:, 3]\n",
    "\n",
    "alpha=1\n",
    "axes[0, 0].hexbin(s_mean, s_scale, C=loglikes, alpha=alpha)\n",
    "axes[0, 0].set_xlabel(\"South Korea Mean\")\n",
    "axes[0, 0].set_ylabel(\"South Korea Scale\")\n",
    "\n",
    "axes[0, 1].hexbin(s_mean, n_mean, C=loglikes, alpha=alpha)\n",
    "axes[0, 1].set_xlabel(\"South Korea Mean\")\n",
    "axes[0, 1].set_ylabel(\"North Korea Mean\")\n",
    "\n",
    "axes[0, 2].hexbin(s_mean, n_scale, C=loglikes, alpha=alpha)\n",
    "axes[0, 2].set_xlabel(\"South Korea Mean\")\n",
    "axes[0, 2].set_ylabel(\"North Korea Scale\")\n",
    "\n",
    "axes[1, 0].hexbin(s_scale, n_mean, C=loglikes, alpha=alpha)\n",
    "axes[1, 0].set_xlabel(\"South Korea Scale\")\n",
    "axes[1, 0].set_ylabel(\"North Korea Mean\")\n",
    "\n",
    "axes[1, 1].hexbin(s_scale, n_scale, C=loglikes, alpha=alpha)\n",
    "axes[1, 1].set_xlabel(\"South Korea Scale\")\n",
    "axes[1, 1].set_ylabel(\"North Korea Scale\")\n",
    "\n",
    "axes[1, 2].hexbin(n_mean, n_scale, C=loglikes, alpha=alpha)\n",
    "axes[1, 2].set_xlabel(\"North Korea Mean\")\n",
    "axes[1, 2].set_ylabel(\"North Korea Scale\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What are _plausible_ values?\n",
    "\n",
    "Given the chart that you see above, \n",
    "what are the plausible values of the mean and scale parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Figuring out plausible values\n",
    "\n",
    "Now that you've seen how to use the `scipy.stats` module to write\n",
    "data-generating stories and simulate data,\n",
    "in the next notebook, we are going to use PyMC3\n",
    "to help us with the inferential protocol,\n",
    "i.e. inferring the most credible values of key model parameters, given the data.\n",
    "Hop over to the next chapter to learn about the Inference Button (tm)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "Here are the solutions to the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions import simulation\n",
    "\n",
    "simulation??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
