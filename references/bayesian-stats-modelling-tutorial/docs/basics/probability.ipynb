{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Learn how to define foundational terminology for reasoning about probability, distributions, and likelihoods.\n",
    "- Use probability distributions implemented in the `scipy.stats` library to understand those terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, what is... probability?\n",
    "\n",
    "As a learner of Bayesian statistics, \n",
    "you, ahem, _probably_ (\\*cough cough\\*) have heard of the term \"probability\".\n",
    "What exactly is this beast?\n",
    "How do I learn it without getting lost in an entanglement of unclear terminology?\n",
    "\n",
    "Getting down to its core, using relatable and understandable words and pictures,\n",
    "is what I want to do in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "\n",
    "Firstly, probability is concerned with _spaces of possible outcomes_,\n",
    "and outcomes are drawn from these spaces of possibilities in a non-deterministic fashion.\n",
    "In classical statistics, this \"space\" of possibilities is also called a **\"support\"**.\n",
    "Remember that term, it's super important!\n",
    "You might encounter it in the literature that you read.\n",
    "\n",
    "To illustrate what \"spaces\" are, here's a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin flips\n",
    "\n",
    "With the classic coin flip, the complete space of possibilities are the heads and tails.\n",
    "Under the most common circumstances, landing on neither heads nor tails\n",
    "is considered out of the space of possibilities.\n",
    "\n",
    "In math notation, we might say the space of possibilities for the coin flip, $S_{\\text{coin}}$ is:\n",
    "\n",
    "$$S_{\\text{coin}} = \\{\\text{H}, \\text{T}\\}$$\n",
    "\n",
    "And visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import PGM\n",
    "\n",
    "G = PGM(grid_unit=1)\n",
    "G.add_node(\"H\", \"H\", x=0, y=0)\n",
    "G.add_node(\"Y\", \"Y\", x=2, y=0)\n",
    "plate = [-1.5, -1.5, 5, 3]\n",
    "G.add_plate(plate, label=\"space of possibilities\")\n",
    "G.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling dice\n",
    "\n",
    "With dice rolling, the complete space of possibilities that we are interested in\n",
    "are each of the sides' values.\n",
    "On a six-sided dice, the space of possibilities are the sides 1-6 inclusive.\n",
    "The side with the number 7... doesn't exist.\n",
    "And as such, it's outside of the space of possibilities\n",
    "\n",
    "In math notation, the space of possibilities $S_{\\text{coin}}$ could be denoted as a set:\n",
    "\n",
    "$$S_{\\text{dice}} = \\{1, 2, 3, 4, 5, 6\\}$$\n",
    "\n",
    "And visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = PGM(grid_unit=1)\n",
    "for i in range(6):\n",
    "    G.add_node(i+1, i+1, x=(i % 3) * 2, y = (i // 3) * 2)\n",
    "plate = [-1.5, -1.5, 7, 5]\n",
    "G.add_plate(plate, label=\"space of possibilities\")\n",
    "G.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of stuff\n",
    "\n",
    "When we record counts of objects,\n",
    "the space of possible values that those counts could take in\n",
    "is the set of all positive integers,\n",
    "including the number 0.\n",
    "For example, when counting the number of car crashes at an intersection,\n",
    "we should expect the positive integers, as well as 0, to be present.\n",
    "We would not expect to see -1 or 3.4 in the set of observed values.\n",
    "\n",
    "In math notation, we might say the set of possible values for counts of stuff, $S_{\\text{counts}}$ is:\n",
    "\n",
    "$$S_{\\text{counts}} = \\{0, 1, 2, \\ldots, +\\infty \\}$$\n",
    "\n",
    "It's a little hard to draw this space visually, but I think you might get the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heights of adult people\n",
    "\n",
    "When measured in meters,\n",
    "the space of possibilities is the set of all positive real numbers.\n",
    "Thus, we would expect to see 1.6, 1.49, and 1.88 as valid heights (in meters),\n",
    "but we would not expect to see -1.3 meters as a valid height.\n",
    "Strictly speaking, we might bound the space of possibilities even further,\n",
    "such as bounding the values to known world records\n",
    "of the shortest and tallest human beings ever seen,\n",
    "though oftentimes for convenience, we might elect not to.\n",
    "(Though biologically highly implausible and improbable,\n",
    "a 0.1m tall adult human is _technically_ within the space of all positive real numbers.)\n",
    "\n",
    "In math notation, we might say that the set of possible values for human heights, $S_{\\text{height}}$, is:\n",
    "\n",
    "$$S_{\\text{height}} = \\mathbb{R}^+$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credibility Points\n",
    "\n",
    "Having dealt with probability's first concept, \"spaces of outcomes\",\n",
    "we now have to turn our attention to what I commonly refer to as \"credibility points\".\n",
    "Probability is concerned with credibility points assigned to each of the possible outcomes.\n",
    "\n",
    "The easiest way to think about these credibility points is to think of it in terms of an \"area\".\n",
    "To help you visualize, imagine that you have a square of area 1.\n",
    "What you want to do is break the square of area 1 into N smaller pieces,\n",
    "each corresponding to one of the possible outcomes in the space of possible outcomes,\n",
    "each of which having an area proportional to the credibility points we are going to assign\n",
    "to each of those possible outcomes.\n",
    "Thus, with a probability distribution, a core thing we must know \n",
    "is the assignments of stick lengths to any individual value.\n",
    "\n",
    "Remember, the _only_ requirements to how area is assigned to each of the outcomes:\n",
    "\n",
    "- It must be positive and real-valued.\n",
    "- The total area assigned to all possible outcomes must sum to 1.\n",
    "\n",
    "Let's look at a few examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete \"Credibility Points\"\n",
    "\n",
    "For a coin flip, if you have a fair coin,\n",
    "you might assign 0.5 of the stick to one outcome, and 0.5 of the stick to the other.\n",
    "Or if you have a biased coin, you would assign $p$ to one outcome and $1-p$ to the other.\n",
    "\n",
    "Visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from itertools import cycle\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "colors = cycle([\"red\", \"blue\"])\n",
    "\n",
    "def plot_outcomes(proportions, colors, ax):\n",
    "    total = 0\n",
    "    for n, h in proportions.items(): # n = name, h = height\n",
    "        rect = patches.Rectangle((0, total), 1, h, linewidth=1, color=next(colors))\n",
    "        ax.add_patch(rect)\n",
    "        total += h\n",
    "\n",
    "plot_outcomes({\"H\": 0.5, \"T\": 0.5}, colors, axes[0])\n",
    "axes[0].set_title(\"Fair dice\")\n",
    "plot_outcomes({\"H\": 0.3, \"T\": 0.7}, colors, axes[1])\n",
    "axes[1].set_title(\"Biased dice\")\n",
    "\n",
    "def set_ticks(ax):\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([0, 1])\n",
    "    \n",
    "set_ticks(axes[0])\n",
    "set_ticks(axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a six-sided dice,\n",
    "you might assign $\\frac{1}{6}$ of the area to each of the outcomes,\n",
    "if the dice were fair.\n",
    "\n",
    "Visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"black\", \"yellow\", \"blue\", \"white\", \"purple\"]\n",
    "fig, ax = plt.subplots()\n",
    "plot_outcomes({i: 1/6 for i in range(6)}, colors=cycle(colors), ax=ax)\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of these examples described above,\n",
    "the assignment of stick length is _uniform_ across all possible outcomes.\n",
    "\n",
    "Now, there is usually a math function that assigns area to each outcome.\n",
    "It's a bit like assigning a lump of mass to each outcome.\n",
    "The function that we get to define,\n",
    "or that others may have defined for us,\n",
    "is called the **probability mass function**.\n",
    "You may have seen it for the coin flip and dice below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey=True)\n",
    "axes[0].bar(x=[\"H\", \"T\"], height=[0.5, 0.5])\n",
    "axes[0].set_title(\"Fair Coin Flip PMF\")\n",
    "axes[1].bar(x=range(6), height=[1/6] * 6)\n",
    "axes[1].set_title(\"Fair Dice Roll PMF\")\n",
    "axes[0].set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heights and Areas\n",
    "\n",
    "At this point, I want to highlight two important points.\n",
    "\n",
    "Firstly, we had the chance to decide what the function is that best expresses our belief\n",
    "over the credibility of each of the outcomes.\n",
    "We used what we might effectively call a \"uniform discrete distribution\"\n",
    "over each of the discrete outcomes.\n",
    "\n",
    "Secondly, it is the _areas_ that express for us\n",
    "the credibility assigned to any of the outcomes.\n",
    "When we unstack the areas\n",
    "and place each of the outcomes on the x-axis with width 1,\n",
    "their heights _just happen_ to equal to their area.\n",
    "This is what happens when we plot the bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous \"Credibility Points\"\n",
    "\n",
    "Now, let's consider the interval $[0, 1]$.\n",
    "How many _real_ numbers lie in that interval? \n",
    "\n",
    "The answer? \n",
    "\n",
    "> _It's actually infinite..._\n",
    "\n",
    "As such, unlike the discrete case, if we try to assign any real area to a single number,\n",
    "within the constrained bound of $[0, 1]$, we would end up with infinite area!\n",
    "\n",
    "So, instead of assigning area to individual values,\n",
    "we assign area to a range of values,\n",
    "with increasing or decreasing _density_ of area per unit interval,\n",
    "taken _in the limit as the interval width approaches 0_.\n",
    "That curve we draw, which tells us the density of area over an infinitesimal interval,\n",
    "is called the **probability density function** (PDF).\n",
    "\n",
    "The corollary of this definition is that there is 0 area, \n",
    "and hence 0 probability, associated with any value;\n",
    "probability can only be associated with a _range_ of values.\n",
    "That is because there is 0 area associated with any value under the PDF.\n",
    "Additionally, by definition the total area under the credibility assignment function\n",
    "must total to 1.0,\n",
    "otherwise we do not have a proper probability function,\n",
    "over the range of possible outcome values.\n",
    "That said, even though there is no _area_ associated with a single value,\n",
    "there is still _height_ associated with it.\n",
    "This point is important; we will use this fact when discussing _likelihoods_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Biased coin flip outcome distribution\n",
    "\n",
    "We have enough ideas covered for our first programming exercise!\n",
    "This first exercise will get you comfortable with the fact that probability distributions\n",
    "are nothing more than math functions that assign credibility points to a space of possible values.\n",
    "Are you ready? Here's the exercise:\n",
    "\n",
    "1. Define a Python function called `coin_distribution()` that assigns area to outcomes of a biased coin as a dictionary mapping.\n",
    "    1. You may define the credibility points any way you want. The coin can be fair or unfair.\n",
    "    1. In the dictionary mapping, the keys should be possible outcomes, and the values should be the credibility points assigned.\n",
    "2. Write a software test that guarantees that the total area assigned equals to 1.\n",
    "\n",
    "An example to get you kickstarted is as follows:\n",
    "\n",
    "```python\n",
    "def some_distribution():\n",
    "    mapping = {\n",
    "        \"outcome_1\": 1/3,\n",
    "        \"outcome_2\": 1/3,\n",
    "        \"outcome_3\": 1/3,\n",
    "    }\n",
    "    return mapping\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import coin_distribution, test_coin_distribution\n",
    "import numpy as np\n",
    "\n",
    "# Your answer below:\n",
    "# def coin_distribution():\n",
    "#     pass\n",
    "\n",
    "# def test_coin_distribution():\n",
    "#     pass\n",
    "\n",
    "test_coin_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Dice outcome distribution\n",
    "\n",
    "Let's do one more exercise, simply to reinforce the ideas.\n",
    "Instead of a coin, let's do a six-sided dice.\n",
    "\n",
    "1. Define a Python function called `dice_distribution()` that assigns credibility area to outcomes of a dice roll.\n",
    "    1. You may define the credibility area any way you want: it can be a fair dice or an unfair dice.\n",
    "    1. In the dictionary mapping, the keys should be possible outcomes, and the values should be the credibility points assigned.\n",
    "1. Write a software test that guarantees that the total area assigned equals to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import dice_distribution, test_dice_distribution\n",
    "    \n",
    "# Your answer below:\n",
    "# def dice_distribution():\n",
    "#     pass\n",
    "\n",
    "# def test_dice_distribution():\n",
    "#     pass\n",
    "\n",
    "\n",
    "test_dice_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this implementation of the coin distribution _feels_ a tad dissatisfying, you're not off the mark!\n",
    "Thus far, for the coin distribution, we've only defined the mapping from outcomes to credibility fraction.\n",
    "We have not, however, defined other things that we might be used to with probability distributions, though,\n",
    "such as drawing numbers from it.\n",
    "That's the topic of the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions have stories\n",
    "\n",
    "You've thus now defined two discrete probability distributions: the coin flip, and the dice roll.\n",
    "The key thing we implemented here was nothing more than\n",
    "**the assignment of probability density/mass (_credibility points_)\n",
    "across the potential outcomes (_support_).**\n",
    "\n",
    "Each of these probability distributions tell a \"data generating story\".\n",
    "There are other probability distributions, and they each have their own data generating story.\n",
    "It pays to know these stories, as they come handy when describing different data generating processes.\n",
    "\n",
    "Justin Bois, an instructor in the Bioengineering department at Caltech,\n",
    "has compiled [an incredibly useful resource][dist_stories] for learning about the data generation stories\n",
    "for most probability distributions.\n",
    "You should bookmark it!\n",
    "\n",
    "[dist_stories]: http://bois.caltech.edu/dist_stories/t3b_probability_stories.html\n",
    "\n",
    "### The shortcut to using distributions...\n",
    "\n",
    "...is to know its support.\n",
    "More often than not, your data restricts the distributions that are valid to describe it.\n",
    "For example, if you have positive-only data, you shouldn't use a distribution that has infinite support.\n",
    "Or if you have continuous data, you can't use discrete distributions.\n",
    "\n",
    "Keep that pro tip in mind!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events and its relation to outcomes\n",
    "\n",
    "Let's talk about \"Events\", then.\n",
    "Events happen, and when they do, we get data.\n",
    "Well, at least, that's the colloquial way of expressing how our data are generated.\n",
    "\n",
    "More formally, when an event takes place, we draw an outcome from our probability distribution\n",
    "in a fashion proportional to the credibility points assigned to that outcome.\n",
    "In the statistics and probability worlds, this is how we might choose to model our **data generation process**.\n",
    "\n",
    "In the Python world, we have the SciPy statistics library that gives us a collection of distributions\n",
    "that we can use in our modelling problems, to illustrate this idea.\n",
    "We will see the library's use throughout this tutorial.\n",
    "For now, let's take a look at the use of the Bernoulli distribution in modelling coin flips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli: Modelling coin flips\n",
    "\n",
    "To model coin flips, the probability model that one might choose\n",
    "is the Bernoulli distribution.\n",
    "This is because its support is binary (`0` and `1`),\n",
    "and it allows us arbitrarily assign probability between them.\n",
    "\n",
    "First off, we instantiate a Bernoulli distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "b = bernoulli(p=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a biased coin flip probability distribution.\n",
    "\n",
    "Now, we can draw values from the Bernoulli, using the `.rvs(n)` method. \n",
    "The etymology of `.rvs` is \"random variates\".\n",
    "We will touch on \"random variables\" (which is related) later.\n",
    "\n",
    "Meanwhile, we can draw 10 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and run the cell above a second time.\n",
    "You should see a different configuration of 10 outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Binomials\n",
    "\n",
    "While Bernoulli distributions provide a data generating model for 1 coin flip,\n",
    "Binomial distributions provide a data generating model for 1 or more coin flips.\n",
    "As such, Binomial distributions are the generalization of Bernoulli distributions.\n",
    "\n",
    "To reinforce this idea, try this next exercise:\n",
    "\n",
    "1. Create a Binomial distribution that generates Bernoulli outcomes. Some hints:\n",
    "    1. The binomial distribution is named `binom` in `scipy.stats`. We've imported it for you. \n",
    "    1. Be sure to reference [the docs][binom]!\n",
    "    1. You will have to set both the `n` and `p` parameters!)\n",
    "1. Then draw 10 outcomes from the Binomial distribution.\n",
    "\n",
    "[binom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "from bayes_tutorial.solutions.probability import binomial_answer\n",
    "\n",
    "# The correct answer is generated from this function.\n",
    "draws = binomial_answer()\n",
    "\n",
    "# Your answer goes here\n",
    "\n",
    "# The test of your answer's correctness is below:\n",
    "assert set(draws).issubset([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Modelling dice rolls\n",
    "\n",
    "The multinomial distribution gives us a natural extension to the Bernoulli.\n",
    "Try using it to model dice rolls.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "1. The multinomial distribution is named `multinomial` in `scipy.stats`\n",
    "1. It too also accepts two arguments, `n` and `p`.\n",
    "    1. `n` is the number of draws to make.\n",
    "    1. `p` is a list/vector of probabilities of each outcome, and must sum to 1!\n",
    "1. Take out 10 draws from the multinomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "from bayes_tutorial.solutions.probability import multinomial_answer\n",
    "\n",
    "# The correct answer is generated from this function.\n",
    "draws = multinomial_answer()\n",
    "\n",
    "# Your answer below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice, to represent six discrete outcomes,\n",
    "we choose to use a \"one-hot\" vector as the representation of the outcomes.\n",
    "This can be converted back to our favourite representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect2dice(v: np.ndarray) -> int:\n",
    "    return np.where(v == 1)[1] + 1\n",
    "\n",
    "vect2dice(draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our understanding of of probability distributions should feel a little more complete.\n",
    "In this section, we saw **the ability to draw _outcomes_ \n",
    "in proportion to the probability density/mass assigned to the outcome**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods\n",
    "\n",
    "We're now going to look at this idea of likelihoods.\n",
    "Likelihood is related to probability,\n",
    "but here, I have chosen to think of it as a separate idea.\n",
    "\n",
    "Probability is the tendency of certain outcome values\n",
    "to be drawn from a distribution.\n",
    "By contrast, likelihood is calculated\n",
    "when we evaluate data against a probability distribution.\n",
    "\n",
    "There is one very important thing for you to keep in mind\n",
    "as you continue reading on:\n",
    "In the vast majority of cases,\n",
    "the math function that defines the tendency of data to be drawn from the distribution,\n",
    "i.e. the _probability mass function_ or the _probability density function_,\n",
    "is **exactly the same math function** used to evaluate the likelihood of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of a coin flip\n",
    "\n",
    "Let's start simple, and look at the ideas in the context of coin flips.\n",
    "\n",
    "When we flip a fair coin once, giving us an event $C$, and we get a heads (denote this with `H`), what is the _likelihood_ (height) of this event happening?\n",
    "From our common knowledge, we might choose to start by setting up a probabilistic model\n",
    "where the probability of heads $P(C=H)$, is given by the PMF of a Bernoulli(0.5).\n",
    "Now, we need to evaluate data _against_ this probabilistic model, \n",
    "so we use the PMF height to give us the \"likelihood\" of observing the data, under the assumed model,\n",
    "because the height of the PMF gives us a measuring rod of sorts to tell us how \"likely\" our data are.\n",
    "Let's denote this \"likelihood\" as $\\mathcal{L}(\\text{H})$:\n",
    "\n",
    "$$\\mathcal{L}(heads)=P(C=H)$$\n",
    "\n",
    "...or in words, the likelihood of getting a heads is evaluated by using the (height of the) probability of the coin flip being heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We are going to calculate the likelihood of coin flip data under the assumption of a fair coin flip, using the `scipy.stats` library.\n",
    "\n",
    "(Do pay attention to the language: with likelihoods, we are always evaluating data against an assumed distribution.)\n",
    "\n",
    "Every `scipy.stats` probability distribution has an associated `.pmf(x)` (discrete distributions) or `.pdf(x)` (continuous distributions) class method. You pass in data `x`, which can be either a scalar number or an ndarray of some kind, and it returns, elementwise, the likelihood of each data point.\n",
    "\n",
    "Your exercise is as follows: calculate the likelihood of each observation in `coin_1_data` and `coin_2_data` below. We have provided a `fair_coin_model` RV that you can use, which is a \"frozen\" or pre-configured Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import fair_coin_model, coin_data_likelihood\n",
    "from inspect import getsource\n",
    "\n",
    "# FYI on how fair_coin_model is defined\n",
    "print(getsource(fair_coin_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = fair_coin_model()\n",
    "coin_data_1 = [0, 1, 1, 0, 1, 0, 0, 1, 0, 0]\n",
    "coin_data_2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# This is the \"correct\" answer.\n",
    "coin_data_likelihood(coin_data_1, coin_data_2)\n",
    "\n",
    "# Your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Notice how I have provocatively placed in two very contrasting situations:\n",
    "one with a roughly even balance of `0`s and `1`s,\n",
    "and one with a completely biased set of `0`s and `1`s.\n",
    "\n",
    "Do the likelihood calculations surprise you?\n",
    "Can you explain the rationale for your answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import likelihood_coin_toss\n",
    "\n",
    "# Uncomment the next line to see my answer.\n",
    "# print(likelihood_coin_toss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint distribution and likelihood of multiple coin flips\n",
    "\n",
    "When we flip a fair coin twice and obtain the outcome `HT` (a head, followed by a tail),\n",
    "what is the _joint likelihood_ of these two events happening in this sequence?\n",
    "\n",
    "To start, just as we use a probability distribution to model a single coin toss,\n",
    "we can use a _joint probability distribution_ as a probability model for two coin tosses.\n",
    "The same rules of probability apply, in that the total \"mass\" or \"density\" assigned must equal to 1,\n",
    "and that the values must be distributed across the valid outcome values (support).\n",
    "\n",
    "Using the notation where $C_i$ refers to the $i$th random variable modelling coin $i$, and the outcomes are $H$ and $T$, the joint pairs are:\n",
    "\n",
    "- ($C_1=H, C_2=H$)\n",
    "- ($C_1=T, C_2=H$)\n",
    "- ($C_1=H, C_2=T$)\n",
    "- ($C_1=T, C_2=T$)\n",
    "\n",
    "We can choose to define their joint probability distribution as $P(C_1, C_2)$,\n",
    "with the following \"canonical\" probability mass allocations:\n",
    "\n",
    "- $P(C_1=H, C_2=H)=P(C_1=H) \\times P(C_2=H)=0.25$\n",
    "- $P(C_1=T, C_2=H)=P(C_1=T) \\times P(C_2=H)=0.25$\n",
    "- $P(C_1=H, C_2=T)=P(C_1=H) \\times P(C_2=T)=0.25$\n",
    "- $P(C_1=T, C_2=T)=P(C_1=T) \\times P(C_2=T)=0.25$\n",
    "\n",
    "You might have noticed the multiplication of independent probabilities.\n",
    "Because two coin tosses are independent,\n",
    "(i.e. one toss doesn't affect the other),\n",
    "one can make the argument that their joint probability mass allocation\n",
    "is equivalent to the product of their individual probability allocation.\n",
    "This comes from the rules of boolean logic.\n",
    "Now, because the coins are fair, the products all end up being the same,\n",
    "but this need not necessarily be true!\n",
    "Especially if one of the coins, or both of them, are modelled using an \"unfair\" coin toss model.\n",
    "\n",
    "You should know also that you _technically_ can define\n",
    "the _joint_ probability mass allocations any way you want,\n",
    "without leveraging independence,\n",
    "_as long as they sum to 1, and cover all possible valid outcomes_.\n",
    "Leveraging this idea of \"independence\" between events simply\n",
    "makes constructing the joint probability model easier,\n",
    "as we won't _otherwise_ have to individually create entries for the joint likelihoods.\n",
    "The deeper concept powering the simple multiplication is the idea of **exchangability**,\n",
    "which colloquially means I could ___exchange___ the positions of our data points,\n",
    "and their joint likelihood does not change.\n",
    "\n",
    "Since we have the probability mass function, we also have the likelihoods (heights!) available to us.\n",
    "Therefore, we can calculate the likelihood of our data using the _joint distribution_ available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Calculate the joint likelihood of the coin tosses above.\n",
    "\n",
    "_Hint:_ If you have an array and you would like to multiply all of the individual elements together, you can use NumPy: `np.product(arr)` gives you the product of all elements in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import coin_data_joint_likelihood\n",
    "\n",
    "# This is the \"correct\" answer\n",
    "coin_data_joint_likelihood(coin_data_1, coin_data_2)\n",
    "\n",
    "# Your answer below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "You'll notice that the numbers get really small.\n",
    "In practice, we usually calculate the joint log-likelihood, \n",
    "rather than the joint likelihood, so that we don't run into underflow issues\n",
    "when doing Bayesian computing.\n",
    "\n",
    "Every `scipy.stats` probability distribution has a `.logpmf(x)` or `.logpdf(x)` class method,\n",
    "whose semantics are similar to `.pdf(x)` and `.pmf(x)`.\n",
    "Use this to calculate the joint **log**-likelihood of the coin tosses.\n",
    "\n",
    "(_Hint:_ The product of numbers, in log-space, is a sum!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import coin_data_joint_loglikelihood\n",
    "\n",
    "# This is the \"correct\" answer\n",
    "coin_data_joint_loglikelihood(coin_data_1, coin_data_2)\n",
    "\n",
    "# Your answer below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data are usually assumed to be \"independent and identically distributed\"\n",
    "\n",
    "As things turn out, it is a common, crucial, but also fairly reasonable assumption\n",
    "that data come to us independent of one another.\n",
    "So in most cases, calculating joint likelihoods of data\n",
    "usually means that we can multiply together their likelihoods under the given model.\n",
    "(This is the \"exchangability idea\" at work!)\n",
    "\n",
    "### Dependencies show up elsewhere\n",
    "\n",
    "That said, there are exceptions, such as in Markov chains and sequential data,\n",
    "where data come to us with sequential dependencies on one another.\n",
    "\n",
    "You'll see this in the next notebook, when we look at how to write simulation models,\n",
    "so don't be too quick to assume that independence shows up everywhere! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Likelihood is all you need\"\n",
    "\n",
    "You might be wondering, what's the difference between probability and likelihood?\n",
    "Put as succinctly as I could think of:\n",
    "\n",
    "- Probability (the _area_) gives us the tendencies for _potential_ outcomes to be drawn on each event. \n",
    "- Likelihoods (the _height_) give us the function to evaluate how probable an _observed_ outcome (or data point) is under a given probability model (i.e. the assignment of area to outcomes (discrete) or ranges of outcomes (continuous)).\n",
    "\n",
    "In statistical inference, likelihood is all you need.\n",
    "Well, strictly speaking, likelihood is _basically what you need_.\n",
    "\n",
    "Remember: likelihood is calculated\n",
    "when we evaluate how likely data, assumed to be drawn from a distribution,\n",
    "were drawn from that distribution.\n",
    "\n",
    "For two independently drawn outcomes,\n",
    "we can multiply their likelihoods together\n",
    "to obtain their joint likelihood.\n",
    "This trivially extends to three or more outcomes.\n",
    "\n",
    "There is a principle in statistics, called the [\"Likelihood principle\"][likelihood].\n",
    "You don't have to remember the term, but it is helpful to remember the idea.\n",
    "From Wikipedia:\n",
    "\n",
    "> In statistics,the likelihood principle is the proposition that, given a statistical model, \n",
    "> all the evidence in a sample relevant to model parameters is contained in the likelihood function.\n",
    "\n",
    "[likelihood]: https://en.wikipedia.org/wiki/Likelihood_principle\n",
    "\n",
    "As such, you'll see that in this suite of notebooks,\n",
    "we will be looking _primarily_ at likelihoods, and not probability.\n",
    "After all, the most common thing that we're trying to do\n",
    "is evaluate data against a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions\n",
    "\n",
    "With these components:\n",
    "\n",
    "- spaces of possible outcomes (i.e. the \"_support_\"),\n",
    "- probability mass or density assigned to each outcome,\n",
    "- total probability assigned across all outcomes summing to 1, and\n",
    "- non-deterministic drawing of outcomes per event,\n",
    "\n",
    "we have enough to define a probability distribution.\n",
    "Pictorially, it looks like the following:\n",
    "\n",
    "![](./images/distributions.png)\n",
    "\n",
    "Remember: The PMF/PDF can be mathematically or empirically defined,\n",
    "it doesn't really matter, as long as the total area is 1.\n",
    "\n",
    "I think we have enough to define a probability distribution in an understandable fashion for programmers:\n",
    "\n",
    "> A probability distribution is a description of how probability mass or density is assigned to valid outcomes (the support of the distribution), such that the sum of masses or integral of densities equals to 1.\n",
    "\n",
    "That _description_ is most commonly done by a math equation that is parametrized.\n",
    "\n",
    "Finally, keep in mind the distinction between how probability, from the perspective of how we use them:\n",
    "- When we draw data from a probability distribution's space of outcomes, the \"probability\" of each outcome is defined by the probability mass/density function.\n",
    "- When we evaluate data against a probability distribution's PMF/PDF, we are evaluating the _likelihood_ of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "\"Random variable\" is probably another term that you've seen floating around.\n",
    "\n",
    "As usual, I think contrasts will be the most illuminating here.\n",
    "\n",
    "When we build a model of the world, we'll usually assign _variables_ to represent things.\n",
    "When we so-called \"run the model\" once, usually we'll assign a real value to those variables,\n",
    "yielding one \"realization\" of the model.\n",
    "Now, those variables can be _deterministic_/_fixed_ or _random_/_stochastic_.\n",
    "If over each realization, we \"fix\" the variable at a given value, then it is a _deterministic_ variable.\n",
    "If over each realization, we allow it to vary stochastically, then it is a _random_ variable.\n",
    "\n",
    "Let's move on to a bit of verbiate. \n",
    "When setting up a problem, we'll usually say something like:\n",
    "\n",
    "> Let $p$ be the __random variable__ that models _the probability of heads_, and let $p$ be __Beta distributed__, with parameters $\\alpha$ and $\\beta$\n",
    ">\n",
    "> Let $c$ be the __random variable__ that models the _outcome of a coin flip_, and let $c$ be __Bernoulli distributed__, with parameter $p$.\n",
    "\n",
    "More generically:\n",
    "\n",
    "> Let `algebraic symbol` be the random variable that models `some real thing, or model component`, and let `algebraic symbol` be distributed by `some distribution`, with parameters `some distribution's parameters`.\n",
    "\n",
    "I think with this in place, we have a sufficiently precise language going forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "This notebook is about Bayesian statistical inference, so we have to talk about Bayes' rule.\n",
    "\n",
    "Prior to reading this notebook, you may have seen Bayes' rule.\n",
    "It's so famous, [it's even become a neon sign][bayesjpg]!\n",
    "\n",
    "[bayesjpg]: https://en.wikipedia.org/wiki/File:Bayes%27_Theorem_MMB_01.jpg\n",
    "\n",
    "Bayes' rule looks like this:\n",
    "\n",
    "$$P(A|B) = \n",
    "\\frac{P(B|A)P(A)}\n",
    "{P(B)}\n",
    "$$\n",
    "\n",
    "It is a natural result that comes straight from the rules of probability,\n",
    "being that the joint distribution of two random variables\n",
    "can be written in two equivalent ways:\n",
    "\n",
    "$$P(A, B) = P(A|B)P(B) = P(B|A)P(A)$$\n",
    "\n",
    "For further treatment of joint probability and Bayes' rule,\n",
    "I'd like to suggest you take a look at Allen Downey's [Think Bayes](http://www.greenteapress.com/thinkbayes/html/thinkbayes002.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From random variables to hypotheses and data\n",
    "\n",
    "Now, I have encountered in many books that write,\n",
    "regarding the application of Bayes' rule to statistical modelling,\n",
    "something along the lines of the following:\n",
    "\n",
    "> Now, there is an alternative _interpretation_ of Bayes' rule,\n",
    "> one that replaces the symbol \"A\" with \"Hypothesis\",\n",
    "> and \"B\" with the \"Data\", such that we get:\n",
    "> \n",
    "> $$P(H|D) = \\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "At first glance, nothing seems wrong about this statement,\n",
    "but I did remember having a lingering nagging feeling\n",
    "that there was a logical jump unexplained here.\n",
    "\n",
    "More specifically, that logical jump yielded the following question: _Why are we allowed to take this interpretation?_\n",
    "\n",
    "It took asking the question to a mathematician friend, Colin Carroll,\n",
    "to finally \"grok\" the idea.\n",
    "Let me try to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces of models and data\n",
    "\n",
    "We have to think back to the fundamental idea of possible spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: How many possible Bernoullis?\n",
    "\n",
    "If we set up a Bernoulli probability distribution with parameter $p$,\n",
    "then what is the space of possible probability distributions that we could instantiate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import spaces_of_p\n",
    "\n",
    "# Uncomment the next line to reveal the answer.\n",
    "# print(spaces_of_p())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this more concretely. \n",
    "\n",
    "It is possible to instantiate many other Bernoulli distributions:\n",
    "\n",
    "```python\n",
    "b1 = bernoulli(p=0.1)\n",
    "b2 = bernoulli(p=0.15)\n",
    "b3 = bernoulli(p=0.78)\n",
    "```\n",
    "\n",
    "Each value of $p$ gives us a different configuration of a Bernoulli distribution.\n",
    "As such, a space of $p$ that gives us an infinite set of possibilities of $p$\n",
    "**will give us an infinite set of Bernoullis**!\n",
    "\n",
    "This result should not surprise you: $p$ can take on any one of an infinite set of values between 0 and 1, each one giving a different instantiated Bernoulli.\n",
    "As such, a `Bernoulli(p)` hypothesis is drawn from a (very large) space of possible `Bernoulli(p)`s,\n",
    "or more abstractly, hypotheses, thereby giving us a $P(H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: How many configurations of data?\n",
    "\n",
    "Moreover, consider our data.\n",
    "The Bernoulli data that came to us, which for example might be `0, 1, 1, 1, 0`.\n",
    "Let's just consider one case: Given five draws from a Bernoulli,\n",
    "how many ways could our data have come in?\n",
    "(_Hint:_ You can choose to simplify the problem by considering three `1`s and two `0`s,\n",
    "or you can try considering the full space of possible outcomes for five draws.\n",
    "Either way, your answer will be illustrative!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.probability import spaces_of_data\n",
    "\n",
    "# Uncomment the next line to see my answer\n",
    "# print(spaces_of_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we have the $P(D)$ interpretation.\n",
    "\n",
    "As a modelling decision, we _choose_ to say\n",
    "that our data and model are jointly distributed,\n",
    "thus we have the _joint distribution_\n",
    "between model and data, $P(H, D)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Let us summarize the main points for you to take away from here:\n",
    "\n",
    "Firstly, a probability distribution is defined by a function\n",
    "that assigns credibility points to a space of possible outcomes.\n",
    "We can use the probability distribution\n",
    "to draw outcomes proportional to the credibility points assigned to each of the outcomes.\n",
    "\n",
    "Secondly, the likelihood principle gives us a way of connecting the sample data\n",
    "to the parameters of a probability distribution.\n",
    "We can evaluate data against a particular distribution\n",
    "to see how _likely_ that observed data was drawn from that given distribution.\n",
    "In evaluating data against that particular distribution, \n",
    "we reuse the function that assigns credibility points,\n",
    "usually known as a probability mass or density function,\n",
    "as the _likelihood_ function.\n",
    "As we will see in later notebooks,\n",
    "we can take advantage of the likelihood principle\n",
    "to infer the most likely parameter values\n",
    "that generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that you've learned about the basics of probability,\n",
    "let's go on to see how we can use probability distributions\n",
    "to simulate our data generating processes.\n",
    "Head over to the next chapter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions import probability\n",
    "\n",
    "probability??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
